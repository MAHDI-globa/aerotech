{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c19ec5-9cef-4d67-8334-eadca07bc02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== data header: folder info ===\n",
      "path: C:\\globasoft\\aerotech\\fic\n",
      "files_found: 1\n",
      "\n",
      "=== file ===\n",
      "fichier qualité des trigrammes.xlsx  (C:\\globasoft\\aerotech\\fic\\fichier qualité des trigrammes.xlsx)\n",
      "\n",
      "--- table detected ---\n",
      "name: fichier qualité des trigrammes__PDG__table_1\n",
      "rows: 20 | cols: 4\n",
      "columns: edition, date, motif, redacteur\n",
      "  edition                 date                                              motif    redacteur\n",
      "0       1  2024-06-10 00:00:00                                           Création  S. BELMONTE\n",
      "1       2  2024-09-16 00:00:00                           Ajout nouveaux arrivants    Y. RAGEOT\n",
      "2       3  2024-10-21 00:00:00                                    Ajout couturier    Y. RAGEOT\n",
      "3       4  2024-10-28 00:00:00  Mise à jour des dates d'entrée \\nAjout des hab...    Y. RAGEOT\n",
      "4       5  2024-12-16 00:00:00  Ajout nouvelle arrivante - 1 personne - Feriel...  F.BOULHABEL\n",
      "5       6  2024-12-18 00:00:00                              Départ de Yann RAGEOT  S. BELMONTE\n",
      "6       7  2024-12-19 00:00:00  Ajout de nouveaux arrivants - 4 personnes - Fr...  F.BOULHABEL\n",
      "7       8  2024-12-20 00:00:00  Ajout de nouvel arrivant - 1 personne - Loris ...  F.BOULHABEL\n",
      "[saved] out\\fichier qualité des trigrammes__PDG__table_1.csv\n",
      "\n",
      "--- table detected ---\n",
      "name: fichier qualité des trigrammes__Liste__table_1\n",
      "rows: 201 | cols: 6\n",
      "columns: nom_prenom, trig, personnel_intext, site, date_dattribution, date_de_retrait\n",
      "                 nom_prenom trig personnel_intext site    date_dattribution      date_de_retrait\n",
      "0              ABAT Nicolas  ABT          Interne  AEC  2025-02-17 00:00:00                  NaN\n",
      "1   ABDELLAOUI Schéhérazade  ADI          Interne  AEB  2025-05-05 00:00:00                  NaN\n",
      "2          ADJEROUD Bastian  BAD          Interne  AEX  2024-05-21 00:00:00                  NaN\n",
      "3              AGREBI Lilia  ARI          Interne  AEC  2025-02-24 00:00:00                  NaN\n",
      "4         ALBERTON YANNICK   YAL          Interne  AEC  2018-11-05 00:00:00  2021-09-10 00:00:00\n",
      "5  ALIAS Séverine née HERAL  ALS          Interne  AEG  2024-05-21 00:00:00                  NaN\n",
      "6      ALLARD Jean-François  ALD          Externe  AEC                  NaN                  NaN\n",
      "7               ALLARD Yann  YAD          Interne  AEC  2024-06-03 00:00:00                  NaN\n",
      "[saved] out\\fichier qualité des trigrammes__Liste__table_1.csv\n",
      "\n",
      "=== done ===\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# ================== PARAMS ================== #\n",
    "ROOT_DIR = Path(\"fic\")                # Dossier d'entrée (récursif)\n",
    "EXPORT_DIR = Path(\"out\")              # Dossier de sortie CSV (créé si export activé)\n",
    "EXPORT_TABLES = True                  # Mettre True pour exporter les tables détectées\n",
    "SHOW_SAMPLES = True                   # Afficher un aperçu (head) des tables détectées\n",
    "\n",
    "# Heuristiques génériques (adapter si besoin)\n",
    "MIN_COLS = 2               # nb min de colonnes non vides pour considérer une ligne \"tabulaire\"\n",
    "MIN_CONSEC_ROWS = 5        # nb min de lignes consécutives \"tabulaires\" pour former un bloc\n",
    "ROW_EMPTY_TOL = 1          # nb max de lignes vides autorisées à l'intérieur d'un bloc\n",
    "STOP_EMPTY_RUN = 5         # stop table si on rencontre STOP_EMPTY_RUN lignes vides consécutives après le header\n",
    "HEADER_SCAN_DEPTH = 6      # nb de premières lignes du bloc à tester pour choisir la meilleure ligne d'en-tête\n",
    "\n",
    "# Filtrage de colonnes après extraction (pour supprimer col/col_2… vides)\n",
    "MIN_NON_NULL_RATIO = 0.05  # garder une colonne si >= 5% de valeurs non nulles\n",
    "MIN_NON_NULL_ABS   = 2     # ... ou au moins 2 valeurs non nulles (filet de sécurité)\n",
    "# ============================================ #\n",
    "\n",
    "def strip_accents_lower(s: str) -> str:\n",
    "    if s is None or pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def safe_read_excel_all_sheets(path: Path):\n",
    "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "    out = []\n",
    "    for sheet in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet, header=None)\n",
    "        out.append((sheet, df))\n",
    "    return out\n",
    "\n",
    "def pick_encoding(path: Path):\n",
    "    try:\n",
    "        from charset_normalizer import from_path\n",
    "        res = from_path(str(path)).best()\n",
    "        return res.encoding if res else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def read_csv_robust(path: Path):\n",
    "    enc_guess = pick_encoding(path)\n",
    "    candidates = [e for e in [enc_guess, \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin-1\", \"iso-8859-1\"] if e]\n",
    "    for enc in candidates:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=None, engine=\"python\", encoding=enc, header=None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # dernier recours permissif\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", encoding=\"latin-1\", encoding_errors=\"replace\", header=None)\n",
    "\n",
    "def is_tabular_row(row, min_cols=MIN_COLS):\n",
    "    return row.notna().sum() >= min_cols\n",
    "\n",
    "def choose_header_row(block: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Choisit la \"meilleure\" ligne d’en-tête dans les HEADER_SCAN_DEPTH premières lignes du bloc :\n",
    "      - favorise la ligne avec le plus de cellules non vides\n",
    "      - favorise la diversité textuelle (moins de chiffres purs)\n",
    "      - évite les 'Unnamed' / vides\n",
    "    Retourne l'index relatif (dans block) de la ligne header choisie.\n",
    "    \"\"\"\n",
    "    best_idx = None\n",
    "    best_score = -1\n",
    "    limit = min(len(block), HEADER_SCAN_DEPTH)\n",
    "    for i in range(limit):\n",
    "        row = block.iloc[i]\n",
    "        vals = row.tolist()\n",
    "        non_empty = sum(pd.notna(v) and str(v).strip() != \"\" for v in vals)\n",
    "        texty = 0\n",
    "        for v in vals:\n",
    "            s = str(v).strip() if pd.notna(v) else \"\"\n",
    "            if s == \"\" or s.lower().startswith(\"unnamed\"):\n",
    "                continue\n",
    "            if re.search(r\"[A-Za-zÀ-ÿ]\", s):\n",
    "                texty += 1\n",
    "        score = non_empty * 2 + texty\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = i\n",
    "    return best_idx if best_idx is not None else 0\n",
    "\n",
    "def clean_columns(vals):\n",
    "    \"\"\"\n",
    "    Nettoie / normalise les noms de colonnes, évite doublons.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    seen = {}\n",
    "    for v in vals:\n",
    "        s = strip_accents_lower(v)\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip(\" -_\")\n",
    "        if s == \"\" or s.startswith(\"unnamed\"):\n",
    "            s = \"col\"\n",
    "        s = re.sub(r\"[^a-z0-9_ ]\", \"\", s)\n",
    "        s = re.sub(r\"\\s+\", \"_\", s).strip(\"_\")\n",
    "        if s == \"\":\n",
    "            s = \"col\"\n",
    "        if s in seen:\n",
    "            seen[s] += 1\n",
    "            s = f\"{s}_{seen[s]}\"\n",
    "        else:\n",
    "            seen[s] = 1\n",
    "        cols.append(s)\n",
    "    return cols\n",
    "\n",
    "def detect_blocks(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Détecte des blocs tabulaires génériques :\n",
    "      - une séquence de >= MIN_CONSEC_ROWS lignes \"tabulaires\"\n",
    "      - tolère des petits trous (ROW_EMPTY_TOL)\n",
    "    Retourne une liste de (start_idx, end_idx) sur l'index du df.\n",
    "    \"\"\"\n",
    "    if list(df.columns) != list(range(df.shape[1])):\n",
    "        df = df.copy()\n",
    "        df.columns = list(range(df.shape[1]))\n",
    "\n",
    "    blocks = []\n",
    "    consec = 0\n",
    "    empties_inside = 0\n",
    "    start = None\n",
    "\n",
    "    for i in df.index:\n",
    "        row = df.loc[i]\n",
    "        if is_tabular_row(row):\n",
    "            if start is None:\n",
    "                start = i\n",
    "                consec = 0\n",
    "                empties_inside = 0\n",
    "            consec += 1\n",
    "        else:\n",
    "            if start is not None:\n",
    "                empties_inside += 1\n",
    "                if empties_inside > ROW_EMPTY_TOL:\n",
    "                    end = i - (empties_inside)\n",
    "                    if end >= start and consec >= MIN_CONSEC_ROWS:\n",
    "                        blocks.append((start, end))\n",
    "                    start = None\n",
    "                    consec = 0\n",
    "                    empties_inside = 0\n",
    "\n",
    "    if start is not None and consec >= MIN_CONSEC_ROWS:\n",
    "        blocks.append((start, df.index[-1]))\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def prune_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Supprime colonnes 100% vides et colonnes trop peu remplies (ratio faible).\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df2 = df.dropna(axis=1, how=\"all\").copy()\n",
    "    if df2.empty:\n",
    "        return df2\n",
    "    n = len(df2)\n",
    "    keep = []\n",
    "    for c in df2.columns:\n",
    "        nnz = df2[c].notna().sum()\n",
    "        if nnz >= max(MIN_NON_NULL_ABS, int(n * MIN_NON_NULL_RATIO)):\n",
    "            keep.append(c)\n",
    "    return df2[keep].copy() if keep else pd.DataFrame()\n",
    "\n",
    "def carve_table_from_block(df_block: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    À partir d'un bloc \"dense\", choisit un header, applique les colonnes,\n",
    "    puis s'arrête quand on rencontre STOP_EMPTY_RUN lignes vides consécutives.\n",
    "    Nettoie ensuite les colonnes vides/quasi vides.\n",
    "    \"\"\"\n",
    "    if df_block.empty:\n",
    "        return None\n",
    "\n",
    "    h_rel = choose_header_row(df_block)\n",
    "    header_vals = df_block.iloc[h_rel].tolist()\n",
    "    cols = clean_columns(header_vals)\n",
    "\n",
    "    data = df_block.iloc[h_rel+1:].copy()\n",
    "    data.columns = cols\n",
    "\n",
    "    # stop à N lignes vides consécutives après le header\n",
    "    empty_run = 0\n",
    "    cut_idx = data.index[-1]\n",
    "    for idx in data.index:\n",
    "        if data.loc[idx].isna().all():\n",
    "            empty_run += 1\n",
    "            if empty_run >= STOP_EMPTY_RUN:\n",
    "                cut_idx = idx - STOP_EMPTY_RUN\n",
    "                break\n",
    "        else:\n",
    "            empty_run = 0\n",
    "\n",
    "    data = data.loc[:cut_idx]\n",
    "\n",
    "    # filtrer lignes trop vides (garde les lignes avec un minimum d'info)\n",
    "    data = data[data.notna().sum(axis=1) >= MIN_COLS]\n",
    "\n",
    "    # >>> prune colonnes vides/quasi vides (supprime col, col_2... 100% NaN)\n",
    "    data = prune_columns(data)\n",
    "\n",
    "    if data.empty:\n",
    "        return None\n",
    "\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "def find_tables_in_sheet(df_raw: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Détecte toutes les tables génériques dans une feuille.\n",
    "    \"\"\"\n",
    "    if list(df_raw.columns) != list(range(df_raw.shape[1])):\n",
    "        df_raw = df_raw.copy()\n",
    "        df_raw.columns = list(range(df_raw.shape[1]))\n",
    "\n",
    "    blocks = detect_blocks(df_raw)\n",
    "    tables = []\n",
    "    for (start, end) in blocks:\n",
    "        block = df_raw.loc[start:end, :]\n",
    "        table = carve_table_from_block(block)\n",
    "        if table is not None and table.shape[1] >= MIN_COLS and table.shape[0] >= 1:\n",
    "            tables.append(table)\n",
    "    return tables\n",
    "\n",
    "def process_file(path: Path):\n",
    "    results = []  # [(sheet, idx, df_table)]\n",
    "    if path.suffix.lower() in (\".xlsx\", \".xlsm\"):\n",
    "        sheets = safe_read_excel_all_sheets(path)\n",
    "        for sheet, df_raw in sheets:\n",
    "            tables = find_tables_in_sheet(df_raw)\n",
    "            for i, t in enumerate(tables, start=1):\n",
    "                results.append((sheet, i, t))\n",
    "    elif path.suffix.lower() == \".csv\":\n",
    "        df_raw = read_csv_robust(path)\n",
    "        tables = find_tables_in_sheet(df_raw)\n",
    "        for i, t in enumerate(tables, start=1):\n",
    "            results.append((None, i, t))\n",
    "    else:\n",
    "        raise ValueError(f\"Extension non gérée: {path.suffix}\")\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    if not ROOT_DIR.exists():\n",
    "        print(f\"[error] Dossier introuvable : {ROOT_DIR.resolve()}\")\n",
    "        return\n",
    "\n",
    "    files = []\n",
    "    for pat in (\"*.xlsx\", \"*.xlsm\", \"*.csv\"):\n",
    "        files += [p for p in ROOT_DIR.rglob(pat) if p.is_file() and not p.name.startswith(\"~$\")]\n",
    "\n",
    "    print(\"=== data header: folder info ===\")\n",
    "    print(f\"path: {ROOT_DIR.resolve()}\")\n",
    "    print(f\"files_found: {len(files)}\")\n",
    "\n",
    "    if not files:\n",
    "        print(\"[warn] Aucun fichier .xlsx/.xlsm/.csv trouvé.\")\n",
    "        return\n",
    "\n",
    "    if EXPORT_TABLES:\n",
    "        EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for f in sorted(files):\n",
    "        print(\"\\n=== file ===\")\n",
    "        print(f\"{f.name}  ({f.resolve()})\")\n",
    "\n",
    "        try:\n",
    "            tables = process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Lecture échouée pour {f.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not tables:\n",
    "            print(\"[info] Aucune table détectée\")\n",
    "            continue\n",
    "\n",
    "        for sheet, idx, df in tables:\n",
    "            title = f\"{f.stem}__{sheet or 'sheet'}__table_{idx}\"\n",
    "            print(f\"\\n--- table detected ---\")\n",
    "            print(f\"name: {title}\")\n",
    "            print(f\"rows: {len(df)} | cols: {df.shape[1]}\")\n",
    "            print(\"columns:\", \", \".join(map(str, df.columns.tolist())))\n",
    "\n",
    "            if SHOW_SAMPLES:\n",
    "                with pd.option_context(\"display.max_columns\", 60, \"display.width\", 160):\n",
    "                    print(df.head(8))\n",
    "\n",
    "            if EXPORT_TABLES:\n",
    "                out_path = EXPORT_DIR / f\"{title}.csv\"\n",
    "                df.to_csv(out_path, index=False)\n",
    "                print(f\"[saved] {out_path}\")\n",
    "\n",
    "    print(\"\\n=== done ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedbf3e-5480-49ff-9106-38db1c92c11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
