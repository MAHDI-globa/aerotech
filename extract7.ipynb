{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee57f4b-4e4a-42ae-9683-343774c348f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyxlsb\n",
    "pip install sqlalchemy psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c19ec5-9cef-4d67-8334-eadca07bc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# --------- Réduire le bruit ---------\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "\n",
    "# ================== PARAMS ================== #\n",
    "ROOT_DIR = Path(\"fic\")\n",
    "SHOW_SAMPLES = True\n",
    "\n",
    "# >>> Perf toggles\n",
    "QUICK_MODE = True              # True = plus rapide (moins d'inférence)\n",
    "INFER_TYPES = True             # Inférence activée\n",
    "VERBOSE = True                 # logs\n",
    "\n",
    "# Limiteurs\n",
    "MAX_FILES = None\n",
    "MAX_SHEETS = None\n",
    "SCAN_MAX_ROWS = 8000\n",
    "MAX_TABLES_PER_SHEET = 2\n",
    "\n",
    "# Heuristiques\n",
    "MIN_COLS = 2\n",
    "MIN_CONSEC_ROWS = 3 if QUICK_MODE else 5\n",
    "STOP_EMPTY_RUN = 3 if QUICK_MODE else 5\n",
    "HEADER_SCAN_DEPTH = 3 if QUICK_MODE else 6\n",
    "\n",
    "# Segmentation verticale\n",
    "COL_DENSITY_THRESHOLD = 0.12\n",
    "MIN_COL_RUN = 2\n",
    "ALLOW_SMALL_GAPS = 1\n",
    "MAX_SEGMENTS_PER_SHEET = None\n",
    "\n",
    "# Filtrage colonnes “génériques”\n",
    "MIN_NON_NULL_RATIO = 0.05\n",
    "MIN_NON_NULL_ABS   = 2\n",
    "GENERIC_COL_RE = re.compile(r\"^col(_\\d+)?$\", re.I)\n",
    "\n",
    "# ================== INFÉRENCE: CONSTANTES ================== #\n",
    "DATE_NAME_HINTS = (\"date\",\"dt_\",\"_dt\",\"attribution\",\"retrait\",\"month\",\"mois\",\"jour\",\"day\",\"time\",\"heure\")\n",
    "TIME_TOKEN_RE = re.compile(r\"\\d{1,2}:\\d{2}(?::\\d{2})?\")\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"[/\\-.]|(?:jan|feb|mar|apr|mai|may|jun|jul|aug|sep|oct|nov|dec|\"\n",
    "    r\"janv|févr|fevr|avr|juil|sept|oct|nov|d[ée]c)\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "# Monnaies: regex strictes\n",
    "# (fix: groupe NON capturant pour .str.contains)\n",
    "CURRENCY_DETECT_VALUE_RE = re.compile(r\"(?:€|\\$|usd|eur|dhs?|mad|£|gbp)\", re.I)\n",
    "CURRENCY_PARSE_RE = re.compile(\n",
    "    r\"(?P<neg>\\()?\\s*(?P<cur>€|\\$|usd|eur|dhs?|mad|£|gbp)?\\s*\"\n",
    "    r\"(?P<num>[+-]?\\s?(?:\\d{1,3}(?:[ .,\\u00A0]\\d{3})+|\\d+)(?:[.,]\\d+)?)(?(neg)\\))\",\n",
    "    re.I\n",
    ")\n",
    "CURRENCY_NAME_HINTS = (\"amount\",\"montant\",\"price\",\"prix\",\"total\",\"ttc\",\"ht\",\"paid\",\"due\",\"debit\",\"credit\")\n",
    "\n",
    "BOOL_TRUE  = {\"true\",\"vrai\",\"oui\",\"y\",\"o\",\"yes\",\"1\"}\n",
    "BOOL_FALSE = {\"false\",\"faux\",\"non\",\"n\",\"no\",\"0\"}\n",
    "\n",
    "# Seuils prudents\n",
    "RATIO_STRICT_BOOL = 0.98     # bool seulement si quasi-pur\n",
    "RATIO_NUM         = 0.85     # numérique si majorité claire\n",
    "RATIO_DATE_HARD   = 0.70\n",
    "RATIO_DATE_SOFT   = 0.30\n",
    "\n",
    "# >>> Exports (optionnels)\n",
    "EXPORT_DIR = Path(\"out_tables\")\n",
    "EXPORT_AS = None              # \"csv\" | \"parquet\" | None\n",
    "EXPORT_PG_DDL = True          # écrire un .sql (CREATE TABLE) par table\n",
    "\n",
    "# -------------------- logging utils -------------------- #\n",
    "def nowstr() -> str:\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    if VERBOSE:\n",
    "        print(f\"[{nowstr()}] {msg}\", flush=True)\n",
    "\n",
    "def step_time(prev=None):\n",
    "    t = time.perf_counter()\n",
    "    if prev is None:\n",
    "        return t, 0.0\n",
    "    return t, (t - prev)\n",
    "\n",
    "# -------------------- utils noms -------------------- #\n",
    "def strip_accents_lower(s: str) -> str:\n",
    "    if s is None or pd.isna(s):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def snake_id(s: str) -> str:\n",
    "    s = strip_accents_lower(s)\n",
    "    s = re.sub(r\"[\\s\\.\\-]+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"sheet\"\n",
    "\n",
    "def is_generic_colname(name: str) -> bool:\n",
    "    return bool(GENERIC_COL_RE.fullmatch(name or \"\"))\n",
    "\n",
    "# -------------------- Helpers parsing -------------------- #\n",
    "def looks_like_code(values: pd.Series) -> bool:\n",
    "    \"\"\"Codes avec zéros en tête / longueur quasi fixe (4-10) -> garder texte.\"\"\"\n",
    "    s = values.astype(\"string\").dropna().str.strip()\n",
    "    if len(s) == 0:\n",
    "        return False\n",
    "    digit_only = s.str.fullmatch(r\"\\d+\")\n",
    "    if (s.str.startswith(\"0\") & digit_only).any():\n",
    "        return True\n",
    "    lengths = s.where(digit_only).str.len().dropna()\n",
    "    if len(lengths) > 0:\n",
    "        mode_len = lengths.mode().iloc[0]\n",
    "        if (lengths.eq(mode_len).mean() >= 0.80) and (4 <= mode_len <= 10):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def parse_number_like(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalise nombres FR/US.\"\"\"\n",
    "    x = s.astype(\"string\")\n",
    "    x = x.str.replace(\"\\u00A0\", \" \", regex=False)\n",
    "    x = x.str.replace(\" \", \"\", regex=False)\n",
    "    # retire . de milliers: 1.234,56 -> 1234,56 (mais ne touche pas aux décimales US .123)\n",
    "    x = x.str.replace(r\"(?<=\\d)\\.(?=\\d{3}(?:\\D|$))\", \"\", regex=True)\n",
    "    x = x.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "def parse_currency(series: pd.Series, col_name: str) -> tuple[pd.Series, bool]:\n",
    "    \"\"\"Détecte monnaie seulement si symbole/code présent dans les valeurs OU si nom le suggère.\"\"\"\n",
    "    s = series.astype(\"string\")\n",
    "    name_hint = any(h in strip_accents_lower(col_name or \"\") for h in CURRENCY_NAME_HINTS)\n",
    "    has_symbol = s.str.contains(CURRENCY_DETECT_VALUE_RE, na=False)\n",
    "    if not name_hint and has_symbol.mean() < 0.5:\n",
    "        return pd.Series(pd.NA, index=series.index, dtype=\"Float64\"), False\n",
    "\n",
    "    def _one(v):\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "            return np.nan\n",
    "        txt = str(v)\n",
    "        m = CURRENCY_PARSE_RE.search(txt)\n",
    "        if not m:\n",
    "            return parse_number_like(pd.Series([txt])).iloc[0]\n",
    "        num = m.group(\"num\") or \"\"\n",
    "        neg = bool(m.group(\"neg\"))\n",
    "        val = parse_number_like(pd.Series([num])).iloc[0]\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        return -val if neg or (\"(\" in txt and \")\" in txt) else val\n",
    "\n",
    "    out = s.map(_one).astype(\"Float64\")\n",
    "    return out, True\n",
    "\n",
    "def parse_percent(s: pd.Series, col_name: str) -> tuple[pd.Series, bool]:\n",
    "    \"\"\"Pourcentages '12,5%' ou '12.5%' ; si '%' absent, exige nom de col type rate/ratio/taux.\"\"\"\n",
    "    raw = s.astype(\"string\")\n",
    "    has_pct = raw.fillna(\"\").str.contains(r\"%\")\n",
    "    name_hint = any(h in strip_accents_lower(col_name or \"\") for h in (\"rate\", \"ratio\", \"taux\", \"pourcent\", \"pct\"))\n",
    "    if has_pct.mean() >= 0.6 or name_hint:\n",
    "        x = raw.str.replace(\"%\", \"\", regex=False)\n",
    "        vals = parse_number_like(x).astype(\"Float64\")\n",
    "        return vals, True\n",
    "    return pd.Series(pd.NA, index=s.index, dtype=\"Float64\"), False\n",
    "\n",
    "def try_parse_datetime(series: pd.Series, favor_day_first=True):\n",
    "    \"\"\"Essaie datetime (heure) puis date, double passe (jour/mois).\"\"\"\n",
    "    s = series.astype(\"string\")\n",
    "    time_ratio = s.str.contains(TIME_TOKEN_RE, na=False).mean() if len(s) > 0 else 0.0\n",
    "    dt1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=favor_day_first)\n",
    "    r1 = dt1.notna().mean() if len(s.dropna()) > 0 else 0.0\n",
    "    if r1 < RATIO_DATE_HARD:\n",
    "        dt2 = pd.to_datetime(s, errors=\"coerce\", dayfirst=not favor_day_first)\n",
    "        r2 = dt2.notna().mean() if len(s.dropna()) > 0 else 0.0\n",
    "        dt, rr = (dt2, r2) if r2 > r1 else (dt1, r1)\n",
    "    else:\n",
    "        dt, rr = dt1, r1\n",
    "    if rr >= RATIO_DATE_HARD or (rr >= RATIO_DATE_SOFT and any(h in strip_accents_lower(series.name or \"\") for h in DATE_NAME_HINTS)):\n",
    "        if time_ratio >= 0.20 or (dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.20:\n",
    "            return dt, \"DATETIME\"\n",
    "        return dt.dt.normalize(), \"DATE\"\n",
    "    return pd.Series(pd.NaT, index=series.index), None\n",
    "\n",
    "# ---------- Lecture Excel/CSV ----------\n",
    "def safe_read_excel_all_sheets(path: Path):\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\"):\n",
    "        engine = \"openpyxl\"\n",
    "    elif ext == \".xlsb\":\n",
    "        engine = \"pyxlsb\"\n",
    "    else:\n",
    "        raise ValueError(f\"Extension Excel non gérée: {ext}\")\n",
    "    log(f\"  -> Ouverture Excel ({engine})\")\n",
    "    t0, _ = step_time()\n",
    "    xls = pd.ExcelFile(path, engine=engine)\n",
    "    sheets = xls.sheet_names[:(MAX_SHEETS or len(xls.sheet_names))]\n",
    "    out = []\n",
    "    nrows = SCAN_MAX_ROWS if SCAN_MAX_ROWS is not None else None\n",
    "    for i, sheet in enumerate(sheets, 1):\n",
    "        ti, _ = step_time()\n",
    "        log(f\"    .. Lecture feuille {i}/{len(sheets)}: '{sheet}' nrows={nrows or 'ALL'}\")\n",
    "        df = pd.read_excel(xls, sheet_name=sheet, header=None, engine=engine, nrows=nrows)\n",
    "        _, dt = step_time(ti)\n",
    "        log(f\"       -> taille: {df.shape[0]}x{df.shape[1]} (en {dt:.2f}s)\")\n",
    "        out.append((sheet, df))\n",
    "    _, dt_total = step_time(t0)\n",
    "    log(f\"  -> Excel chargé en {dt_total:.2f}s\")\n",
    "    return out\n",
    "\n",
    "COMMON_SEPS = [\",\", \";\", \"\\t\", \"|\"]\n",
    "COMMON_ENCODINGS = [\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin-1\"]\n",
    "\n",
    "def guess_sep(first_line: str):\n",
    "    counts = {sep: first_line.count(sep) for sep in COMMON_SEPS}\n",
    "    sep = max(counts, key=counts.get)\n",
    "    return sep if counts[sep] > 0 else \",\"\n",
    "\n",
    "def read_csv_fast(path: Path):\n",
    "    t0, _ = step_time()\n",
    "    log(\"  -> Détection encodage/séparateur (rapide)\")\n",
    "    enc_used = None\n",
    "    sep_used = \",\"\n",
    "    tried = []\n",
    "    head = \"\"\n",
    "    for enc in COMMON_ENCODINGS:\n",
    "        try:\n",
    "            tried.append(enc)\n",
    "            with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n",
    "                head = f.readline()\n",
    "            enc_used = enc\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except Exception:\n",
    "            enc_used = \"latin-1\"\n",
    "            break\n",
    "    if enc_used is None:\n",
    "        enc_used = \"latin-1\"\n",
    "    if not head:\n",
    "        with open(path, \"r\", encoding=enc_used, errors=\"replace\") as f:\n",
    "            head = f.readline()\n",
    "    sep_used = guess_sep(head)\n",
    "    log(f\"     -> encodage: {enc_used} (essais={tried}); sep='{sep_used}'\")\n",
    "    nrows = SCAN_MAX_ROWS if SCAN_MAX_ROWS is not None else None\n",
    "    log(f\"  -> Lecture CSV nrows={nrows or 'ALL'}\")\n",
    "    df = pd.read_csv(path, sep=sep_used, encoding=enc_used, header=None, nrows=nrows)\n",
    "    _, dt = step_time(t0)\n",
    "    log(f\"     -> taille: {df.shape[0]}x{df.shape[1]} (en {dt:.2f}s)\")\n",
    "    return df\n",
    "\n",
    "# ---------- Pré-traitements & détection ----------\n",
    "def ensure_range_columns(df: pd.DataFrame):\n",
    "    if list(df.columns) != list(range(df.shape[1])):\n",
    "        df = df.copy()\n",
    "        df.columns = list(range(df.shape[1]))\n",
    "    return df\n",
    "\n",
    "def detect_blocks(df: pd.DataFrame):\n",
    "    t0, _ = step_time()\n",
    "    df = ensure_range_columns(df)\n",
    "    if df.empty:\n",
    "        log(\"    .. Aucun bloc (df vide)\")\n",
    "        return []\n",
    "    row_nnz = df.notna().sum(axis=1).to_numpy()\n",
    "    tab = row_nnz >= MIN_COLS\n",
    "    blocks = []\n",
    "    if tab.any():\n",
    "        padded = np.r_[False, tab, False]\n",
    "        starts = np.where((~padded[:-1]) & (padded[1:]))[0]\n",
    "        ends = np.where((padded[:-1]) & (~padded[1:]))[0] - 1\n",
    "        for s, e in zip(starts, ends):\n",
    "            if (e - s + 1) >= MIN_CONSEC_ROWS:\n",
    "                blocks.append((df.index[s], df.index[e]))\n",
    "                if QUICK_MODE and MAX_TABLES_PER_SHEET and len(blocks) >= MAX_TABLES_PER_SHEET:\n",
    "                    break\n",
    "    _, dt = step_time(t0)\n",
    "    log(f\"    .. Blocs tabulaires détectés (vertical/lignes): {len(blocks)} (en {dt:.2f}s)\")\n",
    "    return blocks\n",
    "\n",
    "def split_by_vertical_gaps(df: pd.DataFrame):\n",
    "    if df.empty or df.shape[1] <= 1:\n",
    "        return [(df, 0, max(df.shape[1]-1, 0))] if df.shape[1] else []\n",
    "    dens = df.notna().mean(axis=0).to_numpy()\n",
    "    used = dens >= COL_DENSITY_THRESHOLD\n",
    "    if ALLOW_SMALL_GAPS > 0:\n",
    "        n = len(used)\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            if not used[i]:\n",
    "                j = i\n",
    "                while j < n and not used[j]:\n",
    "                    j += 1\n",
    "                gap_len = j - i\n",
    "                left_used = (i - 1 >= 0 and used[i - 1])\n",
    "                right_used = (j < n and used[j])\n",
    "                if left_used and right_used and gap_len <= ALLOW_SMALL_GAPS:\n",
    "                    used[i:j] = True\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "    segments = []\n",
    "    padded = np.r_[False, used, False]\n",
    "    starts = np.where((~padded[:-1]) & (padded[1:]))[0]\n",
    "    ends = np.where((padded[:-1]) & (~padded[1:]))[0] - 1\n",
    "    for s, e in zip(starts, ends):\n",
    "        if (e - s + 1) >= MIN_COL_RUN:\n",
    "            sub = df.iloc[:, s:e + 1]\n",
    "            segments.append((sub, s, e))\n",
    "    return segments if segments else [(df, 0, df.shape[1] - 1)]\n",
    "\n",
    "# ---------- Colonnes ----------\n",
    "def choose_header_row(block: pd.DataFrame):\n",
    "    best_idx, best_score = None, -1\n",
    "    limit = min(len(block), HEADER_SCAN_DEPTH)\n",
    "    for i in range(limit):\n",
    "        row = block.iloc[i]\n",
    "        vals = row.tolist()\n",
    "        non_empty = sum(pd.notna(v) and str(v).strip() != \"\" for v in vals)\n",
    "        texty = 0\n",
    "        for v in vals:\n",
    "            s = str(v).strip() if pd.notna(v) else \"\"\n",
    "            if s and not s.lower().startswith(\"unnamed\") and re.search(r\"[A-Za-zÀ-ÿ]\", s):\n",
    "                texty += 1\n",
    "        score = non_empty * 2 + texty\n",
    "        if score > best_score:\n",
    "            best_score, best_idx = score, i\n",
    "    return best_idx or 0\n",
    "\n",
    "def clean_columns(vals):\n",
    "    cols, seen = [], {}\n",
    "    for v in vals:\n",
    "        s = strip_accents_lower(v).replace(\"\\n\", \" \")\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip(\" -_\")\n",
    "        if not s or s.startswith(\"unnamed\"):\n",
    "            s = \"col\"\n",
    "        s = re.sub(r\"[^a-z0-9_ ]\", \"\", s)\n",
    "        s = re.sub(r\"\\s+\", \"_\", s).strip(\"_\") or \"col\"\n",
    "        if s in seen:\n",
    "            seen[s] += 1\n",
    "            s = f\"{s}_{seen[s]}\"\n",
    "        else:\n",
    "            seen[s] = 1\n",
    "        cols.append(s)\n",
    "    return cols\n",
    "\n",
    "def prune_columns(df: pd.DataFrame, header_keep: set[str]) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    keep = list(header_keep)\n",
    "    n = len(df)\n",
    "    for c in df.columns:\n",
    "        if c in header_keep:\n",
    "            continue\n",
    "        if not is_generic_colname(c):\n",
    "            keep.append(c)\n",
    "            continue\n",
    "        nnz = df[c].notna().sum()\n",
    "        if nnz >= max(MIN_NON_NULL_ABS, int(n * MIN_NON_NULL_RATIO)):\n",
    "            keep.append(c)\n",
    "    keep_ordered = [c for c in df.columns if c in keep]\n",
    "    return df[keep_ordered].copy()\n",
    "\n",
    "# ---------- Inférence de types ----------\n",
    "def infer_and_cast_column(s: pd.Series, col_name: str) -> tuple[pd.Series, str]:\n",
    "    \"\"\"\n",
    "    Retourne (Serie castée, type_str)\n",
    "    type_str ∈ {\"DATE\",\"DATETIME\",\"INT\",\"FLOAT\",\"CURRENCY\",\"PERCENT\",\"BOOL\",\"CODE\",\"STRING\"}\n",
    "    \"\"\"\n",
    "    if not INFER_TYPES:\n",
    "        return s.astype(\"string\"), \"STRING\"\n",
    "\n",
    "    s_obj = s.astype(\"string\")\n",
    "    non_empty = s_obj.dropna().astype(str).str.strip()\n",
    "    if len(non_empty) == 0:\n",
    "        return s_obj.astype(\"string\"), \"STRING\"\n",
    "\n",
    "    # 0) codes\n",
    "    if looks_like_code(non_empty):\n",
    "        return s_obj.astype(\"string\"), \"CODE\"\n",
    "\n",
    "    # 1) dates/datetimes\n",
    "    dt_vals, dt_tag = try_parse_datetime(s_obj)\n",
    "    if dt_tag is not None:\n",
    "        return (dt_vals, dt_tag)\n",
    "\n",
    "    # 2) bool : seulement si quasi pur ET pas numérique dominant\n",
    "    uniq_norm = pd.Series(non_empty).map(strip_accents_lower).dropna().unique().tolist()\n",
    "    if 1 <= len(set(uniq_norm)) <= 3:\n",
    "        mapped = s_obj.map(strip_accents_lower)\n",
    "        def map_bool(x):\n",
    "            if x in BOOL_TRUE:\n",
    "                return True\n",
    "            if x in BOOL_FALSE:\n",
    "                return False\n",
    "            return pd.NA\n",
    "        mb = mapped.map(map_bool)\n",
    "        frac_bool = mb.notna().sum() / max(1, mapped.notna().sum())\n",
    "        # si la colonne est massivement numérique, privilégier le nombre\n",
    "        as_num_probe = parse_number_like(s_obj)\n",
    "        frac_num = as_num_probe.notna().sum() / max(1, s_obj.notna().sum())\n",
    "        if frac_bool >= RATIO_STRICT_BOOL and frac_num < 0.9:\n",
    "            return mb.astype(\"boolean\"), \"BOOL\"\n",
    "\n",
    "    # 3) currency stricte (symbole/code dans valeurs ou nom parlant)\n",
    "    cur_vals, is_cur = parse_currency(s_obj, col_name)\n",
    "    if is_cur:\n",
    "        success = cur_vals.notna().mean() if s_obj.notna().mean() > 0 else 0.0\n",
    "        if success >= RATIO_NUM:\n",
    "            return cur_vals, \"CURRENCY\"\n",
    "\n",
    "    # 4) percent\n",
    "    pct_vals, is_pct = parse_percent(s_obj, col_name)\n",
    "    if is_pct:\n",
    "        success = pct_vals.notna().mean() if s_obj.notna().mean() > 0 else 0.0\n",
    "        if success >= 0.70:\n",
    "            return pct_vals, \"PERCENT\"\n",
    "\n",
    "    # 5) numérique (FR/US)\n",
    "    as_num = parse_number_like(s_obj)\n",
    "    num_ratio = as_num.notna().sum() / max(1, s_obj.notna().sum())\n",
    "    if num_ratio >= RATIO_NUM:\n",
    "        as_int = as_num.dropna()\n",
    "        if len(as_int) == 0:\n",
    "            return as_num.astype(\"Float64\"), \"FLOAT\"\n",
    "        if (as_int % 1 == 0).all():\n",
    "            return as_num.astype(\"Int64\"), \"INT\"\n",
    "        else:\n",
    "            return as_num.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 6) texte\n",
    "    return s_obj.astype(\"string\"), \"STRING\"\n",
    "\n",
    "def infer_types_df(df: pd.DataFrame):\n",
    "    t0, _ = step_time()\n",
    "    out = df.copy()\n",
    "    schema = {}\n",
    "    for c in out.columns:\n",
    "        out[c], t = infer_and_cast_column(out[c], c)\n",
    "        schema[c] = t\n",
    "    _, dt = step_time(t0)\n",
    "    log(f\"    .. Inférence types: {len(out.columns)} colonnes (en {dt:.2f}s, activée={INFER_TYPES})\")\n",
    "    return out, schema\n",
    "\n",
    "def show_schema(df: pd.DataFrame, schema: dict):\n",
    "    print(\"\\n=== schema ===\", flush=True)\n",
    "    for c in df.columns:\n",
    "        t = schema.get(c, str(df[c].dtype)).upper()\n",
    "        print(f\"{c}: {t}\", flush=True)\n",
    "\n",
    "# ---------- Mapping types -> PostgreSQL ----------\n",
    "def pg_type_for(tag: str) -> str:\n",
    "    tag = (tag or \"STRING\").upper()\n",
    "    if tag == \"DATE\":\n",
    "        return \"date\"\n",
    "    if tag == \"DATETIME\":\n",
    "        return \"timestamp without time zone\"\n",
    "    if tag == \"INT\":\n",
    "        return \"integer\"\n",
    "    if tag in (\"FLOAT\", \"PERCENT\", \"CURRENCY\"):\n",
    "        return \"double precision\"\n",
    "    if tag == \"BOOL\":\n",
    "        return \"boolean\"\n",
    "    # CODE/STRING\n",
    "    return \"text\"\n",
    "\n",
    "def write_pg_ddl(table_name: str, df: pd.DataFrame, schema: dict):\n",
    "    if not EXPORT_PG_DDL:\n",
    "        return\n",
    "    EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    pg_table = snake_id(table_name)\n",
    "    lines = []\n",
    "    for c in df.columns:\n",
    "        col = snake_id(str(c))\n",
    "        pgt = pg_type_for(schema.get(c, \"STRING\"))\n",
    "        lines.append(f'\"{col}\" {pgt}')\n",
    "    ddl = f'CREATE TABLE \"{pg_table}\" (\\n  ' + \",\\n  \".join(lines) + \"\\n);\"\n",
    "    (EXPORT_DIR / f\"{pg_table}.sql\").write_text(ddl, encoding=\"utf-8\")\n",
    "\n",
    "# ---------- Extraction d'une table ----------\n",
    "def carve_table_from_block(df_block: pd.DataFrame):\n",
    "    if df_block.empty:\n",
    "        return None\n",
    "    # header\n",
    "    h_rel = choose_header_row(df_block)\n",
    "    cols = clean_columns(df_block.iloc[h_rel].tolist())\n",
    "    data = df_block.iloc[h_rel + 1:].copy()\n",
    "    if data.empty:\n",
    "        return None\n",
    "    data.columns = cols\n",
    "\n",
    "    # couper après trop de lignes vides consécutives\n",
    "    empty_run = 0\n",
    "    cut_idx = data.index[-1]\n",
    "    for idx in data.index:\n",
    "        if data.loc[idx].isna().all():\n",
    "            empty_run += 1\n",
    "            if empty_run >= STOP_EMPTY_RUN:\n",
    "                cut_idx = max(data.index[0], idx - STOP_EMPTY_RUN)\n",
    "                break\n",
    "        else:\n",
    "            empty_run = 0\n",
    "    data = data.loc[:cut_idx]\n",
    "    data = data[data.notna().sum(axis=1) >= MIN_COLS]\n",
    "    if data.empty:\n",
    "        return None\n",
    "\n",
    "    header_keep = {c for c in cols if not is_generic_colname(c)}\n",
    "    data = prune_columns(data, header_keep)\n",
    "    if data.empty:\n",
    "        return None\n",
    "\n",
    "    data, schema = infer_types_df(data)\n",
    "\n",
    "    # mise en forme ISO pour PostgreSQL\n",
    "    for c, t in schema.items():\n",
    "        if t == \"DATE\":\n",
    "            try:\n",
    "                data[c] = pd.to_datetime(data[c], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        elif t == \"DATETIME\":\n",
    "            try:\n",
    "                data[c] = pd.to_datetime(data[c], errors=\"coerce\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        elif t == \"BOOL\":\n",
    "            data[c] = data[c].map(lambda x: True if x is True else (False if x is False else pd.NA))\n",
    "\n",
    "    return data.reset_index(drop=True), schema\n",
    "\n",
    "# ---------- Trouver tables dans une FEUILLE ----------\n",
    "def find_tables_in_sheet(df_raw: pd.DataFrame):\n",
    "    print(\"  -> Détection des blocs tabulaires…\", flush=True)\n",
    "    df_raw = ensure_range_columns(df_raw)\n",
    "    if SCAN_MAX_ROWS is not None and len(df_raw) > SCAN_MAX_ROWS:\n",
    "        df_raw = df_raw.iloc[:SCAN_MAX_ROWS, :]\n",
    "        log(f\"     (tronqué à {SCAN_MAX_ROWS} lignes)\")\n",
    "\n",
    "    segments = split_by_vertical_gaps(df_raw)\n",
    "    if MAX_SEGMENTS_PER_SHEET:\n",
    "        segments = segments[:MAX_SEGMENTS_PER_SHEET]\n",
    "    log(f\"  -> Segments verticaux: {len(segments)}\")\n",
    "\n",
    "    tables = []\n",
    "    table_count = 0\n",
    "    for seg_idx, (seg_df, c0, c1) in enumerate(segments, 1):\n",
    "        log(f\"  -> Segment {seg_idx}: cols {c0}..{c1} (shape {seg_df.shape[0]}x{seg_df.shape[1]})\")\n",
    "        if seg_df.empty:\n",
    "            continue\n",
    "        blocks = detect_blocks(seg_df)\n",
    "        for k, (start, end) in enumerate(blocks, 1):\n",
    "            log(f\"     -> Carve table {k}/{len(blocks)} dans segment {seg_idx} (rows {start}..{end})\")\n",
    "            block = seg_df.loc[start:end, :]\n",
    "            carved = carve_table_from_block(block)\n",
    "            if carved is None:\n",
    "                log(\"        .. ignoré (vide après carve)\")\n",
    "                continue\n",
    "            table, schema = carved\n",
    "            if table is not None and table.shape[1] >= MIN_COLS and table.shape[0] >= 1:\n",
    "                tables.append((seg_idx, k, table, schema))\n",
    "                table_count += 1\n",
    "                log(f\"        .. table retenue: {table.shape[0]}x{table.shape[1]}\")\n",
    "                if QUICK_MODE and MAX_TABLES_PER_SHEET and table_count >= MAX_TABLES_PER_SHEET:\n",
    "                    log(\"        .. limite de tables atteinte (mode rapide)\")\n",
    "                    return tables\n",
    "    log(f\"  -> Tables retenues sur la feuille: {len(tables)}\")\n",
    "    return tables\n",
    "\n",
    "# ---------- Par fichier ----------\n",
    "def process_file(path: Path):\n",
    "    results = []\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\", \".xlsb\"):\n",
    "        for sheet, df_raw in safe_read_excel_all_sheets(path):\n",
    "            log(f\"-- Feuille: {sheet} | taille {df_raw.shape[0]}x{df_raw.shape[1]}\")\n",
    "            for (seg_i, blk_i, t, sc) in find_tables_in_sheet(df_raw):\n",
    "                results.append((sheet, seg_i, blk_i, t, sc))\n",
    "    elif ext == \".csv\":\n",
    "        df_raw = read_csv_fast(path)\n",
    "        log(f\"-- CSV lu | taille {df_raw.shape[0]}x{df_raw.shape[1]}\")\n",
    "        for (seg_i, blk_i, t, sc) in find_tables_in_sheet(df_raw):\n",
    "            results.append((None, seg_i, blk_i, t, sc))\n",
    "    else:\n",
    "        raise ValueError(f\"Extension non gérée: {ext}\")\n",
    "    return results\n",
    "\n",
    "def export_table(df: pd.DataFrame, name: str, schema: dict):\n",
    "    if EXPORT_AS is None and not EXPORT_PG_DDL:\n",
    "        return\n",
    "    EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    base = snake_id(name)\n",
    "    if EXPORT_AS == \"csv\":\n",
    "        # format compatible PG: séparateur virgule, décimales en '.', bool true/false\n",
    "        df.to_csv(EXPORT_DIR / f\"{base}.csv\", index=False)\n",
    "    elif EXPORT_AS == \"parquet\":\n",
    "        df.to_parquet(EXPORT_DIR / f\"{base}.parquet\", index=False)\n",
    "    if EXPORT_PG_DDL:\n",
    "        write_pg_ddl(base, df, schema)\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    print(\"=== data header: folder info ===\", flush=True)\n",
    "    print(f\"path: {ROOT_DIR.resolve()}\", flush=True)\n",
    "\n",
    "    files = []\n",
    "    for pat in (\"*.xlsx\", \"*.xlsm\", \"*.xlsb\", \"*.csv\"):\n",
    "        files += [p for p in ROOT_DIR.rglob(pat) if p.is_file() and not p.name.startswith(\"~$\")]\n",
    "    files = sorted(files)\n",
    "    if MAX_FILES:\n",
    "        files = files[:MAX_FILES]\n",
    "    print(f\"files_found: {len(files)}\", flush=True)\n",
    "\n",
    "    print(\"\\n=== config ===\", flush=True)\n",
    "    print(f\"QUICK_MODE={QUICK_MODE} | INFER_TYPES={INFER_TYPES} | VERBOSE={VERBOSE}\", flush=True)\n",
    "    print(f\"SCAN_MAX_ROWS={SCAN_MAX_ROWS} | MAX_FILES={MAX_FILES} | MAX_SHEETS={MAX_SHEETS} | MAX_TABLES_PER_SHEET={MAX_TABLES_PER_SHEET}\", flush=True)\n",
    "    print(f\"COL_DENSITY_THRESHOLD={COL_DENSITY_THRESHOLD} | MIN_COL_RUN={MIN_COL_RUN} | ALLOW_SMALL_GAPS={ALLOW_SMALL_GAPS} | MAX_SEGMENTS_PER_SHEET={MAX_SEGMENTS_PER_SHEET}\", flush=True)\n",
    "    print(f\"EXPORT_AS={EXPORT_AS} | EXPORT_DIR={EXPORT_DIR} | EXPORT_PG_DDL={EXPORT_PG_DDL}\", flush=True)\n",
    "\n",
    "    if not files:\n",
    "        print(\"[warn] Aucun fichier trouvé.\", flush=True)\n",
    "        return\n",
    "\n",
    "    total_tables = 0\n",
    "    total_sheets = 0\n",
    "\n",
    "    for f in files:\n",
    "        print(\"\\n=== file ===\", flush=True)\n",
    "        print(f\"{f.name}  ({f.resolve()})\", flush=True)\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            tables = process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Lecture échouée pour {f.name}: {e}\", flush=True)\n",
    "            continue\n",
    "\n",
    "        sheets_in_file = len({s for (s, *_rest) in tables if s is not None})\n",
    "        total_sheets += sheets_in_file\n",
    "        total_tables += len(tables)\n",
    "\n",
    "        if not tables:\n",
    "            print(\"[info] Aucune table détectée\", flush=True)\n",
    "        else:\n",
    "            file_id = snake_id(f.stem)\n",
    "            for (sheet, seg_i, blk_i, df, schema) in tables:\n",
    "                sheet_id = snake_id(sheet or \"sheet\")\n",
    "                table_idx = f\"{seg_i}_{blk_i}\"\n",
    "                table_name = f\"{file_id}.{sheet_id}.table_{table_idx}\"\n",
    "                print(f\"\\n--- table detected ---\", flush=True)\n",
    "                print(f\"name: {table_name}\", flush=True)\n",
    "                print(f\"rows: {len(df)} | cols: {df.shape[1]}\", flush=True)\n",
    "                print(\"columns:\", \", \".join(map(str, df.columns.tolist())), flush=True)\n",
    "                show_schema(df, schema)\n",
    "                export_table(df, table_name, schema)\n",
    "                if SHOW_SAMPLES:\n",
    "                    with pd.option_context(\"display.max_columns\", 80, \"display.width\", 200):\n",
    "                        print(\"\\n=== sample (top 8) ===\", flush=True)\n",
    "                        print(df.head(8), flush=True)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"[info] Temps fichier: {t1 - t0:.2f}s\", flush=True)\n",
    "\n",
    "    print(\"\\n=== done ===\", flush=True)\n",
    "    print(f\"Tables: {total_tables} | Feuilles (approx): {total_sheets} | Fichiers: {len(files)}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[info] Interrompu par l'utilisateur.\", flush=True)\n",
    "        sys.exit(130)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6486e77-fdd0-49e0-90c2-094615d930e2",
   "metadata": {},
   "source": [
    "# Connexion BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa857a7-7626-475a-8915-0f79ef1f611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"sqlalchemy>=2\" psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae207269-9650-4173-81eb-b44a74f42e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, connecté.\n",
      "PostgreSQL 17.6 (Debian 17.6-1.pgdg12+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14+deb12u1) 12.2.0, 64-bit\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "import pandas as pd\n",
    "\n",
    "PG_HOST = \"dpg-d3jq6apr0fns738f81i0-a.frankfurt-postgres.render.com\"\n",
    "PG_PORT = 5432\n",
    "PG_DB   = \"aerotec_datawarehouse\"\n",
    "PG_USER = \"aerotec_datawarehouse_user\"\n",
    "PG_PASS = \"LHTYZJ3aUDI8IeylbA1SZs9M9TsKQ4To\"\n",
    "\n",
    "conn_str = (\n",
    "    f\"postgresql+psycopg2://{PG_USER}:{quote_plus(PG_PASS)}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    ")\n",
    "engine = create_engine(conn_str, connect_args={\"sslmode\": \"require\"}, pool_pre_ping=True)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(\"OK, connecté.\")\n",
    "    print(conn.execute(text(\"SELECT version();\")).scalar())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341020a-4972-418e-b2a9-9c904a1541a7",
   "metadata": {},
   "source": [
    " # CAS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127f132-cfba-4b30-b958-7ad1c70d19a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e0087b-c5bc-40e3-9c01-cd4dfaee5ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feuille: PDG -> trigrammes.pdg ---\n",
      "rows=20 | cols=4\n",
      "Colonnes: edition, date, motif, redacteur\n",
      "\n",
      "Schema détecté:\n",
      "  - edition: INT  ->  integer\n",
      "  - date: DATE  ->  date\n",
      "  - motif: STRING  ->  text\n",
      "  - redacteur: STRING  ->  text\n",
      "\n",
      "Sample (top 8):\n",
      "    edition        date                                              motif    redacteur\n",
      "9         1  2024-06-10                                           Création  S. BELMONTE\n",
      "10        2  2024-09-16                           Ajout nouveaux arrivants    Y. RAGEOT\n",
      "11        3  2024-10-21                                    Ajout couturier    Y. RAGEOT\n",
      "12        4  2024-10-28  Mise à jour des dates d'entrée \n",
      "Ajout des habi...    Y. RAGEOT\n",
      "13        5  2024-12-16  Ajout nouvelle arrivante - 1 personne - Feriel...  F.BOULHABEL\n",
      "14        6  2024-12-18                              Départ de Yann RAGEOT  S. BELMONTE\n",
      "15        7  2024-12-19  Ajout de nouveaux arrivants - 4 personnes - Fr...  F.BOULHABEL\n",
      "16        8  2024-12-20  Ajout de nouvel arrivant - 1 personne - Loris ...  F.BOULHABEL\n",
      "\n",
      "--- Feuille: Liste -> trigrammes.liste ---\n",
      "rows=201 | cols=6\n",
      "Colonnes: nom_prenom, trig, personnel_int_ext, site, date_d_attribution, date_de_retrait\n",
      "\n",
      "Schema détecté:\n",
      "  - nom_prenom: STRING  ->  text\n",
      "  - trig: STRING  ->  text\n",
      "  - personnel_int_ext: STRING  ->  text\n",
      "  - site: STRING  ->  text\n",
      "  - date_d_attribution: DATE  ->  date\n",
      "  - date_de_retrait: STRING  ->  text\n",
      "\n",
      "Sample (top 8):\n",
      "                  nom_prenom trig personnel_int_ext site date_d_attribution      date_de_retrait\n",
      "9               ABAT Nicolas  ABT           Interne  AEC         2025-02-17                 <NA>\n",
      "10   ABDELLAOUI Schéhérazade  ADI           Interne  AEB         2025-05-05                 <NA>\n",
      "11          ADJEROUD Bastian  BAD           Interne  AEX         2024-05-21                 <NA>\n",
      "12              AGREBI Lilia  ARI           Interne  AEC         2025-02-24                 <NA>\n",
      "13          ALBERTON YANNICK  YAL           Interne  AEC         2018-11-05  2021-09-10 00:00:00\n",
      "14  ALIAS Séverine née HERAL  ALS           Interne  AEG         2024-05-21                 <NA>\n",
      "15      ALLARD Jean-François  ALD           Externe  AEC                NaN                 <NA>\n",
      "16               ALLARD Yann  YAD           Interne  AEC         2024-06-03                 <NA>\n",
      "\n",
      "Done. Feuilles analysées: 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, unicodedata, warnings\n",
    "\n",
    "FILE_PATH = r\"C:\\globasoft\\aerotech\\fic\\fichier qualité des trigrammes.xlsx\"\n",
    "SHOW_SAMPLE = True\n",
    "\n",
    "# --- utils courts ---\n",
    "def strip_accents_lower(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def snake_id(s):\n",
    "    s = strip_accents_lower(s)\n",
    "    s = re.sub(r\"[\\s\\.\\-]+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"col\"\n",
    "\n",
    "def parse_number_like(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(\"string\").str.replace(\"\\u00A0\",\" \", regex=False).str.replace(\" \",\"\", regex=False)\n",
    "    x = x.str.replace(r\"(?<=\\d)\\.(?=\\d{3}(?:\\D|$))\",\"\", regex=True).str.replace(\",\",\".\", regex=False)\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "# --- header smarter ---\n",
    "HEADER_KEYWORDS = (\n",
    "    \"date\", \"motif\", \"rédact\", \"redact\", \"trig\", \"nom\", \"prénom\", \"prenom\",\n",
    "    \"personnel\", \"site\", \"retrait\", \"édition\", \"edition\"\n",
    ")\n",
    "\n",
    "def choose_header_row(df: pd.DataFrame, scan=25) -> int:\n",
    "    limit = min(len(df), scan)\n",
    "    header_candidate_idx = None\n",
    "    header_candidate_hits = -1\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        hits = 0\n",
    "        for v in row:\n",
    "            t = strip_accents_lower(v)\n",
    "            if t and any(k in t for k in HEADER_KEYWORDS):\n",
    "                hits += 1\n",
    "        if hits >= 2 and hits > header_candidate_hits:\n",
    "            header_candidate_hits = hits\n",
    "            header_candidate_idx = i\n",
    "    if header_candidate_idx is not None:\n",
    "        return header_candidate_idx\n",
    "    best, idx = -1, 0\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        non_empty = row.map(lambda x: x.strip()!=\"\").sum()\n",
    "        texty = row.map(lambda x: bool(re.search(r\"[A-Za-zÀ-ÿ]\", x))).sum()\n",
    "        score = non_empty*2 + texty\n",
    "        if score>best: best, idx = score, i\n",
    "    return idx\n",
    "\n",
    "def normalize_columns(header_vals):\n",
    "    cols, seen = [], {}\n",
    "    for v in header_vals:\n",
    "        s = snake_id(v) if (v is not None and str(v).strip()!=\"\") else \"col\"\n",
    "        seen[s] = seen.get(s,0)+1\n",
    "        cols.append(s if seen[s]==1 else f\"{s}_{seen[s]}\")\n",
    "    return cols\n",
    "\n",
    "def drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    blank_cols = [c for c in df.columns if df[c].dropna().astype(str).str.strip().eq(\"\").all()]\n",
    "    if blank_cols:\n",
    "        df = df.drop(columns=blank_cols)\n",
    "    return df\n",
    "\n",
    "# --- inférence minimale robuste ---\n",
    "MAP_TRUE  = {\"true\",\"vrai\",\"oui\",\"yes\",\"1\",\"y\",\"o\"}\n",
    "MAP_FALSE = {\"false\",\"faux\",\"non\",\"no\",\"0\",\"n\"}\n",
    "\n",
    "# Regex SANS groupes capturants -> évite le warning pandas\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"(?:\\d{4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,2})|(?:\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4})|\"\n",
    "    r\"(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|janv|févr|fevr|avr|mai|juin|juil|sept|oct|nov|d[ée]c)\",\n",
    "    re.I\n",
    ")\n",
    "ISO_DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:[ T]\\d{2}:\\d{2}:\\d{2})?$\")\n",
    "COLNAME_DATE_HINTS = (\"date\", \"dt\", \"heure\", \"time\")\n",
    "\n",
    "def infer_col(s: pd.Series):\n",
    "    s = s.copy()\n",
    "    ss = s.astype(\"string\").str.strip()\n",
    "    ss = ss.where(~ss.fillna(\"\").eq(\"0\"), pd.NA)  # éviter 0 -> 1970-01-01\n",
    "\n",
    "    # 0) ISO détecté -> on parse d'abord en ISO (dayfirst=False) pour éviter le warning\n",
    "    iso_mask = ss.fillna(\"\").str.match(ISO_DATE_RE)\n",
    "    if iso_mask.mean() >= 0.5:\n",
    "        dt_iso = pd.to_datetime(ss.where(iso_mask), errors=\"coerce\", dayfirst=False)\n",
    "        ok = dt_iso.notna().mean() if len(ss) else 0.0\n",
    "        if ok >= 0.5:\n",
    "            has_time = (dt_iso.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                dt_iso.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else dt_iso.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 1) Excel serial dates plausibles\n",
    "    as_num = pd.to_numeric(ss, errors=\"coerce\")\n",
    "    frac_num = as_num.notna().mean() if len(ss) else 0.0\n",
    "    if frac_num >= 0.9:\n",
    "        mask = as_num.between(60, 2950000)  # >=60 = après 1900-03-01\n",
    "        parsed = pd.to_datetime(as_num.where(mask), unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "        ok = parsed.notna().mean() if len(ss) else 0.0\n",
    "        if ok >= 0.7:\n",
    "            has_time = (parsed.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                parsed.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else parsed.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 2) tokens de date ou nom de colonne “datey”\n",
    "    colname_hint = any(h in strip_accents_lower(s.name or \"\") for h in COLNAME_DATE_HINTS)\n",
    "    date_token_ratio = ss.fillna(\"\").str.contains(DATE_TOKEN_RE, na=False).mean()\n",
    "    looks_datey = colname_hint or (date_token_ratio >= 0.30)\n",
    "    if looks_datey:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "            dt1 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=True,  cache=True)\n",
    "            dt2 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=False, cache=True)\n",
    "        dt = dt2 if dt2.notna().sum() > dt1.notna().sum() else dt1\n",
    "        ok = dt.notna().mean() if len(ss) else 0.0\n",
    "        # seuil plus bas si le NOM de colonne suggère une date (ex: 'date_de_retrait')\n",
    "        ok_thresh = 0.50 if colname_hint else 0.70\n",
    "        if ok >= ok_thresh:\n",
    "            has_time = (dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 3) bool strict\n",
    "    mb = ss.str.lower().map(lambda x: True if x in MAP_TRUE else (False if x in MAP_FALSE else pd.NA))\n",
    "    if (mb.notna().mean() if len(ss) else 0) >= 0.98:\n",
    "        return mb.astype(\"boolean\"), \"BOOL\"\n",
    "\n",
    "    # 4) nombre\n",
    "    nums = parse_number_like(ss)\n",
    "    if (nums.notna().mean() if len(ss.dropna()) else 0) >= 0.85:\n",
    "        nz = nums.dropna()\n",
    "        if len(nz) and (np.mod(nz, 1) == 0).all():\n",
    "            return nums.astype(\"Int64\"), \"INT\"\n",
    "        return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 5) texte\n",
    "    return ss.astype(\"string\"), \"STRING\"\n",
    "\n",
    "def pg_type(tag: str) -> str:\n",
    "    return {\n",
    "        \"DATE\":\"date\", \"DATETIME\":\"timestamp without time zone\",\n",
    "        \"INT\":\"integer\", \"FLOAT\":\"double precision\", \"BOOL\":\"boolean\"\n",
    "    }.get(tag, \"text\")\n",
    "\n",
    "# --- main ---\n",
    "def main():\n",
    "    xlsx = Path(FILE_PATH)\n",
    "    if not xlsx.exists():\n",
    "        print(f\"[error] Fichier introuvable: {xlsx}\"); return\n",
    "\n",
    "    sheets = pd.read_excel(xlsx, sheet_name=None, engine=\"openpyxl\", header=None)\n",
    "\n",
    "    done = 0\n",
    "    for name, raw in sheets.items():\n",
    "        raw = raw.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "        if raw.empty: continue\n",
    "\n",
    "        h = choose_header_row(raw, scan=25)\n",
    "        cols = normalize_columns(raw.iloc[h].tolist())\n",
    "        df = raw.iloc[h+1:].copy()\n",
    "        df.columns = cols\n",
    "\n",
    "        df = df.dropna(how=\"all\")\n",
    "        df = drop_empty_columns(df)\n",
    "        if df.empty: \n",
    "            continue\n",
    "\n",
    "        schema = {}\n",
    "        for c in df.columns:\n",
    "            casted, tag = infer_col(df[c])\n",
    "            df[c] = casted\n",
    "            schema[c] = tag\n",
    "\n",
    "        base = f\"trigrammes.{snake_id(name)}\"\n",
    "        print(f\"\\n--- Feuille: {name} -> {base} ---\")\n",
    "        print(f\"rows={len(df)} | cols={df.shape[1]}\")\n",
    "        print(\"Colonnes:\", \", \".join(df.columns.astype(str)))\n",
    "        print(\"\\nSchema détecté:\")\n",
    "        for c in df.columns:\n",
    "            print(f\"  - {c}: {schema[c]}  ->  {pg_type(schema[c])}\")\n",
    "\n",
    "        if SHOW_SAMPLE:\n",
    "            with pd.option_context(\"display.max_columns\", 80, \"display.width\", 200):\n",
    "                print(\"\\nSample (top 8):\")\n",
    "                print(df.head(8))\n",
    "\n",
    "        done += 1\n",
    "\n",
    "    print(f\"\\nDone. Feuilles analysées: {done}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474b0f5-4622-41cf-8de5-e4ebe38094b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
