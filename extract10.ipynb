{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee57f4b-4e4a-42ae-9683-343774c348f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyxlsb\n",
    "pip install sqlalchemy psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6486e77-fdd0-49e0-90c2-094615d930e2",
   "metadata": {},
   "source": [
    "# Connexion BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa857a7-7626-475a-8915-0f79ef1f611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"sqlalchemy>=2\" psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae207269-9650-4173-81eb-b44a74f42e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Connecté à PostgreSQL Render\n",
      "PostgreSQL 17.6 (Debian 17.6-1.pgdg12+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14+deb12u1) 12.2.0, 64-bit\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "PG_HOST = \"dpg-d3jq6apr0fns738f81i0-a.frankfurt-postgres.render.com\"\n",
    "PG_PORT = 5432\n",
    "PG_DB   = \"aerotec_datawarehouse\"\n",
    "PG_USER = \"aerotec_datawarehouse_user\"\n",
    "PG_PASS = \"LHTYZJ3aUDI8IeylbA1SZs9M9TsKQ4To\"\n",
    "\n",
    "conn_str = (\n",
    "    f\"postgresql+psycopg2://{PG_USER}:{quote_plus(PG_PASS)}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    ")\n",
    "\n",
    "# Astuces de stabilité pour Render :\n",
    "# - sslmode=require (obligatoire)\n",
    "# - connect_timeout : évite de bloquer si le réseau/host ne répond pas\n",
    "# - keepalives_* : évite les coupures silencieuses d’idle\n",
    "engine = create_engine(\n",
    "    conn_str,\n",
    "    connect_args={\n",
    "        \"sslmode\": \"require\",\n",
    "        \"connect_timeout\": 5,\n",
    "        \"keepalives\": 1,\n",
    "        \"keepalives_idle\": 30,\n",
    "        \"keepalives_interval\": 10,\n",
    "        \"keepalives_count\": 3,\n",
    "    },\n",
    "    pool_pre_ping=True,     # ping avant réutilisation du pool\n",
    "    pool_recycle=1800,      # recycle connexions > 30 min\n",
    "    pool_size=5,\n",
    "    max_overflow=5,\n",
    ")\n",
    "\n",
    "# Test rapide de connectivité (à garder au début du main)\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        ver = conn.execute(text(\"select version();\")).scalar()\n",
    "        print(\"[ok] Connecté à PostgreSQL Render\")\n",
    "        print(ver)\n",
    "except OperationalError as e:\n",
    "    print(\"[error] Connexion PostgreSQL échouée:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341020a-4972-418e-b2a9-9c904a1541a7",
   "metadata": {},
   "source": [
    " # fichier qualité des trigrammes.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127f132-cfba-4b30-b958-7ad1c70d19a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e0087b-c5bc-40e3-9c01-cd4dfaee5ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Connecté à PostgreSQL\n",
      "PostgreSQL 17.6 (Debian 17.6-1.pgdg12+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14+deb12u1) 12.2.0, 64-bit\n",
      "\n",
      "--- Feuille: PDG -> public.fichier_qualite_des_trigrammes__pdg ---\n",
      "rows=20 | cols=4\n",
      "Colonnes: edition, date, motif, redacteur\n",
      "\n",
      "Schema détecté:\n",
      "  - edition: INT -> Integer\n",
      "  - date: DATE -> Date\n",
      "  - motif: STRING -> Text\n",
      "  - redacteur: STRING -> Text\n",
      "\n",
      "Sample (top 8):\n",
      "    edition       date                                              motif    redacteur\n",
      "9         1 2024-06-10                                           Création  S. BELMONTE\n",
      "10        2 2024-09-16                           Ajout nouveaux arrivants    Y. RAGEOT\n",
      "11        3 2024-10-21                                    Ajout couturier    Y. RAGEOT\n",
      "12        4 2024-10-28  Mise à jour des dates d'entrée \n",
      "Ajout des habi...    Y. RAGEOT\n",
      "13        5 2024-12-16  Ajout nouvelle arrivante - 1 personne - Feriel...  F.BOULHABEL\n",
      "14        6 2024-12-18                              Départ de Yann RAGEOT  S. BELMONTE\n",
      "15        7 2024-12-19  Ajout de nouveaux arrivants - 4 personnes - Fr...  F.BOULHABEL\n",
      "16        8 2024-12-20  Ajout de nouvel arrivant - 1 personne - Loris ...  F.BOULHABEL\n",
      "[ok] Inserted into public.fichier_qualite_des_trigrammes__pdg\n",
      "\n",
      "--- Feuille: Liste -> public.fichier_qualite_des_trigrammes__liste ---\n",
      "rows=201 | cols=6\n",
      "Colonnes: nom_prenom, trig, personnel_int_ext, site, date_d_attribution, date_de_retrait\n",
      "\n",
      "Schema détecté:\n",
      "  - nom_prenom: STRING -> Text\n",
      "  - trig: STRING -> Text\n",
      "  - personnel_int_ext: STRING -> Text\n",
      "  - site: STRING -> Text\n",
      "  - date_d_attribution: DATE -> Date\n",
      "  - date_de_retrait: DATE -> Date\n",
      "\n",
      "Sample (top 8):\n",
      "                  nom_prenom trig personnel_int_ext site date_d_attribution date_de_retrait\n",
      "9               ABAT Nicolas  ABT           Interne  AEC         2025-02-17             NaT\n",
      "10   ABDELLAOUI Schéhérazade  ADI           Interne  AEB         2025-05-05             NaT\n",
      "11          ADJEROUD Bastian  BAD           Interne  AEX         2024-05-21             NaT\n",
      "12              AGREBI Lilia  ARI           Interne  AEC         2025-02-24             NaT\n",
      "13          ALBERTON YANNICK  YAL           Interne  AEC         2018-11-05      2021-09-10\n",
      "14  ALIAS Séverine née HERAL  ALS           Interne  AEG         2024-05-21             NaT\n",
      "15      ALLARD Jean-François  ALD           Externe  AEC                NaT             NaT\n",
      "16               ALLARD Yann  YAD           Interne  AEC         2024-06-03             NaT\n",
      "[ok] Inserted into public.fichier_qualite_des_trigrammes__liste\n",
      "\n",
      "Done. Feuilles analysées & importées: 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, unicodedata, warnings\n",
    "\n",
    "# --- DB: SQLAlchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy import types as satypes\n",
    "from sqlalchemy.exc import OperationalError\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ================== CONFIG ================== #\n",
    "FILE_PATH = r\"C:\\globasoft\\aerotech\\fic\\fichier qualité des trigrammes.xlsx\"\n",
    "SHOW_SAMPLE = True\n",
    "TARGET_SCHEMA = \"public\"          # schéma cible\n",
    "IF_EXISTS_MODE = \"replace\"        # \"replace\" ou \"append\"\n",
    "\n",
    "# Forcer certains noms de colonnes en DATE/DATETIME (optionnel)\n",
    "FORCE_DATE_COLS = {\"date_d_attribution\", \"date_de_retrait\"}  # snake_case attendu\n",
    "\n",
    "# ================== ACCÈS DB ================== #\n",
    "REQUIRED_KEYS = (\"PG_HOST\",\"PG_PORT\",\"PG_DB\",\"PG_USER\",\"PG_PASS\")\n",
    "\n",
    "def _collect_pg_config():\n",
    "    \"\"\"Récupère les creds depuis le 1er bloc (globals) OU depuis l'env.\"\"\"\n",
    "    cfg = {}\n",
    "    g = globals()\n",
    "    # 1) variables déjà définies dans le 1er bloc ?\n",
    "    for k in REQUIRED_KEYS:\n",
    "        if k in g and g[k]:\n",
    "            cfg[k] = g[k]\n",
    "    # 2) sinon variables d'environnement ?\n",
    "    for k in REQUIRED_KEYS:\n",
    "        if k not in cfg or not cfg[k]:\n",
    "            v = os.getenv(k)\n",
    "            if v:\n",
    "                cfg[k] = v\n",
    "\n",
    "    # défauts pratiques si seul le host Render est utilisé\n",
    "    if \"PG_HOST\" not in cfg or not cfg[\"PG_HOST\"]:\n",
    "        cfg[\"PG_HOST\"] = \"dpg-d3jq6apr0fns738f81i0-a.frankfurt-postgres.render.com\"\n",
    "    if \"PG_PORT\" not in cfg or not cfg[\"PG_PORT\"]:\n",
    "        cfg[\"PG_PORT\"] = 5432\n",
    "    else:\n",
    "        cfg[\"PG_PORT\"] = int(cfg[\"PG_PORT\"])\n",
    "\n",
    "    missing = [k for k in REQUIRED_KEYS if k not in cfg or cfg[k] in (\"\", None)]\n",
    "    if missing:\n",
    "        raise RuntimeError(\n",
    "            \"Paramètres DB manquants: \"\n",
    "            + \", \".join(missing)\n",
    "            + \". Définis-les dans le 1er bloc (PG_*) ou via variables d'environnement.\"\n",
    "        )\n",
    "    return cfg\n",
    "\n",
    "def get_engine():\n",
    "    \"\"\"Réutilise l'engine existant si présent. Sinon, en crée un avec SSL Render.\"\"\"\n",
    "    g = globals()\n",
    "    if \"engine\" in g and g[\"engine\"] is not None:\n",
    "        return g[\"engine\"]\n",
    "\n",
    "    cfg = _collect_pg_config()\n",
    "    conn_str = (\n",
    "        f\"postgresql+psycopg2://{cfg['PG_USER']}:{quote_plus(cfg['PG_PASS'])}\"\n",
    "        f\"@{cfg['PG_HOST']}:{cfg['PG_PORT']}/{cfg['PG_DB']}\"\n",
    "    )\n",
    "    eng = create_engine(\n",
    "        conn_str,\n",
    "        connect_args={\n",
    "            \"sslmode\": \"require\",\n",
    "            \"connect_timeout\": 5,\n",
    "            \"keepalives\": 1,\n",
    "            \"keepalives_idle\": 30,\n",
    "            \"keepalives_interval\": 10,\n",
    "            \"keepalives_count\": 3,\n",
    "        },\n",
    "        pool_pre_ping=True,\n",
    "        pool_recycle=1800,\n",
    "        pool_size=5,\n",
    "        max_overflow=5,\n",
    "    )\n",
    "    return eng\n",
    "\n",
    "# ================== UTILS ================== #\n",
    "def strip_accents_lower(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def snake_id(s):\n",
    "    s = strip_accents_lower(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s.strip())           # normalise espaces\n",
    "    s = re.sub(r\"[\\s\\.\\-]+\", \"_\", s)             # espaces/points/tirets -> _\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)            # autres -> _\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"col\"\n",
    "    return s\n",
    "\n",
    "def pg_ident(name: str, maxlen=63) -> str:\n",
    "    \"\"\"PostgreSQL ident max 63 chars.\"\"\"\n",
    "    return name[:maxlen]\n",
    "\n",
    "def parse_number_like(s: pd.Series) -> pd.Series:\n",
    "    # gère espace normal, insécable et fine insécable\n",
    "    x = s.astype(\"string\")\\\n",
    "         .str.replace(\"\\u00A0\", \" \", regex=False)\\\n",
    "         .str.replace(\"\\u202F\", \" \", regex=False)\\\n",
    "         .str.replace(\" \", \"\", regex=False)\n",
    "    x = x.str.replace(r\"(?<=\\d)\\.(?=\\d{3}(?:\\D|$))\", \"\", regex=True)\\\n",
    "         .str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "# --- header smarter ---\n",
    "HEADER_KEYWORDS = (\n",
    "    \"date\", \"motif\", \"rédact\", \"redact\", \"trig\", \"nom\", \"prénom\", \"prenom\",\n",
    "    \"personnel\", \"site\", \"retrait\", \"édition\", \"edition\"\n",
    ")\n",
    "\n",
    "def choose_header_row(df: pd.DataFrame, scan=25) -> int:\n",
    "    limit = min(len(df), scan)\n",
    "    header_candidate_idx = None\n",
    "    header_candidate_hits = -1\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        hits = 0\n",
    "        for v in row:\n",
    "            t = strip_accents_lower(v)\n",
    "            if t and any(k in t for k in HEADER_KEYWORDS):\n",
    "                hits += 1\n",
    "        if hits >= 2 and hits > header_candidate_hits:\n",
    "            header_candidate_hits = hits\n",
    "            header_candidate_idx = i\n",
    "    if header_candidate_idx is not None:\n",
    "        return header_candidate_idx\n",
    "    best, idx = -1, 0\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        non_empty = row.map(lambda x: x.strip()!=\"\").sum()\n",
    "        texty = row.map(lambda x: bool(re.search(r\"[A-Za-zÀ-ÿ]\", x))).sum()\n",
    "        score = non_empty*2 + texty\n",
    "        if score>best: best, idx = score, i\n",
    "    return idx\n",
    "\n",
    "def normalize_columns(header_vals):\n",
    "    cols, seen = [], {}\n",
    "    for v in header_vals:\n",
    "        s = snake_id(v) if (v is not None and str(v).strip()!=\"\") else \"col\"\n",
    "        seen[s] = seen.get(s,0)+1\n",
    "        cols.append(s if seen[s]==1 else f\"{s}_{seen[s]}\")\n",
    "    return cols\n",
    "\n",
    "def drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    blank_cols = [c for c in df.columns\n",
    "                  if df[c].isna().all() or df[c].astype(str).str.strip().eq(\"\").all()]\n",
    "    return df.drop(columns=blank_cols) if blank_cols else df\n",
    "\n",
    "# --- inférence minimale robuste ---\n",
    "MAP_TRUE  = {\"true\",\"vrai\",\"oui\",\"yes\",\"1\",\"y\",\"o\"}\n",
    "MAP_FALSE = {\"false\",\"faux\",\"non\",\"no\",\"0\",\"n\"}\n",
    "\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"(?:\\d{4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,2})|(?:\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4})|\"\n",
    "    r\"(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|janv|févr|fevr|avr|mai|juin|juil|sept|oct|nov|d[ée]c)\",\n",
    "    re.I\n",
    ")\n",
    "ISO_DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:[ T]\\d{2}:\\d{2}:\\d{2})?$\")\n",
    "COLNAME_DATE_HINTS = (\"date\", \"dt\", \"heure\", \"time\")\n",
    "\n",
    "def infer_col(s: pd.Series):\n",
    "    s = s.copy()\n",
    "    ss = s.astype(\"string\").str.strip()\n",
    "    ss = ss.where(~ss.fillna(\"\").eq(\"0\"), pd.NA)  # éviter 0 -> 1970-01-01\n",
    "\n",
    "    # 🔒 Forçage par nom de colonne (date_de_retrait, date_d_attribution) sans warning\n",
    "    if snake_id(s.name) in FORCE_DATE_COLS:\n",
    "        val = ss.fillna(\"\")\n",
    "        iso_mask = val.str.match(ISO_DATE_RE)\n",
    "        if iso_mask.mean() >= 0.5:\n",
    "            # Majoritairement ISO -> pas de warning\n",
    "            dt = pd.to_datetime(ss, errors=\"coerce\", dayfirst=False)\n",
    "        else:\n",
    "            # Essaie les deux et garde le meilleur parse\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "                dt1 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=True,  cache=True)\n",
    "                dt2 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=False, cache=True)\n",
    "            dt = dt2 if dt2.notna().sum() > dt1.notna().sum() else dt1\n",
    "\n",
    "        valid_dt = dt.dropna()\n",
    "        has_time = (len(valid_dt) and (valid_dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2)\n",
    "        return (dt, \"DATETIME\" if has_time else \"DATE\")\n",
    "\n",
    "    # 0) ISO\n",
    "    iso_mask = ss.fillna(\"\").str.match(ISO_DATE_RE)\n",
    "    if iso_mask.mean() >= 0.5:\n",
    "        dt_iso = pd.to_datetime(ss.where(iso_mask), errors=\"coerce\", dayfirst=False)\n",
    "        ok = dt_iso.notna().mean() if len(ss) else 0.0\n",
    "        if ok >= 0.5:\n",
    "            valid_dt = dt_iso.dropna()\n",
    "            has_time = (len(valid_dt) and (valid_dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2)\n",
    "            return (dt_iso, \"DATETIME\" if has_time else \"DATE\")\n",
    "\n",
    "    # 1) Excel serial\n",
    "    as_num = pd.to_numeric(ss, errors=\"coerce\")\n",
    "    frac_num = as_num.notna().mean() if len(ss) else 0.0\n",
    "    if frac_num >= 0.9:\n",
    "        mask = as_num.between(60, 2950000)\n",
    "        parsed = pd.to_datetime(as_num.where(mask), unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "        ok = parsed.notna().mean() if len(ss) else 0.0\n",
    "        if ok >= 0.7:\n",
    "            valid_dt = parsed.dropna()\n",
    "            has_time = (len(valid_dt) and (valid_dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2)\n",
    "            return (parsed, \"DATETIME\" if has_time else \"DATE\")\n",
    "\n",
    "    # 2) tokens / nom de colonne (tolérant aux colonnes clairsemées)\n",
    "    colname_hint = any(h in strip_accents_lower(s.name or \"\") for h in COLNAME_DATE_HINTS)\n",
    "    date_token_ratio = ss.fillna(\"\").str.contains(DATE_TOKEN_RE, na=False).mean()\n",
    "    looks_datey = colname_hint or (date_token_ratio >= 0.30)\n",
    "    if looks_datey:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "            dt1 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=True,  cache=True)\n",
    "            dt2 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=False, cache=True)\n",
    "        dt = dt2 if dt2.notna().sum() > dt1.notna().sum() else dt1\n",
    "\n",
    "        n_all = len(ss)\n",
    "        n_parsed = int(dt.notna().sum())\n",
    "        nonempty_mask = ss.fillna(\"\").str.strip().ne(\"\")\n",
    "        n_nonempty = int(nonempty_mask.sum())\n",
    "\n",
    "        ok_abs = (n_parsed / n_all) if n_all else 0.0\n",
    "        ok_nonempty = (n_parsed / n_nonempty) if n_nonempty else 0.0\n",
    "\n",
    "        if colname_hint:\n",
    "            accept = (n_parsed >= 3 and ok_nonempty >= 0.80) or (n_parsed >= 2 and ok_abs >= 0.20)\n",
    "        else:\n",
    "            accept = ok_abs >= 0.70\n",
    "\n",
    "        if accept:\n",
    "            valid_dt = dt.dropna()\n",
    "            has_time = (len(valid_dt) and (valid_dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2)\n",
    "            return (dt, \"DATETIME\" if has_time else \"DATE\")\n",
    "\n",
    "    # 3) bool strict\n",
    "    mb = ss.str.lower().map(lambda x: True if x in MAP_TRUE else (False if x in MAP_FALSE else pd.NA))\n",
    "    if (mb.notna().mean() if len(ss) else 0) >= 0.98:\n",
    "        return mb.astype(\"boolean\"), \"BOOL\"\n",
    "\n",
    "    # 4) nombre\n",
    "    nums = parse_number_like(ss)\n",
    "    if (nums.notna().mean() if len(ss.dropna()) else 0) >= 0.85:\n",
    "        nz = nums.dropna()\n",
    "        if len(nz) and np.isclose(nz, np.round(nz)).all():\n",
    "            return nums.round().astype(\"Int64\"), \"INT\"\n",
    "        return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 5) texte\n",
    "    return ss.astype(\"string\"), \"STRING\"\n",
    "\n",
    "def pg_type(tag: str):\n",
    "    return {\n",
    "        \"DATE\": satypes.Date(),\n",
    "        \"DATETIME\": satypes.DateTime(),  # timestamp without time zone\n",
    "        \"INT\": satypes.Integer(),\n",
    "        \"FLOAT\": satypes.Float(precision=53),\n",
    "        \"BOOL\": satypes.Boolean(),\n",
    "        \"STRING\": satypes.Text()\n",
    "    }.get(tag, satypes.Text())\n",
    "\n",
    "# ================== DB WRITE ================== #\n",
    "def write_df_to_postgres(engine, df: pd.DataFrame, table_name: str, schema_tags: dict):\n",
    "    # cast pandas → types sûrs\n",
    "    for c, tag in schema_tags.items():\n",
    "        if tag == \"DATE\":\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\").dt.date\n",
    "        elif tag == \"DATETIME\":\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "    dtype_map = {c: pg_type(tag) for c, tag in schema_tags.items()}\n",
    "\n",
    "    df.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        schema=TARGET_SCHEMA,\n",
    "        if_exists=IF_EXISTS_MODE,   # \"replace\" ou \"append\"\n",
    "        index=False,\n",
    "        dtype=dtype_map,\n",
    "        method=\"multi\",\n",
    "        chunksize=1000\n",
    "    )\n",
    "\n",
    "# ================== MAIN ================== #\n",
    "def main():\n",
    "    xlsx = Path(FILE_PATH)\n",
    "    if not xlsx.exists():\n",
    "        print(f\"[error] Fichier introuvable: {xlsx}\")\n",
    "        return\n",
    "\n",
    "    # Connexion DB (réutilise le 1er bloc ou env)\n",
    "    try:\n",
    "        engine = get_engine()\n",
    "        with engine.connect() as conn:\n",
    "            ver = conn.execute(text(\"select version();\")).scalar()\n",
    "            print(\"[ok] Connecté à PostgreSQL\")\n",
    "            print(ver)\n",
    "    except (OperationalError, RuntimeError) as e:\n",
    "        print(\"[error] Connexion PostgreSQL échouée:\", e)\n",
    "        return\n",
    "\n",
    "    # Lecture multi-feuilles\n",
    "    sheets = pd.read_excel(xlsx, sheet_name=None, engine=\"openpyxl\", header=None)\n",
    "\n",
    "    file_base = snake_id(xlsx.stem)\n",
    "    done = 0\n",
    "\n",
    "    for name, raw in sheets.items():\n",
    "        raw = raw.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "        if raw.empty:\n",
    "            continue\n",
    "\n",
    "        h = choose_header_row(raw, scan=25)\n",
    "        cols = normalize_columns(raw.iloc[h].tolist())\n",
    "        df = raw.iloc[h+1:].copy()\n",
    "        df.columns = cols\n",
    "\n",
    "        df = df.dropna(how=\"all\")\n",
    "        df = drop_empty_columns(df)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        schema = {}\n",
    "        for c in df.columns:\n",
    "            casted, tag = infer_col(df[c])\n",
    "            df[c] = casted\n",
    "            schema[c] = tag\n",
    "\n",
    "        # Nom de table: public.<file> si 1 feuille, sinon public.<file>__<sheet>\n",
    "        sheet_base = snake_id(name)\n",
    "        table_name = file_base if len(sheets) == 1 else f\"{file_base}__{sheet_base}\"\n",
    "        table_name = pg_ident(table_name)\n",
    "\n",
    "        print(f\"\\n--- Feuille: {name} -> {TARGET_SCHEMA}.{table_name} ---\")\n",
    "        print(f\"rows={len(df)} | cols={df.shape[1]}\")\n",
    "        print(\"Colonnes:\", \", \".join(df.columns.astype(str)))\n",
    "        print(\"\\nSchema détecté:\")\n",
    "        for c in df.columns:\n",
    "            pgt = type(pg_type(schema[c])).__name__.replace(\"TypeEngine\", \"\")\n",
    "            print(f\"  - {c}: {schema[c]} -> {pgt}\")\n",
    "\n",
    "        if SHOW_SAMPLE:\n",
    "            with pd.option_context(\"display.max_columns\", 80, \"display.width\", 200):\n",
    "                print(\"\\nSample (top 8):\")\n",
    "                print(df.head(8))\n",
    "\n",
    "        # Écriture DB\n",
    "        try:\n",
    "            write_df_to_postgres(engine, df.copy(), table_name, schema)\n",
    "            print(f\"[ok] Inserted into {TARGET_SCHEMA}.{table_name}\")\n",
    "            done += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Échec insertion {TARGET_SCHEMA}.{table_name}: {e}\")\n",
    "\n",
    "    print(f\"\\nDone. Feuilles analysées & importées: {done}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512156ef-83e5-48d0-9366-eb6acf766b53",
   "metadata": {},
   "source": [
    "# Conditionsdepaiement.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e463cb-e4b5-4166-8712-a60c764a7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, unicodedata\n",
    "from datetime import datetime\n",
    "\n",
    "# ========= CONFIG =========\n",
    "FILE_PATH = r\"C:\\globasoft\\aerotech\\fic\\Conditionsdepaiement.xlsx\"\n",
    "SHOW_SAMPLE = True\n",
    "\n",
    "# ========= LOG =========\n",
    "def now(): return datetime.now().strftime(\"%H:%M:%S\")\n",
    "def log(msg): print(f\"[{now()}] {msg}\", flush=True)\n",
    "\n",
    "# ========= UTILS =========\n",
    "def parse_number_like(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalise nombres FR/US et convertit en nombre (ou NaN).\"\"\"\n",
    "    x = series.astype(\"string\").str.replace(\"\\u00A0\", \" \", regex=False).str.replace(\" \", \"\", regex=False)\n",
    "    x = x.str.replace(r\"(?<=\\d)\\.(?=\\d{3}(?:\\D|$))\", \"\", regex=True)  # retire séparateurs de milliers type 1.234\n",
    "    x = x.str.replace(\",\", \".\", regex=False)                          # virgule -> point\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "# ========= MAIN =========\n",
    "def main():\n",
    "    xlsx = Path(FILE_PATH)\n",
    "    log(f\"Fichier: {xlsx}\")\n",
    "    if not xlsx.exists():\n",
    "        log(\"[error] Fichier introuvable\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df0 = pd.read_excel(xlsx, sheet_name=0, header=None, engine=\"openpyxl\")\n",
    "    except Exception as e:\n",
    "        log(f\"[error] Lecture échouée: {e}\")\n",
    "        return\n",
    "\n",
    "    log(f\"Feuille[0]: shape={df0.shape}\")\n",
    "\n",
    "    # Purge lignes/colonnes totalement vides\n",
    "    df = df0.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "    log(f\"Après drop vides: {df0.shape} -> {df.shape}\")\n",
    "    if df.empty:\n",
    "        log(\"Feuille vide après nettoyage.\")\n",
    "        return\n",
    "\n",
    "    # On s'attend à 2 ou 3 colonnes: code, libellé, (option) délai\n",
    "    if df.shape[1] >= 3:\n",
    "        df = df.iloc[:, :3].copy()\n",
    "        df.columns = [\"code_condition\", \"libelle\", \"delai_source\"]\n",
    "        log(\"Colonnes détectées: code_condition, libelle, delai_source\")\n",
    "    elif df.shape[1] == 2:\n",
    "        df = df.iloc[:, :2].copy()\n",
    "        df.columns = [\"code_condition\", \"libelle\"]\n",
    "        log(\"Colonnes détectées: code_condition, libelle (pas de colonne delai_source)\")\n",
    "    else:\n",
    "        log(\"[warn] Moins de 2 colonnes non vides trouvées. Abandon.\")\n",
    "        return\n",
    "\n",
    "    # Types de base + trims\n",
    "    df[\"code_condition\"] = parse_number_like(df[\"code_condition\"]).astype(\"Int64\")\n",
    "    df[\"libelle\"] = df[\"libelle\"].astype(\"string\").str.strip()\n",
    "\n",
    "    # Colonne finale: delai_jours (Int64, nullable)\n",
    "    df[\"delai_jours\"] = pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "\n",
    "    # Si delai_source existe, l'utiliser (aucune déduction depuis libellé)\n",
    "    if \"delai_source\" in df.columns:\n",
    "        d0 = parse_number_like(df[\"delai_source\"]).astype(\"Int64\")\n",
    "        n_before = df[\"delai_jours\"].notna().sum()\n",
    "        df.loc[d0.notna(), \"delai_jours\"] = d0\n",
    "        n_after = df[\"delai_jours\"].notna().sum()\n",
    "        log(f\"Remplissage delai_jours depuis delai_source: +{int(n_after - n_before)} lignes\")\n",
    "\n",
    "    # Conserver UNIQUEMENT (code_condition, libelle, delai_jours)\n",
    "    keep_cols = [\"code_condition\", \"libelle\", \"delai_jours\"]\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # Affichage\n",
    "    print(\"\\n--- Conditions de paiement (collecte) ---\")\n",
    "    print(f\"rows={len(df)} | cols={df.shape[1]}\")\n",
    "    print(\"Colonnes:\", \", \".join(df.columns.astype(str)))\n",
    "\n",
    "    print(\"\\nSchéma final:\")\n",
    "    print(\"  - code_condition : INT (Int64 nullable)\")\n",
    "    print(\"  - libelle        : STRING (string)\")\n",
    "    print(\"  - delai_jours    : INT (Int64 nullable)\")\n",
    "\n",
    "    if SHOW_SAMPLE:\n",
    "        with pd.option_context(\"display.max_columns\", 100, \"display.width\", 200):\n",
    "            print(\"\\nSample (top 15):\")\n",
    "            print(df.head(15))\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_conditions = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904113a8-3a66-4165-81c9-b213fcff63db",
   "metadata": {},
   "source": [
    "# FNP AEC projets - source.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb37216-3eab-4cc2-8ed7-61212191ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, unicodedata, warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "FILE_PATH = r\"C:\\globasoft\\aerotech\\fic\\FNP AEC projets - source.xlsx\"\n",
    "SHOW_SAMPLE = True\n",
    "\n",
    "# ============== LOG ==============\n",
    "def now(): return datetime.now().strftime(\"%H:%M:%S\")\n",
    "def log(msg): print(f\"[{now()}] {msg}\", flush=True)\n",
    "\n",
    "# ============== UTILS ==============\n",
    "def strip_accents_lower(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def snake_id(s):\n",
    "    s = strip_accents_lower(s)\n",
    "    s = re.sub(r\"[\\s\\.\\-]+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"col\"\n",
    "\n",
    "def parse_number_like(series: pd.Series) -> pd.Series:\n",
    "    x = series.astype(\"string\").str.replace(\"\\u00A0\",\" \", regex=False).str.replace(\" \",\"\", regex=False)\n",
    "    x = x.str.replace(r\"(?<=\\d)\\.(?=\\d{3}(?:\\D|$))\",\"\", regex=True)  # retire séparateurs de milliers\n",
    "    x = x.str.replace(\",\",\".\", regex=False)                          # virgule -> point\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "# ============== HEADER PICK + NORMALIZE ==============\n",
    "HEADER_HINTS = (\n",
    "    \"projet\",\"projets\",\"code\",\"id\",\"date\",\"mois\",\"année\",\"client\",\"fournisseur\",\"libelle\",\"libellé\",\n",
    "    \"montant\",\"quantite\",\"qté\",\"qte\",\"prix\",\"ttc\",\"ht\",\"statut\",\"status\",\"site\",\"type\",\"categorie\",\n",
    "    \"echeance\",\"échéance\",\"délai\",\"delai\",\"commentaire\",\"ref\",\"réf\"\n",
    ")\n",
    "\n",
    "def choose_header_row(df: pd.DataFrame, scan=25) -> int:\n",
    "    \"\"\"1) ligne contenant plusieurs mots-clés; 2) densité + présence de texte.\"\"\"\n",
    "    limit = min(len(df), scan)\n",
    "    cand_idx, cand_hits = None, -1\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        hits = 0\n",
    "        for v in row:\n",
    "            t = strip_accents_lower(v)\n",
    "            if t and any(k in t for k in HEADER_HINTS):\n",
    "                hits += 1\n",
    "        if hits >= 2 and hits > cand_hits:\n",
    "            cand_idx, cand_hits = i, hits\n",
    "    if cand_idx is not None:\n",
    "        return cand_idx\n",
    "\n",
    "    best, idx = -1, 0\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        non_empty = row.map(lambda x: x.strip()!=\"\").sum()\n",
    "        texty = row.map(lambda x: bool(re.search(r\"[A-Za-zÀ-ÿ]\", x))).sum()\n",
    "        score = non_empty*2 + texty\n",
    "        if score > best: best, idx = score, i\n",
    "    return idx\n",
    "\n",
    "def normalize_columns(header_vals):\n",
    "    cols, seen = [], {}\n",
    "    for v in header_vals:\n",
    "        s = snake_id(v) if (v is not None and str(v).strip()!=\"\") else \"col\"\n",
    "        seen[s] = seen.get(s,0)+1\n",
    "        cols.append(s if seen[s]==1 else f\"{s}_{seen[s]}\")\n",
    "    return cols\n",
    "\n",
    "def drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    blank_cols = [c for c in df.columns if df[c].dropna().astype(str).str.strip().eq(\"\").all()]\n",
    "    if blank_cols:\n",
    "        df = df.drop(columns=blank_cols)\n",
    "    return df\n",
    "\n",
    "# ============== INFÉRENCE TYPES (simple et robuste) ==============\n",
    "MAP_TRUE  = {\"true\",\"vrai\",\"oui\",\"yes\",\"1\",\"y\",\"o\"}\n",
    "MAP_FALSE = {\"false\",\"faux\",\"non\",\"no\",\"0\",\"n\"}\n",
    "\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"(?:\\d{4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,2})|(?:\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4})|\"\n",
    "    r\"(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|janv|févr|fevr|avr|mai|juin|juil|sept|oct|nov|d[ée]c)\",\n",
    "    re.I\n",
    ")\n",
    "ISO_DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:[ T]\\d{2}:\\d{2}:\\d{2})?$\")\n",
    "COLNAME_DATE_HINTS = (\"date\",\"dt\",\"heure\",\"time\",\"echeance\",\"échéance\")\n",
    "\n",
    "def excel_serial_to_datetime(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Conversion sûre des numéros de date Excel -> datetime.\n",
    "    Fenêtre 'safe' contrainte à [60 .. 106750] jours:\n",
    "    - 60  ~ 1900-03-01\n",
    "    - 106750 ~ limite Timedelta pandas (~2173)\n",
    "    Tout ce qui est hors-fenêtre -> NaN AVANT conversion.\n",
    "    \"\"\"\n",
    "    vals = pd.to_numeric(series, errors=\"coerce\").astype(\"float64\")\n",
    "    vals[~np.isfinite(vals)] = np.nan\n",
    "\n",
    "    lower, upper = 60.0, 106750.0\n",
    "    mask = (vals >= lower) & (vals <= upper)\n",
    "    n_out = int((~mask & ~pd.isna(vals)).sum())\n",
    "    if n_out:\n",
    "        log(f\"[excel_serial_to_datetime] valeurs hors fenêtre [{int(lower)}, {int(upper)}] remplacées par NaN: {n_out}\")\n",
    "    vals = vals.where(mask, np.nan)\n",
    "\n",
    "    base = pd.Timestamp(\"1899-12-30\")\n",
    "    # conversion sûre (les NaN restent NaT)\n",
    "    td = pd.to_timedelta(vals, unit=\"D\", errors=\"coerce\")\n",
    "    dt = base + td\n",
    "    return pd.to_datetime(dt, errors=\"coerce\")\n",
    "\n",
    "def infer_col(col: pd.Series):\n",
    "    s = col.copy()\n",
    "    ss = s.astype(\"string\").str.strip()\n",
    "    ss = ss.where(~ss.fillna(\"\").eq(\"0\"), pd.NA)  # éviter 0 -> 1970-01-01\n",
    "\n",
    "    # 0) ISO direct\n",
    "    iso_mask = ss.fillna(\"\").str.match(ISO_DATE_RE)\n",
    "    if iso_mask.mean() >= 0.5:\n",
    "        dt_iso = pd.to_datetime(ss.where(iso_mask), errors=\"coerce\", dayfirst=False)\n",
    "        ok = dt_iso.notna().mean() if len(ss) else 0.0\n",
    "        if ok >= 0.5:\n",
    "            has_time = (dt_iso.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                dt_iso.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else dt_iso.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 1) Excel serial plausibles (et majoritairement dans la fenêtre sûre)\n",
    "    as_num = pd.to_numeric(ss, errors=\"coerce\")\n",
    "    if len(ss):\n",
    "        safe_mask = (as_num >= 60) & (as_num <= 106750)\n",
    "        safe_ratio = safe_mask.mean()\n",
    "    else:\n",
    "        safe_ratio = 0.0\n",
    "\n",
    "    if (as_num.notna().mean() if len(ss) else 0.0) >= 0.9 and safe_ratio >= 0.7:\n",
    "        parsed = excel_serial_to_datetime(as_num)\n",
    "        ok = parsed.notna().mean() if len(ss) else 0.0\n",
    "        if ok >= 0.7:\n",
    "            has_time = (parsed.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                parsed.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else parsed.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 2) tokens de date / nom “datey”\n",
    "    colname_hint = any(h in strip_accents_lower(s.name or \"\") for h in COLNAME_DATE_HINTS)\n",
    "    date_token_ratio = ss.fillna(\"\").str.contains(DATE_TOKEN_RE, na=False).mean()\n",
    "    looks_datey = colname_hint or (date_token_ratio >= 0.30)\n",
    "    if looks_datey:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "            dt1 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=True,  cache=True)\n",
    "            dt2 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=False, cache=True)\n",
    "        dt = dt2 if dt2.notna().sum() > dt1.notna().sum() else dt1\n",
    "        ok = dt.notna().mean() if len(ss) else 0.0\n",
    "        ok_thresh = 0.50 if colname_hint else 0.70\n",
    "        if ok >= ok_thresh:\n",
    "            has_time = (dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 3) bool strict\n",
    "    mb = ss.str.lower().map(lambda x: True if x in MAP_TRUE else (False if x in MAP_FALSE else pd.NA))\n",
    "    if (mb.notna().mean() if len(ss) else 0) >= 0.98:\n",
    "        return mb.astype(\"boolean\"), \"BOOL\"\n",
    "\n",
    "    # 4) nombre\n",
    "    nums = parse_number_like(ss)\n",
    "    if (nums.notna().mean() if len(ss.dropna()) else 0) >= 0.85:\n",
    "        nz = nums.dropna()\n",
    "        if len(nz) and (np.mod(nz, 1) == 0).all():\n",
    "            return nums.astype(\"Int64\"), \"INT\"\n",
    "        return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 5) texte\n",
    "    return ss.astype(\"string\"), \"STRING\"\n",
    "\n",
    "def pg_type(tag: str) -> str:\n",
    "    return {\n",
    "        \"DATE\":\"date\",\"DATETIME\":\"timestamp without time zone\",\n",
    "        \"INT\":\"integer\",\"FLOAT\":\"double precision\",\"BOOL\":\"boolean\"\n",
    "    }.get(tag, \"text\")\n",
    "\n",
    "# ============== MAIN ==============\n",
    "def main():\n",
    "    xlsx = Path(FILE_PATH)\n",
    "    log(f\"Fichier: {xlsx}\")\n",
    "    if not xlsx.exists():\n",
    "        log(\"[error] Fichier introuvable\"); return\n",
    "\n",
    "    try:\n",
    "        sheets = pd.read_excel(xlsx, sheet_name=None, engine=\"openpyxl\", header=None)\n",
    "    except Exception as e:\n",
    "        log(f\"[error] Lecture échouée: {e}\"); return\n",
    "\n",
    "    done = 0\n",
    "    for name, raw in sheets.items():\n",
    "        log(f\"\\n[feuille] {name}: shape initiale={raw.shape}\")\n",
    "\n",
    "        # nettoyage vide\n",
    "        raw = raw.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "        log(f\"[feuille] {name}: après drop vides -> {raw.shape}\")\n",
    "        if raw.empty: \n",
    "            log(f\"[feuille] {name}: vide, on passe.\")\n",
    "            continue\n",
    "\n",
    "        # header + normalisation\n",
    "        h = choose_header_row(raw, scan=25)\n",
    "        log(f\"[feuille] {name}: header choisi à la ligne {h}\")\n",
    "        cols = normalize_columns(raw.iloc[h].tolist())\n",
    "        log(f\"[feuille] {name}: colonnes normalisées -> {', '.join(cols)}\")\n",
    "        df = raw.iloc[h+1:].copy()\n",
    "        df.columns = cols\n",
    "        df = df.dropna(how=\"all\")\n",
    "        df = drop_empty_columns(df)\n",
    "        log(f\"[feuille] {name}: shape après nettoyage colonnes -> {df.shape}\")\n",
    "        if df.empty:\n",
    "            log(f\"[feuille] {name}: vide après normalisation, on passe.\")\n",
    "            continue\n",
    "\n",
    "        # inférence types\n",
    "        schema = {}\n",
    "        for c in df.columns:\n",
    "            casted, tag = infer_col(df[c])\n",
    "            df[c] = casted\n",
    "            schema[c] = tag\n",
    "\n",
    "        base = f\"{snake_id(xlsx.stem)}.{snake_id(name)}\"\n",
    "        print(f\"\\n--- Feuille: {name} -> {base} ---\")\n",
    "        print(f\"rows={len(df)} | cols={df.shape[1]}\")\n",
    "        print(\"Colonnes:\", \", \".join(df.columns.astype(str)))\n",
    "        print(\"\\nSchema détecté:\")\n",
    "        for c in df.columns:\n",
    "            print(f\"  - {c}: {schema[c]}  ->  {pg_type(schema[c])}\")\n",
    "\n",
    "        if SHOW_SAMPLE:\n",
    "            with pd.option_context(\"display.max_columns\", 120, \"display.width\", 220):\n",
    "                print(\"\\nSample (top 10):\")\n",
    "                print(df.head(10))\n",
    "\n",
    "        done += 1\n",
    "\n",
    "    print(f\"\\nDone. Feuilles analysées: {done}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b778bf4-a237-4cae-be16-be2ab9c4947d",
   "metadata": {},
   "source": [
    "# Projets AEC AE 2025 - source.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3747a9f-d754-44af-ab95-2abb51be1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, unicodedata, warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "FILE_PATH = r\"C:\\globasoft\\aerotech\\fic\\Projets AEC AE 2025 - source.xlsx\"\n",
    "SHOW_SAMPLE = True\n",
    "STRICT_SOURCE_SCHEMA = True  # Afficher uniquement le schéma strict du fichier\n",
    "\n",
    "# ============== LOG ==============\n",
    "def now(): return datetime.now().strftime(\"%H:%M:%S\")\n",
    "def log(msg): print(f\"[{now()}] {msg}\", flush=True)\n",
    "\n",
    "# ============== UTILS ==============\n",
    "def strip_accents_lower(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def snake_id(s):\n",
    "    s = strip_accents_lower(s)\n",
    "    s = re.sub(r\"[\\s\\.\\-]+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"col\"\n",
    "\n",
    "def make_unique_columns(cols):\n",
    "    \"\"\"Rend les noms uniques en suffixant _2, _3... en cas de doublons.\"\"\"\n",
    "    out, seen = [], {}\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            seen[c] = 1\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}_{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def parse_number_like(series: pd.Series) -> pd.Series:\n",
    "    x = series.astype(\"string\").str.replace(\"\\u00A0\",\" \", regex=False).str.replace(\" \",\"\", regex=False)\n",
    "    # retire séparateurs de milliers type \".\" entre chiffres\n",
    "    x = x.str.replace(r\"(?<=\\d)\\.(?=\\d{3}(?:\\D|$))\",\"\", regex=True)\n",
    "    # virgule décimale -> point\n",
    "    x = x.str.replace(\",\",\".\", regex=False)\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "def drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    blank_idx = []\n",
    "    for j in range(df.shape[1]):\n",
    "        s = df.iloc[:, j]\n",
    "        if s.dropna().astype(str).str.strip().eq(\"\").all():\n",
    "            blank_idx.append(j)\n",
    "    if blank_idx:\n",
    "        log(f\"[drop_empty_columns] suppression colonnes blanches (pos): {blank_idx}\")\n",
    "        df = df.drop(df.columns[blank_idx], axis=1)\n",
    "    return df\n",
    "\n",
    "# ============== HEADER PICK ==============\n",
    "HEADER_HINTS = (\n",
    "    \"projet\",\"projets\",\"code\",\"id\",\"réf\",\"ref\",\"libelle\",\"libellé\",\"intitulé\",\n",
    "    \"client\",\"fournisseur\",\"chef de projet\",\"cp\",\"cdp\",\"manager\",\n",
    "    \"statut\",\"status\",\"phase\",\"categorie\",\"catégorie\",\"type\",\n",
    "    \"date debut\",\"date début\",\"debut\",\"début\",\"date fin\",\"fin\",\"echeance\",\"échéance\",\n",
    "    \"budget\",\"montant\",\"cout\",\"coût\",\"prix\",\"ht\",\"ttc\",\n",
    "    \"avancement\",\"progress\",\"%\",\"taux\",\n",
    "    \"site\",\"section analytique\",\"analytique\",\"commentaire\",\"période\",\"periode\",\n",
    "    \"plan\",\"niveau\",\"sop\",\"année\",\"mois\",\"anneemois\",\"nature\",\"compte\",\"crédit\",\"debit\",\"solde\"\n",
    ")\n",
    "\n",
    "def choose_header_row(df: pd.DataFrame, scan=25) -> int:\n",
    "    limit = min(len(df), scan)\n",
    "    cand_idx, cand_hits = None, -1\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        hits = 0\n",
    "        for v in row:\n",
    "            t = strip_accents_lower(v)\n",
    "            if t and any(k in t for k in HEADER_HINTS):\n",
    "                hits += 1\n",
    "        if hits >= 2 and hits > cand_hits:\n",
    "            cand_idx, cand_hits = i, hits\n",
    "    if cand_idx is not None:\n",
    "        return cand_idx\n",
    "\n",
    "    best, idx = -1, 0\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        non_empty = row.map(lambda x: x.strip()!=\"\").sum()\n",
    "        texty = row.map(lambda x: bool(re.search(r\"[A-Za-zÀ-ÿ]\", x))).sum()\n",
    "        score = non_empty*2 + texty\n",
    "        if score > best: best, idx = score, i\n",
    "    return idx\n",
    "\n",
    "# ============== INFÉRENCE TYPES (→ PostgreSQL) ==============\n",
    "MAP_TRUE  = {\"true\",\"vrai\",\"oui\",\"yes\",\"1\",\"y\",\"o\"}\n",
    "MAP_FALSE = {\"false\",\"faux\",\"non\",\"no\",\"0\",\"n\"}\n",
    "\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"(?:\\d{4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,2})|(?:\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4})|\"\n",
    "    r\"(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|janv|févr|fevr|avr|mai|juin|juil|sept|oct|nov|d[ée]c)\",\n",
    "    re.I\n",
    ")\n",
    "ISO_DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:[ T]\\d{2}:\\d{2}:\\d{2})?$\")\n",
    "COLNAME_DATE_HINTS = (\"date\",\"dt\",\"heure\",\"time\",\"echeance\",\"échéance\",\"debut\",\"début\",\"fin\",\"période\",\"periode\")\n",
    "\n",
    "AMOUNT_NAME_HINTS = (\n",
    "    \"montant\",\"debit\",\"débit\",\"credit\",\"crédit\",\"solde\",\"budget\",\"ht\",\"ttc\",\n",
    "    \"amount\",\"total\",\"prix\",\"coût\",\"cout\"\n",
    ")\n",
    "CODE_LIKE_HINTS = (\"code\",\"nature\",\"plan\",\"compte\",\"sop\",\"section\",\"niveau\")\n",
    "\n",
    "def colname_has(hints, name):\n",
    "    n = strip_accents_lower(name or \"\")\n",
    "    return any(h in n for h in hints)\n",
    "\n",
    "def excel_serial_to_datetime(series: pd.Series) -> pd.Series:\n",
    "    vals = pd.to_numeric(series, errors=\"coerce\").astype(\"float64\")\n",
    "    vals[~np.isfinite(vals)] = np.nan\n",
    "    # fenêtre sûre Excel (≈ 1899-12-30 + jours)\n",
    "    lower, upper = 60.0, 60000.0  # upper ~ 2064\n",
    "    mask = (vals >= lower) & (vals <= upper)\n",
    "    n_out = int((~mask & ~pd.isna(vals)).sum())\n",
    "    if n_out:\n",
    "        log(f\"[excel_serial_to_datetime] valeurs hors fenêtre [{int(lower)}, {int(upper)}] -> NaN: {n_out}\")\n",
    "    vals = vals.where(mask, np.nan)\n",
    "    base = pd.Timestamp(\"1899-12-30\")\n",
    "    td = pd.to_timedelta(vals, unit=\"D\", errors=\"coerce\")\n",
    "    dt = base + td\n",
    "    return pd.to_datetime(dt, errors=\"coerce\")\n",
    "\n",
    "def infer_col(col: pd.Series):\n",
    "    \"\"\"\n",
    "    Retourne (serie_casted, tag) avec tag ∈ {\"DATE\",\"DATETIME\",\"INT\",\"FLOAT\",\"BOOL\",\"STRING\"}.\n",
    "    \"\"\"\n",
    "    s = col.copy()\n",
    "    ss = s.astype(\"string\").str.strip()\n",
    "\n",
    "    # 0) ISO direct\n",
    "    iso_mask = ss.fillna(\"\").str.match(ISO_DATE_RE)\n",
    "    if len(ss) and iso_mask.mean() >= 0.5:\n",
    "        dt_iso = pd.to_datetime(ss.where(iso_mask), errors=\"coerce\", dayfirst=False)\n",
    "        ok = dt_iso.notna().mean()\n",
    "        if ok >= 0.5:\n",
    "            has_time = (dt_iso.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                dt_iso.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else dt_iso.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 1) Excel serial dates (restreint)\n",
    "    as_num = pd.to_numeric(ss, errors=\"coerce\")\n",
    "    if len(ss):\n",
    "        safe_mask = (as_num >= 60) & (as_num <= 60000)\n",
    "        safe_ratio = safe_mask.mean()\n",
    "    else:\n",
    "        safe_ratio = 0.0\n",
    "\n",
    "    colname_hint = colname_has(COLNAME_DATE_HINTS, s.name)\n",
    "    high_vals_ratio = (as_num >= 20000).mean() if len(ss) else 0.0  # 20000 ~ 1954\n",
    "\n",
    "    if (as_num.notna().mean() if len(ss) else 0.0) >= 0.9 and safe_ratio >= 0.7 and (colname_hint or high_vals_ratio >= 0.2):\n",
    "        parsed = excel_serial_to_datetime(as_num)\n",
    "        ok = parsed.notna().mean() if len(ss) else 0.0\n",
    "        if ok >= 0.7:\n",
    "            has_time = (parsed.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                parsed.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else parsed.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 2) tokens de date / nom “datey”\n",
    "    date_token_ratio = ss.fillna(\"\").str.contains(DATE_TOKEN_RE, na=False).mean()\n",
    "    looks_datey = colname_hint or (date_token_ratio >= 0.30)\n",
    "    if looks_datey:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "            dt1 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=True,  cache=True)\n",
    "            dt2 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=False, cache=True)\n",
    "        dt = dt2 if dt2.notna().sum() > dt1.notna().sum() else dt1\n",
    "        ok = dt.notna().mean() if len(ss) else 0.0\n",
    "        ok_thresh = 0.50 if colname_hint else 0.70\n",
    "        if ok >= ok_thresh:\n",
    "            has_time = (dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 3) bool strict\n",
    "    mb = ss.str.lower().map(lambda x: True if x in MAP_TRUE else (False if x in MAP_FALSE else pd.NA))\n",
    "    if len(ss) and mb.notna().mean() >= 0.98:\n",
    "        return mb.astype(\"boolean\"), \"BOOL\"\n",
    "\n",
    "    # 3bis) colonnes monétaires : forcer numéric si le nom l'indique\n",
    "    if colname_has(AMOUNT_NAME_HINTS, s.name):\n",
    "        nums_hint = parse_number_like(ss)\n",
    "        if nums_hint.notna().any():\n",
    "            nz = nums_hint.dropna()\n",
    "            if len(nz) and (np.mod(nz, 1) == 0).all():\n",
    "                return nums_hint.astype(\"Int64\"), \"INT\"\n",
    "            return nums_hint.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 3ter) colonnes \"code-like\" : garder du texte\n",
    "    if colname_has(CODE_LIKE_HINTS, s.name):\n",
    "        return ss.astype(\"string\"), \"STRING\"\n",
    "\n",
    "    # 4) nombre générique\n",
    "    nums = parse_number_like(ss)\n",
    "    if (nums.notna().mean() if len(ss) else 0) >= 0.85:\n",
    "        nz = nums.dropna()\n",
    "        if len(nz) and (np.mod(nz, 1) == 0).all():\n",
    "            return nums.astype(\"Int64\"), \"INT\"\n",
    "        return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 5) texte\n",
    "    return ss.astype(\"string\"), \"STRING\"\n",
    "\n",
    "def pg_type(tag: str) -> str:\n",
    "    return {\n",
    "        \"DATE\":\"date\",\n",
    "        \"DATETIME\":\"timestamp without time zone\",\n",
    "        \"INT\":\"integer\",\n",
    "        \"FLOAT\":\"double precision\",\n",
    "        \"BOOL\":\"boolean\",\n",
    "        \"STRING\":\"text\"\n",
    "    }.get(tag, \"text\")\n",
    "\n",
    "# ============== MAIN ==============\n",
    "def main():\n",
    "    xlsx = Path(FILE_PATH)\n",
    "    log(f\"Fichier: {xlsx}\")\n",
    "    if not xlsx.exists():\n",
    "        log(\"[error] Fichier introuvable\"); return\n",
    "\n",
    "    try:\n",
    "        sheets = pd.read_excel(xlsx, sheet_name=None, engine=\"openpyxl\", header=None)\n",
    "    except Exception as e:\n",
    "        log(f\"[error] Lecture échouée: {e}\"); return\n",
    "\n",
    "    done = 0\n",
    "    for name, raw in sheets.items():\n",
    "        log(f\"\\n[feuille] {name}: shape initiale={raw.shape}\")\n",
    "\n",
    "        # nettoyage vide\n",
    "        raw = raw.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "        log(f\"[feuille] {name}: après drop vides -> {raw.shape}\")\n",
    "        if raw.empty:\n",
    "            log(f\"[feuille] {name}: vide, on passe.\")\n",
    "            continue\n",
    "\n",
    "        # repère l'entête dans le brut (sans remap)\n",
    "        h = choose_header_row(raw, scan=25)\n",
    "        log(f\"[feuille] {name}: header choisi à la ligne {h}\")\n",
    "\n",
    "        # ====== VUE SOURCE STRICTE (schéma du fichier) ======\n",
    "        df_src = raw.iloc[h+1:].copy()\n",
    "        src_cols_original = raw.iloc[h].tolist()  # libellés EXACTS du fichier\n",
    "        df_src.columns = src_cols_original\n",
    "        df_src = df_src.dropna(how=\"all\").copy()\n",
    "        df_src = drop_empty_columns(df_src)\n",
    "        log(f\"[feuille] {name}: shape après affectation des en-têtes -> {df_src.shape}\")\n",
    "        if df_src.empty:\n",
    "            log(f\"[feuille] {name}: vide après normalisation, on passe.\")\n",
    "            continue\n",
    "\n",
    "        # noms PostgreSQL (snake_case) correspondants aux libellés Excel\n",
    "        pg_cols = make_unique_columns([snake_id(c) for c in df_src.columns])\n",
    "\n",
    "        # inférence STRICTE sur colonnes d'origine (sans ajout ni remap)\n",
    "        schema_src = {}\n",
    "        casted_df = df_src.copy()\n",
    "        for c in list(df_src.columns):\n",
    "            casted, tag = infer_col(df_src[c])\n",
    "            casted_df[c] = casted\n",
    "            schema_src[c] = tag\n",
    "\n",
    "        # affichage STRICT : schéma = fichier, avec mapping vers noms pg + types pg\n",
    "        base = f\"{snake_id(xlsx.stem)}.{snake_id(name)}\"\n",
    "        print(f\"\\n--- Feuille: {name} -> {base} (SCHÉMA SOURCE STRICT / PostgreSQL) ---\")\n",
    "        print(f\"rows={len(casted_df)} | cols={casted_df.shape[1]}\")\n",
    "\n",
    "        print(\"\\nMapping colonnes (Excel → nom_pg) :\")\n",
    "        for src_name, pg_name in zip(df_src.columns, pg_cols):\n",
    "            print(f\"  - {src_name}  ->  {pg_name}\")\n",
    "\n",
    "        print(\"\\nSchema détecté (types PostgreSQL) :\")\n",
    "        for src_name, pg_name in zip(df_src.columns, pg_cols):\n",
    "            tag = schema_src.get(src_name, \"STRING\")\n",
    "            print(f\"  - {pg_name}: {pg_type(tag)}  (source: {src_name}, type détecté: {tag})\")\n",
    "\n",
    "        if SHOW_SAMPLE:\n",
    "            with pd.option_context(\"display.max_columns\", 140, \"display.width\", 240):\n",
    "                print(\"\\nSample (top 10) :\")\n",
    "                print(casted_df.head(10))\n",
    "\n",
    "        done += 1\n",
    "\n",
    "        # ====== (OPTIONNEL) VUE CIBLE : si tu veux une table prête IFS, remets à False STRICT_SOURCE_SCHEMA et ajoute ta logique ======\n",
    "        if not STRICT_SOURCE_SCHEMA:\n",
    "            pass  # tu peux réinsérer ici ta vue cible/mapping si besoin\n",
    "\n",
    "    print(f\"\\nDone. Feuilles analysées: {done}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1a388-6d54-45d3-9317-41e9bc0826c9",
   "metadata": {},
   "source": [
    "# Relevé pointages AEB.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77832e-161e-444c-9c0f-26345bd8376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, unicodedata, warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "FILE_PATH = r\"C:\\globasoft\\aerotech\\fic\\Relevé pointages AEB.xlsx\"  # <== adapte si besoin\n",
    "SHOW_SAMPLE = True\n",
    "STRICT_SOURCE_SCHEMA = True  # schéma = EXACT du fichier (pas d'ajout/remap)\n",
    "\n",
    "# ============== LOG ==============\n",
    "def now(): return datetime.now().strftime(\"%H:%M:%S\")\n",
    "def log(msg): print(f\"[{now()}] {msg}\", flush=True)\n",
    "\n",
    "# ============== UTILS ==============\n",
    "def strip_accents_lower(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def snake_id(s):\n",
    "    s = strip_accents_lower(s)\n",
    "    s = re.sub(r\"[\\s\\.\\-]+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"col\"\n",
    "\n",
    "def make_unique_columns(cols):\n",
    "    out, seen = [], {}\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            seen[c] = 1\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}_{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def parse_number_like(series: pd.Series) -> pd.Series:\n",
    "    x = series.astype(\"string\").str.replace(\"\\u00A0\",\" \", regex=False).str.replace(\" \",\"\", regex=False)\n",
    "    x = x.str.replace(r\"(?<=\\d)\\.(?=\\d{3}(?:\\D|$))\",\"\", regex=True)  # 1.234 -> 1234 (milliers)\n",
    "    x = x.str.replace(\",\",\".\", regex=False)  # virgule décimale -> point\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "def drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    blank_idx = []\n",
    "    for j in range(df.shape[1]):\n",
    "        s = df.iloc[:, j]\n",
    "        if s.dropna().astype(str).str.strip().eq(\"\").all():\n",
    "            blank_idx.append(j)\n",
    "    if blank_idx:\n",
    "        log(f\"[drop_empty_columns] suppression colonnes blanches (pos): {blank_idx}\")\n",
    "        df = df.drop(df.columns[blank_idx], axis=1)\n",
    "    return df\n",
    "\n",
    "# ============== HEADER PICK ==============\n",
    "HEADER_HINTS = (\n",
    "    # génériques\n",
    "    \"projet\",\"projets\",\"code\",\"id\",\"réf\",\"ref\",\"libelle\",\"libellé\",\"intitulé\",\n",
    "    \"client\",\"fournisseur\",\"chef de projet\",\"cp\",\"cdp\",\"manager\",\n",
    "    \"statut\",\"status\",\"phase\",\"categorie\",\"catégorie\",\"type\",\n",
    "    \"date\",\"heure\",\"time\",\"période\",\"periode\",\"echeance\",\"échéance\",\n",
    "    \"budget\",\"montant\",\"cout\",\"coût\",\"prix\",\"ht\",\"ttc\",\"avancement\",\"progress\",\"%\",\"taux\",\n",
    "    \"site\",\"section analytique\",\"analytique\",\"commentaire\",\"observations\",\"notes\",\"remarques\",\n",
    "    # spécifiques vus dans le fichier de pointage\n",
    "    \"trigramme\",\"nom\",\"cat pointage\",\"n° dossier\",\"categorie\",\"ot\",\"ata\",\"type de tâche\",\"commentaires\",\n",
    "    \"bu\",\"immat\",\"wp\",\"semaine\",\"mois\",\"annee\",\"année\",\"cat point\",\"annee - semaine\",\"mois.\"\n",
    ")\n",
    "\n",
    "def choose_header_row(df: pd.DataFrame, scan=25) -> int:\n",
    "    limit = min(len(df), scan)\n",
    "    cand_idx, cand_hits = None, -1\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        hits = 0\n",
    "        for v in row:\n",
    "            t = strip_accents_lower(v)\n",
    "            if t and any(k in t for k in HEADER_HINTS):\n",
    "                hits += 1\n",
    "        if hits >= 2 and hits > cand_hits:\n",
    "            cand_idx, cand_hits = i, hits\n",
    "    if cand_idx is not None:\n",
    "        return cand_idx\n",
    "\n",
    "    best, idx = -1, 0\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        non_empty = row.map(lambda x: x.strip()!=\"\").sum()\n",
    "        texty = row.map(lambda x: bool(re.search(r\"[A-Za-zÀ-ÿ]\", x))).sum()\n",
    "        score = non_empty*2 + texty\n",
    "        if score > best: best, idx = score, i\n",
    "    return idx\n",
    "\n",
    "# ============== INFÉRENCE TYPES (→ PostgreSQL) ==============\n",
    "MAP_TRUE  = {\"true\",\"vrai\",\"oui\",\"yes\",\"1\",\"y\",\"o\"}\n",
    "MAP_FALSE = {\"false\",\"faux\",\"non\",\"no\",\"0\",\"n\"}\n",
    "\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"(?:\\d{4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,2})|(?:\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4})|\"\n",
    "    r\"(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|janv|févr|fevr|avr|mai|juin|juil|sept|oct|nov|d[ée]c)\",\n",
    "    re.I\n",
    ")\n",
    "ISO_DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:[ T]\\d{2}:\\d{2}:\\d{2})?$\")\n",
    "COLNAME_DATE_HINTS = (\"date\",\"dt\",\"heure\",\"time\",\"echeance\",\"échéance\",\"debut\",\"début\",\"fin\",\"période\",\"periode\",\"semaine\",\"mois\",\"année\",\"annee\")\n",
    "\n",
    "AMOUNT_NAME_HINTS = (\n",
    "    \"montant\",\"debit\",\"débit\",\"credit\",\"crédit\",\"solde\",\"budget\",\"ht\",\"ttc\",\n",
    "    \"amount\",\"total\",\"prix\",\"coût\",\"cout\",\"heures\",\"h\"\n",
    ")\n",
    "CODE_LIKE_HINTS = (\"code\",\"nature\",\"plan\",\"compte\",\"sop\",\"section\",\"niveau\",\"trigramme\",\"immat\",\"ot\",\"ata\",\"wp\",\"n° dossier\",\"n_dossier\",\"dossier\",\"cat point\",\"cat pointage\")\n",
    "\n",
    "def colname_has(hints, name):\n",
    "    n = strip_accents_lower(name or \"\")\n",
    "    return any(h in n for h in hints)\n",
    "\n",
    "def excel_serial_to_datetime(series: pd.Series) -> pd.Series:\n",
    "    vals = pd.to_numeric(series, errors=\"coerce\").astype(\"float64\")\n",
    "    vals[~np.isfinite(vals)] = np.nan\n",
    "    lower, upper = 60.0, 60000.0  # ~ 1900..2064\n",
    "    mask = (vals >= lower) & (vals <= upper)\n",
    "    n_out = int((~mask & ~pd.isna(vals)).sum())\n",
    "    if n_out:\n",
    "        log(f\"[excel_serial_to_datetime] valeurs hors fenêtre [{int(lower)}, {int(upper)}] -> NaN: {n_out}\")\n",
    "    base = pd.Timestamp(\"1899-12-30\")\n",
    "    td = pd.to_timedelta(vals.where(mask, np.nan), unit=\"D\", errors=\"coerce\")\n",
    "    dt = base + td\n",
    "    return pd.to_datetime(dt, errors=\"coerce\")\n",
    "\n",
    "def infer_col(col: pd.Series):\n",
    "    \"\"\"\n",
    "    Retourne (serie_casted, tag) avec tag ∈ {\"DATE\",\"DATETIME\",\"INT\",\"FLOAT\",\"BOOL\",\"STRING\"}.\n",
    "    \"\"\"\n",
    "    s = col.copy()\n",
    "    ss = s.astype(\"string\").str.strip()\n",
    "\n",
    "    # 0) ISO direct\n",
    "    iso_mask = ss.fillna(\"\").str.match(ISO_DATE_RE)\n",
    "    if len(ss) and iso_mask.mean() >= 0.5:\n",
    "        dt_iso = pd.to_datetime(ss.where(iso_mask), errors=\"coerce\", dayfirst=False)\n",
    "        ok = dt_iso.notna().mean()\n",
    "        if ok >= 0.5:\n",
    "            has_time = (dt_iso.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                dt_iso.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else dt_iso.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 1) Excel serial dates (restreint)\n",
    "    as_num = pd.to_numeric(ss, errors=\"coerce\")\n",
    "    safe_mask = (as_num >= 60) & (as_num <= 60000)\n",
    "    safe_ratio = safe_mask.mean() if len(ss) else 0.0\n",
    "\n",
    "    colname_hint = colname_has(COLNAME_DATE_HINTS, s.name)\n",
    "    high_vals_ratio = (as_num >= 20000).mean() if len(ss) else 0.0  # 20000 ~ 1954\n",
    "\n",
    "    if (as_num.notna().mean() if len(ss) else 0.0) >= 0.9 and safe_ratio >= 0.7 and (colname_hint or high_vals_ratio >= 0.2):\n",
    "        parsed = excel_serial_to_datetime(as_num)\n",
    "        ok = parsed.notna().mean() if len(ss) else 0.0\n",
    "        if ok >= 0.7:\n",
    "            has_time = (parsed.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                parsed.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else parsed.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 2) tokens de date / nom “datey”\n",
    "    date_token_ratio = ss.fillna(\"\").str.contains(DATE_TOKEN_RE, na=False).mean()\n",
    "    looks_datey = colname_hint or (date_token_ratio >= 0.30)\n",
    "    if looks_datey:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "            dt1 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=True,  cache=True)\n",
    "            dt2 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=False, cache=True)\n",
    "        dt = dt2 if dt2.notna().sum() > dt1.notna().sum() else dt1\n",
    "        ok = dt.notna().mean() if len(ss) else 0.0\n",
    "        ok_thresh = 0.50 if colname_hint else 0.70\n",
    "        if ok >= ok_thresh:\n",
    "            has_time = (dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 3) bool strict\n",
    "    mb = ss.str.lower().map(lambda x: True if x in MAP_TRUE else (False if x in MAP_FALSE else pd.NA))\n",
    "    if len(ss) and mb.notna().mean() >= 0.98:\n",
    "        return mb.astype(\"boolean\"), \"BOOL\"\n",
    "\n",
    "    # 3bis) colonnes monétaires / heures : forcer numérique si le nom l'indique\n",
    "    if colname_has(AMOUNT_NAME_HINTS, s.name):\n",
    "        nums_hint = parse_number_like(ss)\n",
    "        if nums_hint.notna().any():\n",
    "            nz = nums_hint.dropna()\n",
    "            if len(nz) and (np.mod(nz, 1) == 0).all():\n",
    "                return nums_hint.astype(\"Int64\"), \"INT\"\n",
    "            return nums_hint.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 3ter) colonnes \"code-like\" : garder du texte\n",
    "    if colname_has(CODE_LIKE_HINTS, s.name):\n",
    "        return ss.astype(\"string\"), \"STRING\"\n",
    "\n",
    "    # 4) nombre générique\n",
    "    nums = parse_number_like(ss)\n",
    "    if (nums.notna().mean() if len(ss) else 0) >= 0.85:\n",
    "        nz = nums.dropna()\n",
    "        if len(nz) and (np.mod(nz, 1) == 0).all():\n",
    "            return nums.astype(\"Int64\"), \"INT\"\n",
    "        return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 5) texte\n",
    "    return ss.astype(\"string\"), \"STRING\"\n",
    "\n",
    "def pg_type(tag: str) -> str:\n",
    "    return {\n",
    "        \"DATE\":\"date\",\n",
    "        \"DATETIME\":\"timestamp without time zone\",\n",
    "        \"INT\":\"integer\",\n",
    "        \"FLOAT\":\"double precision\",\n",
    "        \"BOOL\":\"boolean\",\n",
    "        \"STRING\":\"text\"\n",
    "    }.get(tag, \"text\")\n",
    "\n",
    "# ============== MAIN ==============\n",
    "def main():\n",
    "    xlsx = Path(FILE_PATH)\n",
    "    log(f\"Fichier: {xlsx}\")\n",
    "    if not xlsx.exists():\n",
    "        log(\"[error] Fichier introuvable\"); return\n",
    "\n",
    "    try:\n",
    "        sheets = pd.read_excel(xlsx, sheet_name=None, engine=\"openpyxl\", header=None)\n",
    "    except Exception as e:\n",
    "        log(f\"[error] Lecture échouée: {e}\"); return\n",
    "\n",
    "    done = 0\n",
    "    for name, raw in sheets.items():\n",
    "        log(f\"\\n[feuille] {name}: shape initiale={raw.shape}\")\n",
    "\n",
    "        # nettoyage vide\n",
    "        raw = raw.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "        log(f\"[feuille] {name}: après drop vides -> {raw.shape}\")\n",
    "        if raw.empty:\n",
    "            log(f\"[feuille] {name}: vide, on passe.\")\n",
    "            continue\n",
    "\n",
    "        # repère l'entête dans le brut (sans remap)\n",
    "        h = choose_header_row(raw, scan=25)\n",
    "        log(f\"[feuille] {name}: header choisi à la ligne {h}\")\n",
    "\n",
    "        # ====== VUE SOURCE STRICTE (schéma du fichier) ======\n",
    "        df_src = raw.iloc[h+1:].copy()\n",
    "        src_cols_original = raw.iloc[h].tolist()  # libellés EXACTS du fichier\n",
    "        df_src.columns = src_cols_original\n",
    "        df_src = df_src.dropna(how=\"all\").copy()\n",
    "        df_src = drop_empty_columns(df_src)\n",
    "        log(f\"[feuille] {name}: shape après affectation des en-têtes -> {df_src.shape}\")\n",
    "        if df_src.empty:\n",
    "            log(f\"[feuille] {name}: vide après normalisation, on passe.\")\n",
    "            continue\n",
    "\n",
    "        # noms PostgreSQL (snake_case) correspondants aux libellés Excel\n",
    "        pg_cols = make_unique_columns([snake_id(c) for c in df_src.columns])\n",
    "\n",
    "        # inférence STRICTE sur colonnes d'origine (sans ajout ni remap)\n",
    "        schema_src = {}\n",
    "        casted_df = df_src.copy()\n",
    "        for c in list(df_src.columns):\n",
    "            casted, tag = infer_col(df_src[c])\n",
    "            casted_df[c] = casted\n",
    "            schema_src[c] = tag\n",
    "\n",
    "        # affichage STRICT : schéma = fichier, avec mapping vers noms pg + types pg\n",
    "        base = f\"{snake_id(xlsx.stem)}.{snake_id(name)}\"\n",
    "        print(f\"\\n--- Feuille: {name} -> {base} (SCHÉMA SOURCE STRICT / PostgreSQL) ---\")\n",
    "        print(f\"rows={len(casted_df)} | cols={casted_df.shape[1]}\")\n",
    "\n",
    "        print(\"\\nMapping colonnes (Excel → nom_pg) :\")\n",
    "        for src_name, pg_name in zip(df_src.columns, pg_cols):\n",
    "            print(f\"  - {src_name}  ->  {pg_name}\")\n",
    "\n",
    "        print(\"\\nSchema détecté (types PostgreSQL) :\")\n",
    "        for src_name, pg_name in zip(df_src.columns, pg_cols):\n",
    "            tag = schema_src.get(src_name, \"STRING\")\n",
    "            print(f\"  - {pg_name}: {pg_type(tag)}  (source: {src_name}, type détecté: {tag})\")\n",
    "\n",
    "        if SHOW_SAMPLE:\n",
    "            with pd.option_context(\"display.max_columns\", 180, \"display.width\", 260):\n",
    "                print(\"\\nSample (top 10) :\")\n",
    "                print(casted_df.head(10))\n",
    "\n",
    "        done += 1\n",
    "\n",
    "        # ====== (OPTIONNEL) VUE CIBLE : si tu veux une table cible, remets STRICT_SOURCE_SCHEMA=False et implémente ici ======\n",
    "        if not STRICT_SOURCE_SCHEMA:\n",
    "            pass\n",
    "\n",
    "    print(f\"\\nDone. Feuilles analysées: {done}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03eef39-a59e-42e3-89de-29289b1d2da5",
   "metadata": {},
   "source": [
    "# Suivi Projets AEC - source.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafa3aa-24bf-410f-8d3d-02507da60b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, unicodedata, warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "FILE_PATH = r\"C:\\globasoft\\aerotech\\fic\\Suivi Projets AEC - source.xlsx\"\n",
    "SHOW_SAMPLE = True\n",
    "\n",
    "# (optionnel) masquer un warning bruyant de pandas\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Parsing dates in %Y-%m-%d %H:%M:%S format.*\", category=UserWarning)\n",
    "\n",
    "# ============== LOG ==============\n",
    "def now(): return datetime.now().strftime(\"%H:%M:%S\")\n",
    "def log(msg): print(f\"[{now()}] {msg}\", flush=True)\n",
    "\n",
    "# ============== UTILS ==============\n",
    "def strip_accents_lower(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def snake_id(s):\n",
    "    s = strip_accents_lower(s)\n",
    "    s = re.sub(r\"[\\s\\.\\-]+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"col\"\n",
    "\n",
    "def make_unique_columns(cols):\n",
    "    out, seen = [], {}\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            seen[c] = 1\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}_{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def _preclean_numeric_strings(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"Nettoyage avant parse : retire True/False, espaces insécables, milliers, virgule décimale.\"\"\"\n",
    "    s = x.astype(\"string\")\n",
    "    # True/False/Yes/No -> vide (on les considère comme 'pas de valeur' dans colonnes numériques)\n",
    "    s = s.str.replace(r\"^(true|false|yes|no|vrai|faux)$\", \"\", flags=re.I, regex=True)\n",
    "    s = s.str.replace(\"\\u00A0\",\" \", regex=False).str.replace(\" \",\"\", regex=False)\n",
    "    s = s.str.replace(r\"(?<=\\d)\\.(?=\\d{3}(?:\\D|$))\",\"\", regex=True)  # mille: 1.234 -> 1234\n",
    "    s = s.str.replace(\",\",\".\", regex=False)  # décimale: 1,23 -> 1.23\n",
    "    return s\n",
    "\n",
    "def parse_number_like(series: pd.Series) -> pd.Series:\n",
    "    s = _preclean_numeric_strings(series)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    blank_idx = []\n",
    "    for j in range(df.shape[1]):\n",
    "        s = df.iloc[:, j]\n",
    "        if s.dropna().astype(str).str.strip().eq(\"\").all():\n",
    "            blank_idx.append(j)\n",
    "    if blank_idx:\n",
    "        log(f\"[drop_empty_columns] suppression colonnes blanches (pos): {blank_idx}\")\n",
    "        df = df.drop(df.columns[blank_idx], axis=1)\n",
    "    return df\n",
    "\n",
    "# ============== HEADER PICK ==============\n",
    "HEADER_HINTS = (\n",
    "    # génériques\n",
    "    \"projet\",\"projets\",\"code\",\"id\",\"réf\",\"ref\",\"libelle\",\"libellé\",\"intitulé\",\n",
    "    \"client\",\"fournisseur\",\"chef de projet\",\"cp\",\"cdp\",\"manager\",\n",
    "    \"statut\",\"status\",\"phase\",\"categorie\",\"catégorie\",\"type\",\n",
    "    \"date\",\"heure\",\"time\",\"période\",\"periode\",\"echeance\",\"échéance\",\"fin\",\"début\",\"debut\",\n",
    "    \"budget\",\"montant\",\"cout\",\"coût\",\"prix\",\"ht\",\"ttc\",\"avancement\",\"progress\",\"%\",\"taux\",\n",
    "    \"site\",\"section analytique\",\"analytique\",\"commentaire\",\"observations\",\"notes\",\"remarques\",\n",
    "    # vus dans ton fichier\n",
    "    \"comments\",\"revue raf\",\"statut clipper\",\"gtm\",\"pôle\",\"pole\",\n",
    "    \"remaining forecast\",\"facturation mois\",\"hrs\",\"heures\",\"(h)\",\n",
    "    \"ca \",\"charges\",\"marge\",\"achats\",\"commandé\",\"enregistré\",\"fnp\",\"int\",\"raf\"\n",
    ")\n",
    "\n",
    "def choose_header_row(df: pd.DataFrame, scan=30) -> int:\n",
    "    limit = min(len(df), scan)\n",
    "    cand_idx, cand_hits = None, -1\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        hits = 0\n",
    "        for v in row:\n",
    "            t = strip_accents_lower(v)\n",
    "            if t and any(k in t for k in HEADER_HINTS):\n",
    "                hits += 1\n",
    "        if hits >= 2 and hits > cand_hits:\n",
    "            cand_idx, cand_hits = i, hits\n",
    "    if cand_idx is not None:\n",
    "        return cand_idx\n",
    "\n",
    "    best, idx = -1, 0\n",
    "    for i in range(limit):\n",
    "        row = df.iloc[i].astype(str)\n",
    "        non_empty = row.map(lambda x: x.strip()!=\"\").sum()\n",
    "        texty = row.map(lambda x: bool(re.search(r\"[A-Za-zÀ-ÿ]\", x))).sum()\n",
    "        score = non_empty*2 + texty\n",
    "        if score > best: best, idx = score, i\n",
    "    return idx\n",
    "\n",
    "# ============== INFÉRENCE TYPES (→ PostgreSQL) ==============\n",
    "MAP_TRUE  = {\"true\",\"vrai\",\"oui\",\"yes\",\"1\",\"y\",\"o\"}\n",
    "MAP_FALSE = {\"false\",\"faux\",\"non\",\"no\",\"0\",\"n\"}\n",
    "\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"(?:\\d{4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,2})|(?:\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4})|\"\n",
    "    r\"(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|janv|févr|fevr|avr|mai|juin|juil|sept|oct|nov|d[ée]c)\",\n",
    "    re.I\n",
    ")\n",
    "ISO_DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:[ T]\\d{2}:\\d{2}:\\d{2})?$\")\n",
    "\n",
    "# Catégories par en-tête (PRIMES sur le contenu)\n",
    "DATE_NAME_HINTS   = (\"date\",\"dt\",\"heure\",\"time\",\"echeance\",\"échéance\",\"debut\",\"début\",\"fin\",\"période\",\"periode\",\"semaine\",\"mois\",\"année\",\"annee\")\n",
    "MONEY_NAME_HINTS  = (\"€\",\"ht\",\"budget\",\"montant\",\"amount\",\"total\",\"prix\",\"coût\",\"cout\",\"facturation\",\"forecast\",\"achats\",\"commandé\",\"enregistré\",\"fnp\",\"ca \",\"charges\",\"marge\",\"int\",\"raf\")\n",
    "HOUR_NAME_HINTS   = (\"heures\",\"hrs\",\"(h)\",\" h \")\n",
    "PERCENT_NAME_HINTS= (\"%\", \"taux\", \"marge\", \"avancement\", \"écart\", \"ecart\", \"pourcent\")\n",
    "\n",
    "CODE_LIKE_HINTS   = (\"code\",\"nature\",\"plan\",\"compte\",\"sop\",\"section\",\"niveau\",\"trigramme\",\"immat\",\"ot\",\"ata\",\"wp\",\"n° dossier\",\"n_dossier\",\"dossier\",\"cat point\",\"cat pointage\",\"statut clipper\",\"gtm\",\"pôle\",\"pole\",\"comments\",\"cdp\",\"top custo\",\"réunion raf\",\"reunion raf\",\"statut suivi de projets\",\"statut suivi de projet\")\n",
    "\n",
    "def colname_has(hints, name):\n",
    "    n = strip_accents_lower(name or \"\")\n",
    "    return any(h in n for h in hints)\n",
    "\n",
    "def excel_serial_to_datetime(series: pd.Series) -> pd.Series:\n",
    "    vals = pd.to_numeric(series, errors=\"coerce\").astype(\"float64\")\n",
    "    vals[~np.isfinite(vals)] = np.nan\n",
    "    lower, upper = 60.0, 60000.0  # ~ 1900..2064\n",
    "    mask = (vals >= lower) & (vals <= upper)\n",
    "    n_out = int((~mask & ~pd.isna(vals)).sum())\n",
    "    if n_out:\n",
    "        log(f\"[excel_serial_to_datetime] valeurs hors fenêtre [{int(lower)}, {int(upper)}] -> NaN: {n_out}\")\n",
    "    base = pd.Timestamp(\"1899-12-30\")\n",
    "    td = pd.to_timedelta(vals.where(mask, np.nan), unit=\"D\", errors=\"coerce\")\n",
    "    dt = base + td\n",
    "    return pd.to_datetime(dt, errors=\"coerce\")\n",
    "\n",
    "def infer_col(col: pd.Series):\n",
    "    \"\"\"\n",
    "    Retourne (serie_casted, tag) avec tag ∈ {\"DATE\",\"DATETIME\",\"INT\",\"FLOAT\",\"BOOL\",\"STRING\"}.\n",
    "    PRÉCÉDENCE EN-TÊTE > CONTENU pour éviter les faux types.\n",
    "    \"\"\"\n",
    "    s = col.copy()\n",
    "    ss = s.astype(\"string\").str.strip()\n",
    "    name = s.name or \"\"\n",
    "\n",
    "    # 1) Colonnes € / CA / Charges / Marge / Facturation / Achats / RAF / Int  -> NUMÉRIQUE\n",
    "    if colname_has(MONEY_NAME_HINTS, name):\n",
    "        nums = parse_number_like(ss)\n",
    "        if nums.notna().any():\n",
    "            nz = nums.dropna()\n",
    "            # monétaire et métriques -> float par défaut\n",
    "            return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 2) Colonnes Heures -> NUMÉRIQUE (float)\n",
    "    if colname_has(HOUR_NAME_HINTS, name):\n",
    "        nums = parse_number_like(ss)\n",
    "        if nums.notna().any():\n",
    "            return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 3) Colonnes % / taux / avancement / écart -> NUMÉRIQUE (float)\n",
    "    if (\"%\" in (name or \"\")) or colname_has(PERCENT_NAME_HINTS, name):\n",
    "        nums = parse_number_like(ss)\n",
    "        if nums.notna().any():\n",
    "            return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 4) Dates ISO explicites\n",
    "    iso_mask = ss.fillna(\"\").str.match(ISO_DATE_RE)\n",
    "    if len(ss) and iso_mask.mean() >= 0.5:\n",
    "        dt_iso = pd.to_datetime(ss.where(iso_mask), errors=\"coerce\", dayfirst=False)\n",
    "        ok = dt_iso.notna().mean()\n",
    "        if ok >= 0.5:\n",
    "            has_time = (dt_iso.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                dt_iso.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else dt_iso.dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 5) Excel-serial -> Date : UNIQUEMENT si le nom évoque une date/heure\n",
    "    if colname_has(DATE_NAME_HINTS, name):\n",
    "        as_num = pd.to_numeric(ss, errors=\"coerce\")\n",
    "        safe_mask = (as_num >= 60) & (as_num <= 60000)\n",
    "        if (as_num.notna().mean() if len(ss) else 0.0) >= 0.9 and safe_mask.mean() >= 0.7:\n",
    "            parsed = excel_serial_to_datetime(as_num)\n",
    "            ok = parsed.notna().mean() if len(ss) else 0.0\n",
    "            if ok >= 0.7:\n",
    "                has_time = (parsed.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "                return (\n",
    "                    parsed.dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else parsed.dt.strftime(\"%Y-%m-%d\"),\n",
    "                    \"DATETIME\" if has_time else \"DATE\"\n",
    "                )\n",
    "\n",
    "    # 6) Tokens de date\n",
    "    date_token_ratio = ss.fillna(\"\").str.contains(DATE_TOKEN_RE, na=False).mean()\n",
    "    if colname_has(DATE_NAME_HINTS, name) or (date_token_ratio >= 0.30):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "            dt1 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=True,  cache=True)\n",
    "            dt2 = pd.to_datetime(ss, errors=\"coerce\", dayfirst=False, cache=True)\n",
    "        dt = dt2 if dt2.notna().sum() > dt1.notna().sum() else dt1\n",
    "        ok = dt.notna().mean() if len(ss) else 0.0\n",
    "        ok_thresh = 0.50 if colname_has(DATE_NAME_HINTS, name) else 0.70\n",
    "        if ok >= ok_thresh:\n",
    "            has_time = (dt.dt.time.astype(str) != \"00:00:00\").mean() > 0.2\n",
    "            return (\n",
    "                pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d %H:%M:%S\") if has_time else pd.to_datetime(dt).dt.strftime(\"%Y-%m-%d\"),\n",
    "                \"DATETIME\" if has_time else \"DATE\"\n",
    "            )\n",
    "\n",
    "    # 7) Bool strict (après les overrides ci-dessus)\n",
    "    mb = ss.str.lower().map(lambda x: True if x in MAP_TRUE else (False if x in MAP_FALSE else pd.NA))\n",
    "    if len(ss) and mb.notna().mean() >= 0.98:\n",
    "        return mb.astype(\"boolean\"), \"BOOL\"\n",
    "\n",
    "    # 8) Nombre générique\n",
    "    nums = parse_number_like(ss)\n",
    "    if (nums.notna().mean() if len(ss) else 0) >= 0.85:\n",
    "        nz = nums.dropna()\n",
    "        if len(nz) and (np.mod(nz, 1) == 0).all():\n",
    "            return nums.astype(\"Int64\"), \"INT\"\n",
    "        return nums.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # 9) Texte (codes/labels)\n",
    "    if colname_has(CODE_LIKE_HINTS, name):\n",
    "        return ss.astype(\"string\"), \"STRING\"\n",
    "    return ss.astype(\"string\"), \"STRING\"\n",
    "\n",
    "def pg_type(tag: str) -> str:\n",
    "    return {\n",
    "        \"DATE\":\"date\",\n",
    "        \"DATETIME\":\"timestamp without time zone\",\n",
    "        \"INT\":\"integer\",\n",
    "        \"FLOAT\":\"double precision\",\n",
    "        \"BOOL\":\"boolean\",\n",
    "        \"STRING\":\"text\"\n",
    "    }.get(tag, \"text\")\n",
    "\n",
    "# ============== MAIN ==============\n",
    "def main():\n",
    "    xlsx = Path(FILE_PATH)\n",
    "    log(f\"Fichier: {xlsx}\")\n",
    "    if not xlsx.exists():\n",
    "        log(\"[error] Fichier introuvable\"); return\n",
    "\n",
    "    try:\n",
    "        sheets = pd.read_excel(xlsx, sheet_name=None, engine=\"openpyxl\", header=None)\n",
    "    except Exception as e:\n",
    "        log(f\"[error] Lecture échouée: {e}\"); return\n",
    "\n",
    "    done = 0\n",
    "    for name, raw in sheets.items():\n",
    "        log(f\"\\n[feuille] {name}: shape initiale={raw.shape}\")\n",
    "\n",
    "        raw = raw.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "        log(f\"[feuille] {name}: après drop vides -> {raw.shape}\")\n",
    "        if raw.empty:\n",
    "            log(f\"[feuille] {name}: vide, on passe.\")\n",
    "            continue\n",
    "\n",
    "        h = choose_header_row(raw, scan=30)\n",
    "        log(f\"[feuille] {name}: header choisi à la ligne {h}\")\n",
    "\n",
    "        df_src = raw.iloc[h+1:].copy()\n",
    "        src_cols_original = raw.iloc[h].tolist()\n",
    "        df_src.columns = src_cols_original\n",
    "        df_src = df_src.dropna(how=\"all\").copy()\n",
    "        df_src = drop_empty_columns(df_src)\n",
    "        log(f\"[feuille] {name}: shape après affectation des en-têtes -> {df_src.shape}\")\n",
    "        if df_src.empty:\n",
    "            log(f\"[feuille] {name}: vide après normalisation, on passe.\")\n",
    "            continue\n",
    "\n",
    "        pg_cols = make_unique_columns([snake_id(c) for c in df_src.columns])\n",
    "\n",
    "        schema_src = {}\n",
    "        casted_df = df_src.copy()\n",
    "        for c in list(df_src.columns):\n",
    "            casted, tag = infer_col(df_src[c])\n",
    "            casted_df[c] = casted\n",
    "            schema_src[c] = tag\n",
    "\n",
    "        base = f\"{snake_id(xlsx.stem)}.{snake_id(name)}\"\n",
    "        print(f\"\\n--- Feuille: {name} -> {base} (SCHÉMA SOURCE STRICT / PostgreSQL) ---\")\n",
    "        print(f\"rows={len(casted_df)} | cols={casted_df.shape[1]}\")\n",
    "\n",
    "        print(\"\\nMapping colonnes (Excel → nom_pg) :\")\n",
    "        for src_name, pg_name in zip(df_src.columns, pg_cols):\n",
    "            print(f\"  - {src_name}  ->  {pg_name}\")\n",
    "\n",
    "        print(\"\\nSchema détecté (types PostgreSQL) :\")\n",
    "        for src_name, pg_name in zip(df_src.columns, pg_cols):\n",
    "            tag = schema_src.get(src_name, \"STRING\")\n",
    "            print(f\"  - {pg_name}: {pg_type(tag)}  (source: {src_name}, type détecté: {tag})\")\n",
    "\n",
    "        if SHOW_SAMPLE:\n",
    "            with pd.option_context(\"display.max_columns\", 220, \"display.width\", 260):\n",
    "                print(\"\\nSample (top 10) :\")\n",
    "                print(casted_df.head(10))\n",
    "\n",
    "        done += 1\n",
    "\n",
    "    print(f\"\\nDone. Feuilles analysées: {done}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ffa8a-32de-4cfa-b21c-b99e422057ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
