{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c19ec5-9cef-4d67-8334-eadca07bc02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== data header: folder info ===\n",
      "path: C:\\globasoft\\aerotech\\fic\n",
      "files_found: 0\n",
      "[warn] Aucun fichier .xlsx/.xlsm/.csv trouvé.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "\n",
    "# ---- Désactive l'avertissement bruyant de pandas sur l'inférence de format de date ----\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "\n",
    "# ================== PARAMS ================== #\n",
    "ROOT_DIR = Path(\"fic\")                # Dossier d'entrée (récursif)\n",
    "EXPORT_DIR = Path(\"out\")              # Dossier de sortie CSV (créé si export activé)\n",
    "EXPORT_TABLES = True                  # Mettre True pour exporter les tables détectées\n",
    "SHOW_SAMPLES = True                   # Afficher un aperçu (head) des tables détectées\n",
    "\n",
    "# Heuristiques génériques (adapter si besoin)\n",
    "MIN_COLS = 2\n",
    "MIN_CONSEC_ROWS = 5\n",
    "ROW_EMPTY_TOL = 1\n",
    "STOP_EMPTY_RUN = 5\n",
    "HEADER_SCAN_DEPTH = 6\n",
    "\n",
    "# Filtrage de colonnes après extraction (pour supprimer col/col_2… vides)\n",
    "MIN_NON_NULL_RATIO = 0.05\n",
    "MIN_NON_NULL_ABS   = 2\n",
    "\n",
    "# Inférence de types\n",
    "INFER_TYPES = True\n",
    "DATE_NAME_HINTS = (\"date\", \"dt_\", \"_dt\", \"attribution\", \"retrait\")\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"[/\\-.]|(?:jan|feb|mar|apr|mai|may|jun|jul|aug|sep|oct|nov|dec|\"\n",
    "    r\"janv|févr|fevr|avr|juil|sept|oct|nov|déc|dec)\",\n",
    "    re.I\n",
    ")\n",
    "BOOL_TRUE  = {\"true\",\"vrai\",\"oui\",\"y\",\"1\"}\n",
    "BOOL_FALSE = {\"false\",\"faux\",\"non\",\"n\",\"0\"}\n",
    "\n",
    "PRE_PARSE_TOKEN_RATIO_IF_NAME   = 0.10\n",
    "PRE_PARSE_TOKEN_RATIO_NO_NAME   = 0.30\n",
    "# ============================================ #\n",
    "\n",
    "def strip_accents_lower(s: str) -> str:\n",
    "    if s is None or pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def safe_read_excel_all_sheets(path: Path):\n",
    "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "    out = []\n",
    "    for sheet in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet, header=None)\n",
    "        out.append((sheet, df))\n",
    "    return out\n",
    "\n",
    "def pick_encoding(path: Path):\n",
    "    try:\n",
    "        from charset_normalizer import from_path\n",
    "        res = from_path(str(path)).best()\n",
    "        return res.encoding if res else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def read_csv_robust(path: Path):\n",
    "    enc_guess = pick_encoding(path)\n",
    "    candidates = [e for e in [enc_guess, \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin-1\", \"iso-8859-1\"] if e]\n",
    "    for enc in candidates:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=None, engine=\"python\", encoding=enc, header=None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", encoding=\"latin-1\", encoding_errors=\"replace\", header=None)\n",
    "\n",
    "def is_tabular_row(row, min_cols=MIN_COLS):\n",
    "    return row.notna().sum() >= min_cols\n",
    "\n",
    "def choose_header_row(block: pd.DataFrame):\n",
    "    best_idx = None\n",
    "    best_score = -1\n",
    "    limit = min(len(block), HEADER_SCAN_DEPTH)\n",
    "    for i in range(limit):\n",
    "        row = block.iloc[i]\n",
    "        vals = row.tolist()\n",
    "        non_empty = sum(pd.notna(v) and str(v).strip() != \"\" for v in vals)\n",
    "        texty = 0\n",
    "        for v in vals:\n",
    "            s = str(v).strip() if pd.notna(v) else \"\"\n",
    "            if s == \"\" or s.lower().startswith(\"unnamed\"):\n",
    "                continue\n",
    "            if re.search(r\"[A-Za-zÀ-ÿ]\", s):\n",
    "                texty += 1\n",
    "        score = non_empty * 2 + texty\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = i\n",
    "    return best_idx if best_idx is not None else 0\n",
    "\n",
    "def clean_columns(vals):\n",
    "    cols = []\n",
    "    seen = {}\n",
    "    for v in vals:\n",
    "        s = strip_accents_lower(v)\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip(\" -_\")\n",
    "        if s == \"\" or s.startswith(\"unnamed\"):\n",
    "            s = \"col\"\n",
    "        s = re.sub(r\"[^a-z0-9_ ]\", \"\", s)\n",
    "        s = re.sub(r\"\\s+\", \"_\", s).strip(\"_\")\n",
    "        if s == \"\":\n",
    "            s = \"col\"\n",
    "        if s in seen:\n",
    "            seen[s] += 1\n",
    "            s = f\"{s}_{seen[s]}\"\n",
    "        else:\n",
    "            seen[s] = 1\n",
    "        cols.append(s)\n",
    "    return cols\n",
    "\n",
    "# --- NEW: util pour savoir si un nom de colonne est \"générique\" (à autoriser au prune) ---\n",
    "GENERIC_COL_RE = re.compile(r\"^col(_\\d+)?$\", re.I)\n",
    "def is_generic_colname(name: str) -> bool:\n",
    "    return bool(GENERIC_COL_RE.fullmatch(name or \"\"))\n",
    "\n",
    "def detect_blocks(df: pd.DataFrame):\n",
    "    if list(df.columns) != list(range(df.shape[1])):\n",
    "        df = df.copy()\n",
    "        df.columns = list(range(df.shape[1]))\n",
    "\n",
    "    blocks = []\n",
    "    consec = 0\n",
    "    empties_inside = 0\n",
    "    start = None\n",
    "\n",
    "    for i in df.index:\n",
    "        row = df.loc[i]\n",
    "        if is_tabular_row(row):\n",
    "            if start is None:\n",
    "                start = i\n",
    "                consec = 0\n",
    "                empties_inside = 0\n",
    "            consec += 1\n",
    "        else:\n",
    "            if start is not None:\n",
    "                empties_inside += 1\n",
    "                if empties_inside > ROW_EMPTY_TOL:\n",
    "                    end = i - (empties_inside)\n",
    "                    if end >= start and consec >= MIN_CONSEC_ROWS:\n",
    "                        blocks.append((start, end))\n",
    "                    start = None\n",
    "                    consec = 0\n",
    "                    empties_inside = 0\n",
    "\n",
    "    if start is not None and consec >= MIN_CONSEC_ROWS:\n",
    "        blocks.append((start, df.index[-1]))\n",
    "\n",
    "    return blocks\n",
    "\n",
    "# ---------- CHANGED: prune_columns conserve toutes les colonnes à nom SIGNIFICATIF ----------\n",
    "def prune_columns(df: pd.DataFrame, header_keep: set[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Garde:\n",
    "      - toutes les colonnes dont le nom d'en-tête est significatif (≠ 'col', 'col_2', ...)\n",
    "      - et, parmi les noms génériques, celles qui ont assez de données.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # On ne supprime jamais les colonnes d'en-tête \"significatives\"\n",
    "    keep = list(header_keep)\n",
    "\n",
    "    # Évalue les colonnes génériques restantes\n",
    "    n = len(df)\n",
    "    for c in df.columns:\n",
    "        if c in header_keep:\n",
    "            continue\n",
    "        if not is_generic_colname(c):\n",
    "            # par sécurité : conserver aussi toute colonne non générique\n",
    "            keep.append(c)\n",
    "            continue\n",
    "        nnz = df[c].notna().sum()\n",
    "        if nnz >= max(MIN_NON_NULL_ABS, int(n * MIN_NON_NULL_RATIO)):\n",
    "            keep.append(c)\n",
    "\n",
    "    # Conserver l'ordre original\n",
    "    keep_ordered = [c for c in df.columns if c in keep]\n",
    "    return df[keep_ordered].copy()\n",
    "\n",
    "# ---------- Inférence de types & affichage schéma ---------- #\n",
    "\n",
    "def infer_and_cast_column(s: pd.Series, col_name: str) -> tuple[pd.Series, str]:\n",
    "    \"\"\"\n",
    "    Types possibles: date, int, float, bool, string.\n",
    "    \"\"\"\n",
    "    name_norm = strip_accents_lower(col_name)\n",
    "    looks_like_date_name = any(h in name_norm for h in DATE_NAME_HINTS)\n",
    "\n",
    "    s_obj = s.astype(\"string\")\n",
    "    non_empty = s_obj.dropna()\n",
    "\n",
    "    numeric_only_ratio = 0.0\n",
    "    if len(non_empty) > 0:\n",
    "        numeric_only_ratio = sum(bool(re.fullmatch(r\"\\d+(?:[.,]\\d+)?\", str(x).strip()))\n",
    "                                 for x in non_empty) / len(non_empty)\n",
    "\n",
    "    date_token_ratio = 0.0\n",
    "    if len(non_empty) > 0:\n",
    "        date_token_ratio = sum(bool(DATE_TOKEN_RE.search(str(x)))\n",
    "                               for x in non_empty) / len(non_empty)\n",
    "\n",
    "    should_try_parse = (\n",
    "        (looks_like_date_name and date_token_ratio >= PRE_PARSE_TOKEN_RATIO_IF_NAME) or\n",
    "        ((not looks_like_date_name) and date_token_ratio >= PRE_PARSE_TOKEN_RATIO_NO_NAME and numeric_only_ratio < 0.80)\n",
    "    )\n",
    "\n",
    "    if should_try_parse:\n",
    "        parsed_dates = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "        date_ratio = parsed_dates.notna().sum() / max(1, s.notna().sum())\n",
    "        accept_date = (date_ratio >= 0.30) if looks_like_date_name else (date_ratio >= 0.70)\n",
    "        if accept_date:\n",
    "            return parsed_dates.dt.normalize(), \"date\"\n",
    "\n",
    "    as_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    num_ratio = as_num.notna().sum() / max(1, s.notna().sum())\n",
    "    if num_ratio >= 0.85:\n",
    "        as_int = as_num.dropna()\n",
    "        if len(as_int) == 0:\n",
    "            return as_num.astype(\"Float64\"), \"float\"\n",
    "        if (as_int % 1 == 0).all():\n",
    "            return as_num.astype(\"Int64\"), \"int\"\n",
    "        else:\n",
    "            return as_num.astype(\"Float64\"), \"float\"\n",
    "\n",
    "    vals = non_empty.map(strip_accents_lower).unique().tolist()\n",
    "    small_set = set(vals)\n",
    "    if 1 <= len(small_set) <= 3:\n",
    "        mapped = s.astype(str).map(strip_accents_lower)\n",
    "        def map_bool(x):\n",
    "            if x in BOOL_TRUE: return True\n",
    "            if x in BOOL_FALSE: return False\n",
    "            return pd.NA\n",
    "        mb = mapped.map(map_bool)\n",
    "        if mb.notna().sum() / max(1, mapped.notna().sum()) >= 0.9:\n",
    "            return mb.astype(\"boolean\"), \"bool\"\n",
    "\n",
    "    return s.astype(\"string\"), \"string\"\n",
    "\n",
    "def infer_types_df(df: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    schema = {}\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        out[c], t = infer_and_cast_column(out[c], c)\n",
    "        schema[c] = t\n",
    "    return out, schema\n",
    "\n",
    "def show_schema(df: pd.DataFrame, schema: dict):\n",
    "    print(\"\\n=== schema ===\")\n",
    "    for c in df.columns:\n",
    "        t = schema.get(c, str(df[c].dtype)).upper()\n",
    "        print(f\"{c}: {t}\")\n",
    "\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "def carve_table_from_block(df_block: pd.DataFrame):\n",
    "    if df_block.empty:\n",
    "        return None\n",
    "\n",
    "    # header\n",
    "    h_rel = choose_header_row(df_block)\n",
    "    raw_header_vals = df_block.iloc[h_rel].tolist()\n",
    "    cols = clean_columns(raw_header_vals)\n",
    "\n",
    "    data = df_block.iloc[h_rel+1:].copy()\n",
    "    data.columns = cols\n",
    "\n",
    "    # stop à N lignes vides consécutives après le header\n",
    "    empty_run = 0\n",
    "    cut_idx = data.index[-1]\n",
    "    for idx in data.index:\n",
    "        if data.loc[idx].isna().all():\n",
    "            empty_run += 1\n",
    "            if empty_run >= STOP_EMPTY_RUN:\n",
    "                cut_idx = idx - STOP_EMPTY_RUN\n",
    "                break\n",
    "        else:\n",
    "            empty_run = 0\n",
    "\n",
    "    data = data.loc[:cut_idx]\n",
    "    data = data[data.notna().sum(axis=1) >= MIN_COLS]\n",
    "\n",
    "    # --- NEW: déterminer quelles colonnes d'en-tête sont \"significatives\"\n",
    "    header_keep = set()\n",
    "    for c in cols:\n",
    "        if not is_generic_colname(c):\n",
    "            header_keep.add(c)\n",
    "\n",
    "    # prune en gardant toujours les colonnes nommées (ex: 'mois', 'tarif_facture', etc.)\n",
    "    data = prune_columns(data, header_keep)\n",
    "    if data.empty:\n",
    "        return None\n",
    "\n",
    "    # inférence & cast de types\n",
    "    if INFER_TYPES:\n",
    "        data, schema = infer_types_df(data)\n",
    "    else:\n",
    "        schema = {c: str(data[c].dtype) for c in data.columns}\n",
    "\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # dates en YYYY-MM-DD (affichage + CSV)\n",
    "    for c, t in schema.items():\n",
    "        if t == \"date\":\n",
    "            try:\n",
    "                data[c] = pd.to_datetime(data[c], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return data, schema\n",
    "\n",
    "def find_tables_in_sheet(df_raw: pd.DataFrame):\n",
    "    if list(df_raw.columns) != list(range(df_raw.shape[1])):\n",
    "        df_raw = df_raw.copy()\n",
    "        df_raw.columns = list(range(df_raw.shape[1]))\n",
    "\n",
    "    blocks = detect_blocks(df_raw)\n",
    "\n",
    "    tables = []\n",
    "    for (start, end) in blocks:\n",
    "        block = df_raw.loc[start:end, :]\n",
    "        carved = carve_table_from_block(block)\n",
    "        if carved is None:\n",
    "            continue\n",
    "        table, schema = carved\n",
    "        if table is not None and table.shape[1] >= MIN_COLS and table.shape[0] >= 1:\n",
    "            tables.append((table, schema))\n",
    "    return tables\n",
    "\n",
    "def process_file(path: Path):\n",
    "    results = []\n",
    "    if path.suffix.lower() in (\".xlsx\", \".xlsm\"):\n",
    "        sheets = safe_read_excel_all_sheets(path)\n",
    "        for sheet, df_raw in sheets:\n",
    "            tables = find_tables_in_sheet(df_raw)\n",
    "            for i, (t, sc) in enumerate(tables, start=1):\n",
    "                results.append((sheet, i, t, sc))\n",
    "    elif path.suffix.lower() == \".csv\":\n",
    "        df_raw = read_csv_robust(path)\n",
    "        tables = find_tables_in_sheet(df_raw)\n",
    "        for i, (t, sc) in enumerate(tables, start=1):\n",
    "            results.append((None, i, t, sc))\n",
    "    else:\n",
    "        raise ValueError(f\"Extension non gérée: {path.suffix}\")\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    if not ROOT_DIR.exists():\n",
    "        print(f\"[error] Dossier introuvable : {ROOT_DIR.resolve()}\")\n",
    "        return\n",
    "\n",
    "    files = []\n",
    "    for pat in (\"*.xlsx\", \"*.xlsm\", \"*.csv\"):\n",
    "        files += [p for p in ROOT_DIR.rglob(pat) if p.is_file() and not p.name.startswith(\"~$\")]\n",
    "\n",
    "    print(\"=== data header: folder info ===\")\n",
    "    print(f\"path: {ROOT_DIR.resolve()}\")\n",
    "    print(f\"files_found: {len(files)}\")\n",
    "\n",
    "    if not files:\n",
    "        print(\"[warn] Aucun fichier .xlsx/.xlsm/.csv trouvé.\")\n",
    "        return\n",
    "\n",
    "    if EXPORT_TABLES:\n",
    "        EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for f in sorted(files):\n",
    "        print(\"\\n=== file ===\")\n",
    "        print(f\"{f.name}  ({f.resolve()})\")\n",
    "\n",
    "        try:\n",
    "            tables = process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Lecture échouée pour {f.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not tables:\n",
    "            print(\"[info] Aucune table détectée\")\n",
    "            continue\n",
    "\n",
    "        for sheet, idx, df, schema in tables:\n",
    "            title = f\"{f.stem}__{sheet or 'sheet'}__table_{idx}\"\n",
    "            print(f\"\\n--- table detected ---\")\n",
    "            print(f\"name: {title}\")\n",
    "            print(f\"rows: {len(df)} | cols: {df.shape[1]}\")\n",
    "            print(\"columns:\", \", \".join(map(str, df.columns.tolist())))\n",
    "\n",
    "            show_schema(df, schema)\n",
    "\n",
    "            if SHOW_SAMPLES:\n",
    "                with pd.option_context(\"display.max_columns\", 80, \"display.width\", 200):\n",
    "                    print(\"\\n=== sample (top 8) ===\")\n",
    "                    print(df.head(8))\n",
    "\n",
    "            if EXPORT_TABLES:\n",
    "                out_path = EXPORT_DIR / f\"{title}.csv\"\n",
    "                df.to_csv(out_path, index=False)\n",
    "                print(f\"[saved] {out_path}\")\n",
    "\n",
    "    print(\"\\n=== done ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedbf3e-5480-49ff-9106-38db1c92c11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
