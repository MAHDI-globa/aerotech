{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee57f4b-4e4a-42ae-9683-343774c348f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyxlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c19ec5-9cef-4d67-8334-eadca07bc02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== data header: folder info ===\n",
      "path: C:\\globasoft\\aerotech\\fic\n",
      "files_found: 1\n",
      "\n",
      "=== config ===\n",
      "QUICK_MODE=True | INFER_TYPES=False | VERBOSE=True\n",
      "SCAN_MAX_ROWS=8000 | MAX_FILES=None | MAX_SHEETS=None | MAX_TABLES_PER_SHEET=2\n",
      "MIN_COLS=2 | MIN_CONSEC_ROWS=3 | HEADER_SCAN_DEPTH=3\n",
      "COL_DENSITY_THRESHOLD=0.12 | MIN_COL_RUN=2 | ALLOW_SMALL_GAPS=1\n",
      "\n",
      "=== file ===\n",
      "Tauxdechange.xlsx  (C:\\globasoft\\aerotech\\fic\\Tauxdechange.xlsx)\n",
      "[14:46:04]   -> Ouverture Excel (openpyxl)\n",
      "[14:46:04]   -> Feuilles détectées: 2 (limite=Aucune)\n",
      "[14:46:04]     .. Lecture feuille 1/2: '2024' nrows=8000\n",
      "[14:46:04]        -> taille: 257x12 (en 0.08s)\n",
      "[14:46:04]     .. Lecture feuille 2/2: '2025' nrows=8000\n",
      "[14:46:04]        -> taille: 129x12 (en 0.05s)\n",
      "[14:46:04]   -> Excel chargé en 0.70s\n",
      "[14:46:04] -- Feuille: 2024 | taille 257x12\n",
      "  -> Détection des blocs tabulaires…\n",
      "[14:46:04]   -> Segments verticaux: 1\n",
      "[14:46:04]   -> Segment 1: cols 0..11 (shape 257x12)\n",
      "[14:46:04]     .. Blocs tabulaires détectés (vertical/lignes): 1 (en 0.00s)\n",
      "[14:46:04]      -> Carve table 1/1 dans segment 1 (rows 0..256)\n",
      "[14:46:04]     .. Sélection header\n",
      "[14:46:04]     .. Header choisi ligne relative 0 -> 12 colonnes\n",
      "[14:46:04]     .. Après nettoyage lignes: 256x12\n",
      "[14:46:04]     .. Après prune colonnes: 256x12\n",
      "[14:46:04]     .. Inférence types: 12 colonnes (en 0.01s, activée=False)\n",
      "[14:46:04]         .. table retenue: 256x12\n",
      "[14:46:04]   -> Tables retenues sur la feuille: 1\n",
      "[14:46:04] -- Feuille: 2025 | taille 129x12\n",
      "  -> Détection des blocs tabulaires…\n",
      "[14:46:04]   -> Segments verticaux: 1\n",
      "[14:46:04]   -> Segment 1: cols 0..11 (shape 129x12)\n",
      "[14:46:04]     .. Blocs tabulaires détectés (vertical/lignes): 1 (en 0.00s)\n",
      "[14:46:04]      -> Carve table 1/1 dans segment 1 (rows 0..128)\n",
      "[14:46:04]     .. Sélection header\n",
      "[14:46:04]     .. Header choisi ligne relative 0 -> 12 colonnes\n",
      "[14:46:04]     .. Après nettoyage lignes: 128x12\n",
      "[14:46:05]     .. Après prune colonnes: 128x12\n",
      "[14:46:05]     .. Inférence types: 12 colonnes (en 0.02s, activée=False)\n",
      "[14:46:05]         .. table retenue: 128x12\n",
      "[14:46:05]   -> Tables retenues sur la feuille: 1\n",
      "\n",
      "--- table detected ---\n",
      "name: tauxdechange.2024.table_1_1\n",
      "rows: 256 | cols: 12\n",
      "columns: date, eur, usd, gbp, jpy, chf, aud, cad, cny, sek, nzd, zar\n",
      "\n",
      "=== schema ===\n",
      "date: STRING\n",
      "eur: STRING\n",
      "usd: STRING\n",
      "gbp: STRING\n",
      "jpy: STRING\n",
      "chf: STRING\n",
      "aud: STRING\n",
      "cad: STRING\n",
      "cny: STRING\n",
      "sek: STRING\n",
      "nzd: STRING\n",
      "zar: STRING\n",
      "\n",
      "=== sample (top 8) ===\n",
      "                  date eur      usd     gbp      jpy     chf      aud      cad      cny      sek      nzd      zar\n",
      "0  2024-01-02 00:00:00   1  0.91274  1.1541  0.00642  1.0747  0.61931  0.68658  0.12777  0.08965  0.57238   0.0491\n",
      "1  2024-01-03 00:00:00   1  0.91583  1.1565   0.0064  1.0727  0.61592  0.68615  0.12811  0.08935  0.57094   0.0487\n",
      "2  2024-01-04 00:00:00   1  0.91299   1.159  0.00633  1.0738  0.61425  0.68479  0.12767  0.08936  0.57052  0.04895\n",
      "3  2024-01-05 00:00:00   1  0.91567    1.16  0.00631   1.073  0.61211  0.68493  0.12799  0.08901  0.56935  0.04866\n",
      "4  2024-01-08 00:00:00   1  0.91358  1.1608  0.00632  1.0743  0.61058  0.68255  0.12756  0.08921  0.56796  0.04879\n",
      "5  2024-01-09 00:00:00   1  0.91408  1.1636  0.00634  1.0738  0.61203  0.68432  0.12758  0.08919  0.57058  0.04893\n",
      "6  2024-01-10 00:00:00   1  0.91358  1.1625  0.00629  1.0711  0.61222  0.68264  0.12743  0.08931  0.56925  0.04899\n",
      "7  2024-01-11 00:00:00   1  0.91017  1.1608  0.00626  1.0709  0.61091  0.68134  0.12715  0.08931  0.56893  0.04894\n",
      "\n",
      "--- table detected ---\n",
      "name: tauxdechange.2025.table_1_1\n",
      "rows: 128 | cols: 12\n",
      "columns: date, eur, usd, gbp, jpy, chf, aud, cad, cny, sek, nzd, zar\n",
      "\n",
      "=== schema ===\n",
      "date: STRING\n",
      "eur: STRING\n",
      "usd: STRING\n",
      "gbp: STRING\n",
      "jpy: STRING\n",
      "chf: STRING\n",
      "aud: STRING\n",
      "cad: STRING\n",
      "cny: STRING\n",
      "sek: STRING\n",
      "nzd: STRING\n",
      "zar: STRING\n",
      "\n",
      "=== sample (top 8) ===\n",
      "                  date eur      usd     gbp      jpy     chf      aud      cad      cny      sek      nzd      zar\n",
      "0  2025-01-02 00:00:00   1   0.9689  1.2031  0.00617  1.0671  0.60176  0.67182  0.13274  0.08755  0.54342  0.05189\n",
      "1  2025-01-03 00:00:00   1  0.97097  1.2049  0.00618  1.0681  0.60401  0.67376  0.13268  0.08742  0.54448  0.05183\n",
      "2  2025-01-06 00:00:00   1  0.95914  1.2034  0.00613  1.0643  0.60405  0.67051  0.13109  0.08723  0.54484  0.05189\n",
      "3  2025-01-07 00:00:00   1  0.96219  1.2061   0.0061  1.0609  0.60397  0.67213  0.13133  0.08715  0.54594  0.05176\n",
      "4  2025-01-08 00:00:00   1   0.9722   1.199  0.00614  1.0662  0.60241  0.67554   0.1326  0.08686  0.54398  0.05145\n",
      "5  2025-01-09 00:00:00   1   0.9704  1.1932  0.00615  1.0642  0.60132  0.67426  0.13235    0.087  0.54298   0.0514\n",
      "6  2025-01-10 00:00:00   1   0.9705  1.1948  0.00614   1.062  0.60042  0.67363  0.13238  0.08698  0.54189  0.05098\n",
      "7  2025-01-13 00:00:00   1  0.98058  1.1882  0.00624    1.07  0.60321  0.68032  0.13374  0.08691  0.54472  0.05122\n",
      "[info] Temps fichier: 1.01s\n",
      "\n",
      "=== done ===\n",
      "Tables: 2 | Feuilles (approx): 2 | Fichiers: 1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# --------- Réduire le bruit ---------\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not infer format.*\", category=UserWarning)\n",
    "\n",
    "# ================== PARAMS ================== #\n",
    "ROOT_DIR = Path(\"fic\")\n",
    "SHOW_SAMPLES = True\n",
    "\n",
    "# >>> Perf toggles\n",
    "QUICK_MODE = True              # True = plus rapide (moins d'inférence & heuristiques allégées)\n",
    "INFER_TYPES = not QUICK_MODE   # on désactive l'inférence en mode rapide\n",
    "VERBOSE = True                 # <== afficher le détail des étapes\n",
    "\n",
    "# Limiteurs (pour gros dossiers/fichiers)\n",
    "MAX_FILES = None               # ex: 5 pour limiter aux 5 premiers fichiers\n",
    "MAX_SHEETS = None              # ex: 3 pour limiter aux 3 premiers onglets par fichier\n",
    "SCAN_MAX_ROWS = 8000           # n-rows max par feuille (None = tout lire)\n",
    "MAX_TABLES_PER_SHEET = 2       # None = pas de limite\n",
    "\n",
    "# Heuristiques génériques (vertical = par lignes)\n",
    "MIN_COLS = 2\n",
    "MIN_CONSEC_ROWS = 5 if not QUICK_MODE else 3\n",
    "ROW_EMPTY_TOL = 1\n",
    "STOP_EMPTY_RUN = 5 if not QUICK_MODE else 3\n",
    "HEADER_SCAN_DEPTH = 6 if not QUICK_MODE else 3\n",
    "\n",
    "# >>> NEW: segmentation verticale (pour tables côte-à-côte)\n",
    "COL_DENSITY_THRESHOLD = 0.12   # min ratio de non-nuls pour considérer une colonne \"utilisée\"\n",
    "MIN_COL_RUN = 2                # nb min de colonnes consécutives pour former un segment vertical\n",
    "ALLOW_SMALL_GAPS = 1           # tolère jusqu'à N colonnes vides à l'intérieur d'un segment\n",
    "MAX_SEGMENTS_PER_SHEET = None  # limiter le nombre de segments par feuille (None = illimité)\n",
    "\n",
    "# Filtrage colonnes “génériques”\n",
    "MIN_NON_NULL_RATIO = 0.05\n",
    "MIN_NON_NULL_ABS   = 2\n",
    "GENERIC_COL_RE = re.compile(r\"^col(_\\d+)?$\", re.I)\n",
    "\n",
    "# Inférence (si activée)\n",
    "DATE_NAME_HINTS = (\"date\", \"dt_\", \"_dt\", \"attribution\", \"retrait\", \"month\", \"mois\")\n",
    "DATE_TOKEN_RE = re.compile(\n",
    "    r\"[/\\-.]|(?:jan|feb|mar|apr|mai|may|jun|jul|aug|sep|oct|nov|dec|\"\n",
    "    r\"janv|févr|fevr|avr|juil|sept|oct|nov|déc|dec)\",\n",
    "    re.I\n",
    ")\n",
    "BOOL_TRUE  = {\"true\",\"vrai\",\"oui\",\"y\",\"1\"}\n",
    "BOOL_FALSE = {\"false\",\"faux\",\"non\",\"n\",\"0\"}\n",
    "PRE_PARSE_TOKEN_RATIO_IF_NAME   = 0.10\n",
    "PRE_PARSE_TOKEN_RATIO_NO_NAME   = 0.30\n",
    "\n",
    "# -------------------- logging utils -------------------- #\n",
    "def nowstr(): return datetime.now().strftime(\"%H:%M:%S\")\n",
    "def log(msg):\n",
    "    if VERBOSE:\n",
    "        print(f\"[{nowstr()}] {msg}\", flush=True)\n",
    "\n",
    "def step_time(prev=None):\n",
    "    t = time.perf_counter()\n",
    "    if prev is None:\n",
    "        return t, 0.0\n",
    "    return t, (t - prev)\n",
    "\n",
    "# -------------------- utils noms -------------------- #\n",
    "def strip_accents_lower(s: str) -> str:\n",
    "    if s is None or pd.isna(s): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "def safe_id(s: str) -> str:\n",
    "    \"\"\"Normalise un nom (fichier/onglet) pour un identifiant exploitable: minuscules, accents out, non-alnum->'_'.\"\"\"\n",
    "    s = strip_accents_lower(s)\n",
    "    s = re.sub(r\"[\\s\\.\\-]+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"sheet\"\n",
    "\n",
    "def is_generic_colname(name: str) -> bool:\n",
    "    return bool(GENERIC_COL_RE.fullmatch(name or \"\"))\n",
    "\n",
    "# ---------- Lecture Excel rapide ----------\n",
    "def safe_read_excel_all_sheets(path: Path):\n",
    "    \"\"\"\n",
    "    .xlsx/.xlsm: openpyxl ; .xlsb: pyxlsb\n",
    "    Si SCAN_MAX_ROWS est défini, on tronque la lecture à SCAN_MAX_ROWS pour accélérer.\n",
    "    \"\"\"\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\"):\n",
    "        engine = \"openpyxl\"\n",
    "    elif ext == \".xlsb\":\n",
    "        engine = \"pyxlsb\"\n",
    "    else:\n",
    "        raise ValueError(f\"Extension Excel non gérée: {ext}\")\n",
    "\n",
    "    log(f\"  -> Ouverture Excel ({engine})\")\n",
    "    t0, _ = step_time()\n",
    "    xls = pd.ExcelFile(path, engine=engine)\n",
    "    sheets = xls.sheet_names[:(MAX_SHEETS or len(xls.sheet_names))]\n",
    "    log(f\"  -> Feuilles détectées: {len(sheets)} (limite={MAX_SHEETS or 'Aucune'})\")\n",
    "\n",
    "    out = []\n",
    "    nrows = SCAN_MAX_ROWS if SCAN_MAX_ROWS is not None else None\n",
    "    for i, sheet in enumerate(sheets, 1):\n",
    "        ti, _ = step_time()\n",
    "        log(f\"    .. Lecture feuille {i}/{len(sheets)}: '{sheet}' nrows={nrows or 'ALL'}\")\n",
    "        df = pd.read_excel(xls, sheet_name=sheet, header=None, engine=engine, nrows=nrows)\n",
    "        _, dt = step_time(ti)\n",
    "        log(f\"       -> taille: {df.shape[0]}x{df.shape[1]} (en {dt:.2f}s)\")\n",
    "        out.append((sheet, df))\n",
    "    _, dt_total = step_time(t0)\n",
    "    log(f\"  -> Excel chargé en {dt_total:.2f}s\")\n",
    "    return out\n",
    "\n",
    "# ---------- Lecture CSV rapide ----------\n",
    "COMMON_SEPS = [\",\", \";\", \"\\t\", \"|\"]\n",
    "COMMON_ENCODINGS = [\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin-1\"]\n",
    "\n",
    "def guess_sep(first_line: str):\n",
    "    counts = {sep: first_line.count(sep) for sep in COMMON_SEPS}\n",
    "    sep = max(counts, key=counts.get)\n",
    "    return sep if counts[sep] > 0 else \",\"\n",
    "\n",
    "def read_csv_fast(path: Path):\n",
    "    t0, _ = step_time()\n",
    "    log(\"  -> Détection encodage/séparateur (rapide)\")\n",
    "    enc_used = None\n",
    "    sep_used = \",\"\n",
    "    tried = []\n",
    "    try:\n",
    "        for enc in COMMON_ENCODINGS:\n",
    "            tried.append(enc)\n",
    "            with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n",
    "                head = f.readline()\n",
    "            enc_used = enc\n",
    "            sep_used = guess_sep(head)\n",
    "            break\n",
    "    except Exception:\n",
    "        enc_used = \"latin-1\"\n",
    "        with open(path, \"r\", encoding=enc_used, errors=\"replace\") as f:\n",
    "            head = f.readline()\n",
    "        sep_used = guess_sep(head)\n",
    "    log(f\"     -> encodage: {enc_used} (essais={tried}); sep='{sep_used}'\")\n",
    "\n",
    "    nrows = SCAN_MAX_ROWS if SCAN_MAX_ROWS is not None else None\n",
    "    log(f\"  -> Lecture CSV nrows={nrows or 'ALL'}\")\n",
    "    df = pd.read_csv(path, sep=sep_used, encoding=enc_used, header=None, nrows=nrows)\n",
    "    _, dt = step_time(t0)\n",
    "    log(f\"     -> taille: {df.shape[0]}x{df.shape[1]} (en {dt:.2f}s)\")\n",
    "    return df\n",
    "\n",
    "# ---------- Pré-traitements & détection ----------\n",
    "def ensure_range_columns(df: pd.DataFrame):\n",
    "    if list(df.columns) != list(range(df.shape[1])):\n",
    "        df = df.copy()\n",
    "        df.columns = list(range(df.shape[1]))\n",
    "    return df\n",
    "\n",
    "def detect_blocks(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renvoie les (start, end) de runs True dans le masque tabulaire (>= MIN_COLS non nuls).\n",
    "    Version vectorisée très rapide.\n",
    "    \"\"\"\n",
    "    t0, _ = step_time()\n",
    "    df = ensure_range_columns(df)\n",
    "    row_nnz = df.notna().sum(axis=1).to_numpy()\n",
    "    tab = row_nnz >= MIN_COLS\n",
    "    blocks = []\n",
    "    if tab.any():\n",
    "        padded = np.r_[False, tab, False]\n",
    "        starts = np.where((~padded[:-1]) & (padded[1:]))[0]\n",
    "        ends   = np.where((padded[:-1]) & (~padded[1:]))[0] - 1\n",
    "        for s, e in zip(starts, ends):\n",
    "            if (e - s + 1) >= MIN_CONSEC_ROWS:\n",
    "                blocks.append((df.index[s], df.index[e]))\n",
    "                if QUICK_MODE and MAX_TABLES_PER_SHEET and len(blocks) >= MAX_TABLES_PER_SHEET:\n",
    "                    break\n",
    "    _, dt = step_time(t0)\n",
    "    log(f\"    .. Blocs tabulaires détectés (vertical/lignes): {len(blocks)} (en {dt:.2f}s)\")\n",
    "    return blocks\n",
    "\n",
    "# ---------- Segmentation verticale (tables côte-à-côte) ----------\n",
    "def split_by_vertical_gaps(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Retourne une liste de segments (sub_df, start_col, end_col) correspondant à des groupes de colonnes \"denses\".\n",
    "    \"\"\"\n",
    "    if df.empty or df.shape[1] <= 1:\n",
    "        return [(df, 0, df.shape[1]-1)] if df.shape[1] else []\n",
    "\n",
    "    dens = df.notna().mean(axis=0).to_numpy()  # ratio non-null par colonne\n",
    "    used = dens >= COL_DENSITY_THRESHOLD\n",
    "\n",
    "    # ponte les petits trous entre colonnes utilisées\n",
    "    if ALLOW_SMALL_GAPS > 0:\n",
    "        n = len(used)\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            if not used[i]:\n",
    "                j = i\n",
    "                while j < n and not used[j]:\n",
    "                    j += 1\n",
    "                gap_len = j - i\n",
    "                left_used = (i-1 >= 0 and used[i-1])\n",
    "                right_used = (j < n and used[j])\n",
    "                if left_used and right_used and gap_len <= ALLOW_SMALL_GAPS:\n",
    "                    used[i:j] = True\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    segments = []\n",
    "    padded = np.r_[False, used, False]\n",
    "    starts = np.where((~padded[:-1]) & (padded[1:]))[0]\n",
    "    ends   = np.where((padded[:-1]) & (~padded[1:]))[0] - 1\n",
    "\n",
    "    for s, e in zip(starts, ends):\n",
    "        if (e - s + 1) >= MIN_COL_RUN:\n",
    "            sub = df.iloc[:, s:e+1]\n",
    "            segments.append((sub, s, e))\n",
    "\n",
    "    return segments if segments else [(df, 0, df.shape[1]-1)]\n",
    "\n",
    "# ---------- Colonnes ----------\n",
    "def choose_header_row(block: pd.DataFrame):\n",
    "    best_idx, best_score = None, -1\n",
    "    limit = min(len(block), HEADER_SCAN_DEPTH)\n",
    "    for i in range(limit):\n",
    "        row = block.iloc[i]\n",
    "        vals = row.tolist()\n",
    "        non_empty = sum(pd.notna(v) and str(v).strip() != \"\" for v in vals)\n",
    "        texty = 0\n",
    "        for v in vals:\n",
    "            s = str(v).strip() if pd.notna(v) else \"\"\n",
    "            if s and not s.lower().startswith(\"unnamed\") and re.search(r\"[A-Za-zÀ-ÿ]\", s):\n",
    "                texty += 1\n",
    "        score = non_empty * 2 + texty\n",
    "        if score > best_score:\n",
    "            best_score, best_idx = score, i\n",
    "    return best_idx or 0\n",
    "\n",
    "def clean_columns(vals):\n",
    "    cols, seen = [], {}\n",
    "    for v in vals:\n",
    "        s = strip_accents_lower(v).replace(\"\\n\", \" \")\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip(\" -_\")\n",
    "        if not s or s.startswith(\"unnamed\"):\n",
    "            s = \"col\"\n",
    "        s = re.sub(r\"[^a-z0-9_ ]\", \"\", s)\n",
    "        s = re.sub(r\"\\s+\", \"_\", s).strip(\"_\") or \"col\"\n",
    "        if s in seen:\n",
    "            seen[s] += 1\n",
    "            s = f\"{s}_{seen[s]}\"\n",
    "        else:\n",
    "            seen[s] = 1\n",
    "        cols.append(s)\n",
    "    return cols\n",
    "\n",
    "def prune_columns(df: pd.DataFrame, header_keep: set[str]) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    keep = list(header_keep)\n",
    "    n = len(df)\n",
    "    for c in df.columns:\n",
    "        if c in header_keep:\n",
    "            continue\n",
    "        if not is_generic_colname(c):\n",
    "            keep.append(c)\n",
    "            continue\n",
    "        nnz = df[c].notna().sum()\n",
    "        if nnz >= max(MIN_NON_NULL_ABS, int(n * MIN_NON_NULL_RATIO)):\n",
    "            keep.append(c)\n",
    "    keep_ordered = [c for c in df.columns if c in keep]\n",
    "    return df[keep_ordered].copy()\n",
    "\n",
    "# ---------- Inférence de types (optionnelle) ----------\n",
    "def infer_and_cast_column(s: pd.Series, col_name: str) -> tuple[pd.Series, str]:\n",
    "    if not INFER_TYPES:\n",
    "        return s.astype(\"string\"), \"STRING\"\n",
    "    name_norm = strip_accents_lower(col_name)\n",
    "    looks_like_date_name = any(h in name_norm for h in DATE_NAME_HINTS)\n",
    "    s_obj = s.astype(\"string\")\n",
    "    non_empty = s_obj.dropna()\n",
    "\n",
    "    numeric_only_ratio = 0.0\n",
    "    if len(non_empty) > 0:\n",
    "        numeric_only_ratio = sum(bool(re.fullmatch(r\"\\d+(?:[.,]\\d+)?\", str(x).strip()))\n",
    "                                 for x in non_empty) / len(non_empty)\n",
    "\n",
    "    date_token_ratio = 0.0\n",
    "    if len(non_empty) > 0:\n",
    "        date_token_ratio = sum(bool(DATE_TOKEN_RE.search(str(x)))\n",
    "                               for x in non_empty) / len(non_empty)\n",
    "\n",
    "    # tentative dates\n",
    "    should_try_parse = (\n",
    "        (looks_like_date_name and date_token_ratio >= PRE_PARSE_TOKEN_RATIO_IF_NAME) or\n",
    "        ((not looks_like_date_name) and date_token_ratio >= PRE_PARSE_TOKEN_RATIO_NO_NAME and numeric_only_ratio < 0.80)\n",
    "    )\n",
    "    if should_try_parse:\n",
    "        parsed_dates = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "        date_ratio = parsed_dates.notna().sum() / max(1, s.notna().sum())\n",
    "        accept_date = (date_ratio >= 0.30) if looks_like_date_name else (date_ratio >= 0.70)\n",
    "        if accept_date:\n",
    "            return parsed_dates.dt.normalize(), \"DATE\"\n",
    "\n",
    "    # numériques\n",
    "    as_num = pd.to_numeric(s_obj.str.replace(\"\\u00A0\",\" \", regex=False)\n",
    "                               .str.replace(\" \",\"\", regex=False)\n",
    "                               .str.replace(\",\",\".\", regex=False), errors=\"coerce\")\n",
    "    num_ratio = as_num.notna().sum() / max(1, s.notna().sum())\n",
    "    if num_ratio >= 0.85:\n",
    "        as_int = as_num.dropna()\n",
    "        if len(as_int) == 0:\n",
    "            return as_num.astype(\"Float64\"), \"FLOAT\"\n",
    "        if (as_int % 1 == 0).all():\n",
    "            return as_num.astype(\"Int64\"), \"INT\"\n",
    "        else:\n",
    "            return as_num.astype(\"Float64\"), \"FLOAT\"\n",
    "\n",
    "    # bool\n",
    "    vals = non_empty.map(strip_accents_lower).unique().tolist()\n",
    "    if 1 <= len(set(vals)) <= 3:\n",
    "        mapped = s_obj.map(strip_accents_lower)\n",
    "        def map_bool(x):\n",
    "            if x in BOOL_TRUE: return True\n",
    "            if x in BOOL_FALSE: return False\n",
    "            return pd.NA\n",
    "        mb = mapped.map(map_bool)\n",
    "        if mb.notna().sum() / max(1, mapped.notna().sum()) >= 0.9:\n",
    "            return mb.astype(\"boolean\"), \"BOOL\"\n",
    "\n",
    "    return s_obj, \"STRING\"\n",
    "\n",
    "def infer_types_df(df: pd.DataFrame):\n",
    "    t0, _ = step_time()\n",
    "    out = df.copy()\n",
    "    schema = {}\n",
    "    for c in out.columns:\n",
    "        out[c], t = infer_and_cast_column(out[c], c)\n",
    "        schema[c] = t\n",
    "    _, dt = step_time(t0)\n",
    "    log(f\"    .. Inférence types: {len(out.columns)} colonnes (en {dt:.2f}s, activée={INFER_TYPES})\")\n",
    "    return out, schema\n",
    "\n",
    "def show_schema(df: pd.DataFrame, schema: dict):\n",
    "    print(\"\\n=== schema ===\", flush=True)\n",
    "    for c in df.columns:\n",
    "        t = schema.get(c, str(df[c].dtype)).upper()\n",
    "        print(f\"{c}: {t}\", flush=True)\n",
    "\n",
    "# ---------- Extraction d'une table sur un bloc (lignes) ----------\n",
    "def carve_table_from_block(df_block: pd.DataFrame):\n",
    "    if df_block.empty:\n",
    "        return None\n",
    "\n",
    "    log(\"    .. Sélection header\")\n",
    "    h_rel = choose_header_row(df_block)\n",
    "    raw_header_vals = df_block.iloc[h_rel].tolist()\n",
    "    cols = clean_columns(raw_header_vals)\n",
    "\n",
    "    data = df_block.iloc[h_rel+1:].copy()\n",
    "    data.columns = cols\n",
    "    log(f\"    .. Header choisi ligne relative {h_rel} -> {len(cols)} colonnes\")\n",
    "\n",
    "    # stop à N lignes vides consécutives\n",
    "    empty_run = 0\n",
    "    cut_idx = data.index[-1]\n",
    "    for idx in data.index:\n",
    "        if data.loc[idx].isna().all():\n",
    "            empty_run += 1\n",
    "            if empty_run >= STOP_EMPTY_RUN:\n",
    "                cut_idx = idx - STOP_EMPTY_RUN\n",
    "                break\n",
    "        else:\n",
    "            empty_run = 0\n",
    "\n",
    "    data = data.loc[:cut_idx]\n",
    "    data = data[data.notna().sum(axis=1) >= MIN_COLS]\n",
    "    log(f\"    .. Après nettoyage lignes: {data.shape[0]}x{data.shape[1]}\")\n",
    "\n",
    "    header_keep = {c for c in cols if not is_generic_colname(c)}\n",
    "    data = prune_columns(data, header_keep)\n",
    "    log(f\"    .. Après prune colonnes: {data.shape[0]}x{data.shape[1]}\")\n",
    "    if data.empty:\n",
    "        return None\n",
    "\n",
    "    data, schema = infer_types_df(data)\n",
    "\n",
    "    # normalisation visuelle dates si inférées\n",
    "    if INFER_TYPES:\n",
    "        for c, t in schema.items():\n",
    "            if t == \"DATE\":\n",
    "                try:\n",
    "                    data[c] = pd.to_datetime(data[c], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    return data.reset_index(drop=True), schema\n",
    "\n",
    "# ---------- Trouver tables dans une FEUILLE (segments horizontaux + blocs verticaux) ----------\n",
    "def find_tables_in_sheet(df_raw: pd.DataFrame):\n",
    "    print(\"  -> Détection des blocs tabulaires…\", flush=True)\n",
    "    df_raw = ensure_range_columns(df_raw)\n",
    "    if SCAN_MAX_ROWS is not None and len(df_raw) > SCAN_MAX_ROWS:\n",
    "        df_raw = df_raw.iloc[:SCAN_MAX_ROWS, :]\n",
    "        log(f\"     (tronqué à {SCAN_MAX_ROWS} lignes)\")\n",
    "\n",
    "    # Segments verticaux (tables côte-à-côte)\n",
    "    segments = split_by_vertical_gaps(df_raw)\n",
    "    if MAX_SEGMENTS_PER_SHEET:\n",
    "        segments = segments[:MAX_SEGMENTS_PER_SHEET]\n",
    "    log(f\"  -> Segments verticaux: {len(segments)}\")\n",
    "\n",
    "    tables = []\n",
    "    table_count = 0\n",
    "    for seg_idx, (seg_df, c0, c1) in enumerate(segments, 1):\n",
    "        log(f\"  -> Segment {seg_idx}: cols {c0}..{c1} (shape {seg_df.shape[0]}x{seg_df.shape[1]})\")\n",
    "        # blocs verticaux (par lignes) à l'intérieur du segment\n",
    "        blocks = detect_blocks(seg_df)\n",
    "        for k, (start, end) in enumerate(blocks, 1):\n",
    "            log(f\"     -> Carve table {k}/{len(blocks)} dans segment {seg_idx} (rows {start}..{end})\")\n",
    "            block = seg_df.loc[start:end, :]\n",
    "            carved = carve_table_from_block(block)\n",
    "            if carved is None:\n",
    "                log(\"        .. ignoré (vide après carve)\")\n",
    "                continue\n",
    "            table, schema = carved\n",
    "            if table is not None and table.shape[1] >= MIN_COLS and table.shape[0] >= 1:\n",
    "                tables.append((seg_idx, k, table, schema))\n",
    "                table_count += 1\n",
    "                log(f\"        .. table retenue: {table.shape[0]}x{table.shape[1]}\")\n",
    "                if QUICK_MODE and MAX_TABLES_PER_SHEET and table_count >= MAX_TABLES_PER_SHEET:\n",
    "                    log(\"        .. limite de tables atteinte (mode rapide)\")\n",
    "                    return tables\n",
    "    log(f\"  -> Tables retenues sur la feuille: {len(tables)}\")\n",
    "    return tables\n",
    "\n",
    "# ---------- Par fichier ----------\n",
    "def process_file(path: Path):\n",
    "    results = []  # tuples: (sheet_name, seg_idx, block_idx, df, schema)\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\", \".xlsb\"):\n",
    "        for sheet, df_raw in safe_read_excel_all_sheets(path):\n",
    "            log(f\"-- Feuille: {sheet} | taille {df_raw.shape[0]}x{df_raw.shape[1]}\")\n",
    "            for (seg_i, blk_i, t, sc) in find_tables_in_sheet(df_raw):\n",
    "                results.append((sheet, seg_i, blk_i, t, sc))\n",
    "    elif ext == \".csv\":\n",
    "        df_raw = read_csv_fast(path)\n",
    "        log(f\"-- CSV lu | taille {df_raw.shape[0]}x{df_raw.shape[1]}\")\n",
    "        for (seg_i, blk_i, t, sc) in find_tables_in_sheet(df_raw):\n",
    "            results.append((None, seg_i, blk_i, t, sc))\n",
    "    else:\n",
    "        raise ValueError(f\"Extension non gérée: {ext}\")\n",
    "    return results\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    print(\"=== data header: folder info ===\", flush=True)\n",
    "    print(f\"path: {ROOT_DIR.resolve()}\", flush=True)\n",
    "\n",
    "    files = []\n",
    "    for pat in (\"*.xlsx\", \"*.xlsm\", \"*.xlsb\", \"*.csv\"):\n",
    "        files += [p for p in ROOT_DIR.rglob(pat) if p.is_file() and not p.name.startswith(\"~$\")]\n",
    "    files = sorted(files)\n",
    "    if MAX_FILES:\n",
    "        files = files[:MAX_FILES]\n",
    "    print(f\"files_found: {len(files)}\", flush=True)\n",
    "\n",
    "    print(\"\\n=== config ===\", flush=True)\n",
    "    print(f\"QUICK_MODE={QUICK_MODE} | INFER_TYPES={INFER_TYPES} | VERBOSE={VERBOSE}\", flush=True)\n",
    "    print(f\"SCAN_MAX_ROWS={SCAN_MAX_ROWS} | MAX_FILES={MAX_FILES} | MAX_SHEETS={MAX_SHEETS} | MAX_TABLES_PER_SHEET={MAX_TABLES_PER_SHEET}\", flush=True)\n",
    "    print(f\"MIN_COLS={MIN_COLS} | MIN_CONSEC_ROWS={MIN_CONSEC_ROWS} | HEADER_SCAN_DEPTH={HEADER_SCAN_DEPTH}\", flush=True)\n",
    "    print(f\"COL_DENSITY_THRESHOLD={COL_DENSITY_THRESHOLD} | MIN_COL_RUN={MIN_COL_RUN} | ALLOW_SMALL_GAPS={ALLOW_SMALL_GAPS}\", flush=True)\n",
    "\n",
    "    if not files:\n",
    "        print(\"[warn] Aucun fichier trouvé.\", flush=True); return\n",
    "\n",
    "    total_tables = 0\n",
    "    total_sheets = 0\n",
    "\n",
    "    for f in files:\n",
    "        print(\"\\n=== file ===\", flush=True)\n",
    "        print(f\"{f.name}  ({f.resolve()})\", flush=True)\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            tables = process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Lecture échouée pour {f.name}: {e}\", flush=True)\n",
    "            continue\n",
    "\n",
    "        # comptage feuilles approx\n",
    "        sheets_in_file = len({s for (s, *_rest) in tables if s is not None})\n",
    "        total_sheets += sheets_in_file\n",
    "        total_tables += len(tables)\n",
    "\n",
    "        if not tables:\n",
    "            print(\"[info] Aucune table détectée\", flush=True)\n",
    "        else:\n",
    "            file_id = safe_id(f.stem)\n",
    "            for (sheet, seg_i, blk_i, df, schema) in tables:\n",
    "                sheet_id = safe_id(sheet or \"sheet\")\n",
    "                table_idx = f\"{seg_i}_{blk_i}\"  # segment X, bloc Y\n",
    "                table_name = f\"{file_id}.{sheet_id}.table_{table_idx}\"\n",
    "                print(f\"\\n--- table detected ---\", flush=True)\n",
    "                print(f\"name: {table_name}\", flush=True)  # <<<< nommage exploitable nom_fic.nom_onglet.table_X_Y\n",
    "                print(f\"rows: {len(df)} | cols: {df.shape[1]}\", flush=True)\n",
    "                print(\"columns:\", \", \".join(map(str, df.columns.tolist())), flush=True)\n",
    "                show_schema(df, schema)\n",
    "                if SHOW_SAMPLES:\n",
    "                    with pd.option_context(\"display.max_columns\", 80, \"display.width\", 200):\n",
    "                        print(\"\\n=== sample (top 8) ===\", flush=True)\n",
    "                        print(df.head(8), flush=True)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"[info] Temps fichier: {t1 - t0:.2f}s\", flush=True)\n",
    "\n",
    "    print(\"\\n=== done ===\", flush=True)\n",
    "    print(f\"Tables: {total_tables} | Feuilles (approx): {total_sheets} | Fichiers: {len(files)}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[info] Interrompu par l'utilisateur.\", flush=True)\n",
    "        sys.exit(130)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedbf3e-5480-49ff-9106-38db1c92c11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0087b-c5bc-40e3-9c01-cd4dfaee5ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
